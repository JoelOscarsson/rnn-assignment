{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "The goal for this assignment is to classify feelings in a text corpus. Each row has a sentence with corresponding emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 22:28:06.566209: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joeloscarsson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joeloscarsson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/joeloscarsson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import timeit\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN, GRU\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import ssl \n",
    "\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_DEFAULT = nltk.corpus.stopwords.words('english')\n",
    "LEMMATIZER = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "FILE = \"./data/emotions.csv\"\n",
    "LOGS_PATH = (\"./logs\")\n",
    "\n",
    "\n",
    "# EPOCHS = 30\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_separator():\n",
    "    \"\"\" Returns a separator \"\"\"\n",
    "    print(\"======================================================\")\n",
    "    \n",
    "\n",
    "def get_file():\n",
    "    \"\"\" Get the file and return it as a pandas dataframe\"\"\"\n",
    "    data = pd.read_csv(FILE)\n",
    "    pd.set_option('display.max_colwidth', 200) # too be able to see more of the text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel so pissed off over an old friend and some friends</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ive found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i also feel it is unfortunate that nearly all the readers of going to meet the man will be african americans unlike myself</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel petty a href http clairee</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i used to believe that a feeling like fear was to be ignored or suppressed right away more on this in a moment</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>i was i might be buying stuff from there but i feel the clothes are too casual</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>i like sonam deepika and genelia who i feel are very talented and beautiful</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>i feel pathetic that i can hardly go a whole day not talking to him</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>id have spent more time with her on reading i feel a bit guilty about that</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>i do however feel like one of those pathetic girls who make up excuses because of a guy</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        text  \\\n",
       "0                                                                                   i feel so pissed off over an old friend and some friends   \n",
       "1      ive found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated   \n",
       "2                 i also feel it is unfortunate that nearly all the readers of going to meet the man will be african americans unlike myself   \n",
       "3                                                                                                           i feel petty a href http clairee   \n",
       "4                             i used to believe that a feeling like fear was to be ignored or suppressed right away more on this in a moment   \n",
       "...                                                                                                                                      ...   \n",
       "19995                                                         i was i might be buying stuff from there but i feel the clothes are too casual   \n",
       "19996                                                            i like sonam deepika and genelia who i feel are very talented and beautiful   \n",
       "19997                                                                    i feel pathetic that i can hardly go a whole day not talking to him   \n",
       "19998                                                             id have spent more time with her on reading i feel a bit guilty about that   \n",
       "19999                                                i do however feel like one of those pathetic girls who make up excuses because of a guy   \n",
       "\n",
       "         label  \n",
       "0        anger  \n",
       "1        anger  \n",
       "2      sadness  \n",
       "3        anger  \n",
       "4      sadness  \n",
       "...        ...  \n",
       "19995      joy  \n",
       "19996      joy  \n",
       "19997  sadness  \n",
       "19998  sadness  \n",
       "19999  sadness  \n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    20000 non-null  object\n",
      " 1   label   20000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 312.6+ KB\n"
     ]
    }
   ],
   "source": [
    "get_file().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'sadness', 'joy', 'love', 'fear', 'surprise'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file()['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGxCAYAAACDV6ltAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7JklEQVR4nO3de1RVZf7H8Q+IXAQPiArIiIRpCqWWWHqy7IZSUaOTNl0cxUSdHMxRJjXXNGR2sdHKtLyMXaSLTnazUkfUvKbiJQpTUVKjcFKwi3DSFBCe3x8u9s8zmhoiB93v11p75dnPd+/z3XvROZ+1z3P28TLGGAEAANiYt6cbAAAA8DQCEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0fTzdwIaisrNS+ffvUsGFDeXl5ebodAABwFowx+vnnnxUZGSlv79NfAyIQnYV9+/YpKirK020AAIBq2Lt3r5o3b37aGo8GoksuuUTffvvtSev/8pe/aNq0aTp69Kj+9re/6e2331ZpaakSExM1ffp0hYeHW7UFBQUaOnSoVq5cqaCgICUnJ2vChAny8fn/Q1u1apXS0tK0fft2RUVF6dFHH9WAAQPOus+GDRtKOn5CHQ5H9Q8YAADUGpfLpaioKOt9/HQ8Gog2b96siooK6/G2bdvUvXt33X333ZKkkSNHatGiRXr33XcVHBysYcOG6a677tK6deskSRUVFUpKSlJERITWr1+v/fv3q3///qpfv76efvppSVJ+fr6SkpL04IMPas6cOVq+fLkGDRqkZs2aKTEx8az6rPqYzOFwEIgAALjAnM10F6+69OOuI0aM0MKFC7Vr1y65XC41bdpUc+fOVZ8+fSRJO3fuVGxsrLKystSlSxctXrxYd9xxh/bt22ddNZo5c6bGjBmj77//Xr6+vhozZowWLVqkbdu2Wc9z7733qri4WJmZmWfVl8vlUnBwsEpKSghEAABcIH7L+3ed+ZZZWVmZ3nrrLQ0cOFBeXl7Kzs5WeXm5EhISrJq2bduqRYsWysrKkiRlZWWpXbt2bh+hJSYmyuVyafv27VbNifuoqqnax6mUlpbK5XK5LQAA4OJVZwLRhx9+qOLiYmtuT2FhoXx9fRUSEuJWFx4ersLCQqvmxDBUNV41droal8ulI0eOnLKXCRMmKDg42FqYUA0AwMWtzgSiV199VbfddpsiIyM93YrGjh2rkpISa9m7d6+nWwIAAOdRnfja/bfffqtPPvlEH3zwgbUuIiJCZWVlKi4udrtKVFRUpIiICKtm06ZNbvsqKiqyxqr+W7XuxBqHw6GAgIBT9uPn5yc/P79zPi4AAHBhqBNXiGbPnq2wsDAlJSVZ6+Lj41W/fn0tX77cWpeXl6eCggI5nU5JktPp1NatW3XgwAGrZtmyZXI4HIqLi7NqTtxHVU3VPgAAADweiCorKzV79mwlJye73TsoODhYKSkpSktL08qVK5Wdna0HHnhATqdTXbp0kST16NFDcXFx6tevn7Zs2aIlS5bo0UcfVWpqqnWF58EHH9TXX3+t0aNHa+fOnZo+fbreeecdjRw50iPHCwAA6h6Pf2T2ySefqKCgQAMHDjxpbPLkyfL29lbv3r3dbsxYpV69elq4cKGGDh0qp9OpwMBAJScna/z48VZNTEyMFi1apJEjR2rKlClq3ry5XnnllbO+BxEAALj41an7ENVV3IcIAIALzwV5HyIAAABPIRABAADbIxABAADbIxABAADbIxABAADbIxABAADb8/h9iACcP/Gj3vB0C3VC9qT+nm4BQB3HFSIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7Hg9E3333nf70pz+pcePGCggIULt27fTZZ59Z48YYpaenq1mzZgoICFBCQoJ27drlto+ffvpJffv2lcPhUEhIiFJSUnTo0CG3mi+//FLXX3+9/P39FRUVpYkTJ9bK8QEAgLrPo4Ho4MGD6tq1q+rXr6/FixcrNzdXzz33nBo1amTVTJw4UVOnTtXMmTO1ceNGBQYGKjExUUePHrVq+vbtq+3bt2vZsmVauHCh1qxZoyFDhljjLpdLPXr0UHR0tLKzszVp0iSNGzdOs2bNqtXjBQAAdZOXMcZ46skfeeQRrVu3Tp9++ukpx40xioyM1N/+9jc9/PDDkqSSkhKFh4crIyND9957r3bs2KG4uDht3rxZnTp1kiRlZmbq9ttv13//+19FRkZqxowZ+vvf/67CwkL5+vpaz/3hhx9q586dZ+zT5XIpODhYJSUlcjgcNXT0wPkXP+oNT7dQJ2RP6u/pFgB4wG95//boFaKPP/5YnTp10t13362wsDBdddVVevnll63x/Px8FRYWKiEhwVoXHByszp07KysrS5KUlZWlkJAQKwxJUkJCgry9vbVx40arplu3blYYkqTExETl5eXp4MGDJ/VVWloql8vltgAAgIuXRwPR119/rRkzZqh169ZasmSJhg4dquHDh+v111+XJBUWFkqSwsPD3bYLDw+3xgoLCxUWFuY27uPjo9DQULeaU+3jxOc40YQJExQcHGwtUVFRNXC0AACgrvJoIKqsrFTHjh319NNP66qrrtKQIUM0ePBgzZw505NtaezYsSopKbGWvXv3erQfAABwfnk0EDVr1kxxcXFu62JjY1VQUCBJioiIkCQVFRW51RQVFVljEREROnDggNv4sWPH9NNPP7nVnGofJz7Hifz8/ORwONwWAABw8fJoIOratavy8vLc1n311VeKjo6WJMXExCgiIkLLly+3xl0ulzZu3Cin0ylJcjqdKi4uVnZ2tlWzYsUKVVZWqnPnzlbNmjVrVF5ebtUsW7ZMbdq0cftGGwAAsCePBqKRI0dqw4YNevrpp7V7927NnTtXs2bNUmpqqiTJy8tLI0aM0JNPPqmPP/5YW7duVf/+/RUZGalevXpJOn5F6dZbb9XgwYO1adMmrVu3TsOGDdO9996ryMhISdL9998vX19fpaSkaPv27Zo3b56mTJmitLQ0Tx06AACoQ3w8+eRXX3215s+fr7Fjx2r8+PGKiYnRCy+8oL59+1o1o0eP1uHDhzVkyBAVFxfruuuuU2Zmpvz9/a2aOXPmaNiwYbrlllvk7e2t3r17a+rUqdZ4cHCwli5dqtTUVMXHx6tJkyZKT093u1cRAACwL4/eh+hCwX2IcKHiPkTHcR8iwJ4umPsQAQAA1AUEIgAAYHsEIgAAYHsEIgAAYHse/ZYZcCpMBD6OicAAUHu4QgQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGyPQAQAAGzPo4Fo3Lhx8vLyclvatm1rjR89elSpqalq3LixgoKC1Lt3bxUVFbnto6CgQElJSWrQoIHCwsI0atQoHTt2zK1m1apV6tixo/z8/NSqVStlZGTUxuEBAIALhMevEF1++eXav3+/taxdu9YaGzlypBYsWKB3331Xq1ev1r59+3TXXXdZ4xUVFUpKSlJZWZnWr1+v119/XRkZGUpPT7dq8vPzlZSUpJtuukk5OTkaMWKEBg0apCVLltTqcQIAgLrLx+MN+PgoIiLipPUlJSV69dVXNXfuXN18882SpNmzZys2NlYbNmxQly5dtHTpUuXm5uqTTz5ReHi4rrzySj3xxBMaM2aMxo0bJ19fX82cOVMxMTF67rnnJEmxsbFau3atJk+erMTExFo9VgAAUDd5/ArRrl27FBkZqZYtW6pv374qKCiQJGVnZ6u8vFwJCQlWbdu2bdWiRQtlZWVJkrKystSuXTuFh4dbNYmJiXK5XNq+fbtVc+I+qmqq9nEqpaWlcrlcbgsAALh4eTQQde7cWRkZGcrMzNSMGTOUn5+v66+/Xj///LMKCwvl6+urkJAQt23Cw8NVWFgoSSosLHQLQ1XjVWOnq3G5XDpy5Mgp+5owYYKCg4OtJSoqqiYOFwAA1FEe/cjstttus/7dvn17de7cWdHR0XrnnXcUEBDgsb7Gjh2rtLQ067HL5SIUAQBwEfP4R2YnCgkJ0WWXXabdu3crIiJCZWVlKi4udqspKiqy5hxFRESc9K2zqsdnqnE4HL8auvz8/ORwONwWAABw8apTgejQoUPas2ePmjVrpvj4eNWvX1/Lly+3xvPy8lRQUCCn0ylJcjqd2rp1qw4cOGDVLFu2TA6HQ3FxcVbNifuoqqnaBwAAgEcD0cMPP6zVq1frm2++0fr16/WHP/xB9erV03333afg4GClpKQoLS1NK1euVHZ2th544AE5nU516dJFktSjRw/FxcWpX79+2rJli5YsWaJHH31Uqamp8vPzkyQ9+OCD+vrrrzV69Gjt3LlT06dP1zvvvKORI0d68tABAEAd4tE5RP/9739133336ccff1TTpk113XXXacOGDWratKkkafLkyfL29lbv3r1VWlqqxMRETZ8+3dq+Xr16WrhwoYYOHSqn06nAwEAlJydr/PjxVk1MTIwWLVqkkSNHasqUKWrevLleeeUVvnIPAAAsXsYY4+km6jqXy6Xg4GCVlJQwn6gWxI96w9Mt1AnZk/qf8z44l8fVxLkEcOH5Le/fdWoOEQAAgCcQiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO3VmUD0zDPPyMvLSyNGjLDWHT16VKmpqWrcuLGCgoLUu3dvFRUVuW1XUFCgpKQkNWjQQGFhYRo1apSOHTvmVrNq1Sp17NhRfn5+atWqlTIyMmrhiAAAwIWiTgSizZs361//+pfat2/vtn7kyJFasGCB3n33Xa1evVr79u3TXXfdZY1XVFQoKSlJZWVlWr9+vV5//XVlZGQoPT3dqsnPz1dSUpJuuukm5eTkaMSIERo0aJCWLFlSa8cHAADqNo8HokOHDqlv3756+eWX1ahRI2t9SUmJXn31VT3//PO6+eabFR8fr9mzZ2v9+vXasGGDJGnp0qXKzc3VW2+9pSuvvFK33XabnnjiCU2bNk1lZWWSpJkzZyomJkbPPfecYmNjNWzYMPXp00eTJ0/2yPECAIC6x+OBKDU1VUlJSUpISHBbn52drfLycrf1bdu2VYsWLZSVlSVJysrKUrt27RQeHm7VJCYmyuVyafv27VbN/+47MTHR2seplJaWyuVyuS0AAODi5ePJJ3/77bf1+eefa/PmzSeNFRYWytfXVyEhIW7rw8PDVVhYaNWcGIaqxqvGTlfjcrl05MgRBQQEnPTcEyZM0OOPP17t4wIAABcWj10h2rt3r/76179qzpw58vf391QbpzR27FiVlJRYy969ez3dEgAAOI88Foiys7N14MABdezYUT4+PvLx8dHq1as1depU+fj4KDw8XGVlZSouLnbbrqioSBEREZKkiIiIk751VvX4TDUOh+OUV4ckyc/PTw6Hw20BAAAXL48FoltuuUVbt25VTk6OtXTq1El9+/a1/l2/fn0tX77c2iYvL08FBQVyOp2SJKfTqa1bt+rAgQNWzbJly+RwOBQXF2fVnLiPqpqqfQAAAHhsDlHDhg11xRVXuK0LDAxU48aNrfUpKSlKS0tTaGioHA6HHnroITmdTnXp0kWS1KNHD8XFxalfv36aOHGiCgsL9eijjyo1NVV+fn6SpAcffFAvvfSSRo8erYEDB2rFihV65513tGjRoto9YAAAUGd5dFL1mUyePFne3t7q3bu3SktLlZiYqOnTp1vj9erV08KFCzV06FA5nU4FBgYqOTlZ48ePt2piYmK0aNEijRw5UlOmTFHz5s31yiuvKDEx0ROHBAAA6iAvY4zxdBN1ncvlUnBwsEpKSphPVAviR73h6RbqhOxJ/c95H5zL42riXAK48PyW9+9qzSG6+eabT5rsXPXEN998c3V2CQAA4DHVCkSrVq2y7gR9oqNHj+rTTz8956YAAABq02+aQ/Tll19a/87NzbVufigd/12xzMxM/e53v6u57gAAAGrBbwpEV155pby8vOTl5XXKj8YCAgL04osv1lhzAAAAteE3BaL8/HwZY9SyZUtt2rRJTZs2tcZ8fX0VFhamevXq1XiTAAAA59NvCkTR0dGSpMrKyvPSDAAAgCdU+z5Eu3bt0sqVK3XgwIGTAlJ6evo5NwYAAFBbqhWIXn75ZQ0dOlRNmjRRRESEvLy8rDEvLy8CEQAAuKBUKxA9+eSTeuqppzRmzJia7gcAAKDWVes+RAcPHtTdd99d070AAAB4RLUC0d13362lS5fWdC8AAAAeUa2PzFq1aqV//OMf2rBhg9q1a6f69eu7jQ8fPrxGmgMAAKgN1QpEs2bNUlBQkFavXq3Vq1e7jXl5eRGIAADABaVagSg/P7+m+wAAAPCYas0hAgAAuJhU6wrRwIEDTzv+2muvVasZAAAAT6hWIDp48KDb4/Lycm3btk3FxcWn/NFXAACAuqxagWj+/PknrausrNTQoUN16aWXnnNTAAAAtanG5hB5e3srLS1NkydPrqldAgAA1IoanVS9Z88eHTt2rCZ3CQAAcN5V6yOztLQ0t8fGGO3fv1+LFi1ScnJyjTQGAABQW6oViL744gu3x97e3mratKmee+65M34DDQAAoK6pViBauXJlTfcBAADgMdUKRFW+//575eXlSZLatGmjpk2b1khTAAAAtalak6oPHz6sgQMHqlmzZurWrZu6deumyMhIpaSk6JdffqnpHgEAAM6ragWitLQ0rV69WgsWLFBxcbGKi4v10UcfafXq1frb3/5W0z0CAACcV9X6yOz999/Xe++9pxtvvNFad/vttysgIEB//OMfNWPGjJrqDwAA4Lyr1hWiX375ReHh4SetDwsL4yMzAABwwalWIHI6nXrsscd09OhRa92RI0f0+OOPy+l01lhzAAAAtaFaH5m98MILuvXWW9W8eXN16NBBkrRlyxb5+flp6dKlNdogAADA+VatQNSuXTvt2rVLc+bM0c6dOyVJ9913n/r27auAgIAabRAAAOB8q1YgmjBhgsLDwzV48GC39a+99pq+//57jRkzpkaaAwAAqA3VmkP0r3/9S23btj1p/eWXX66ZM2eec1MAAAC1qVqBqLCwUM2aNTtpfdOmTbV///5zbgoAAKA2VSsQRUVFad26dSetX7dunSIjI8+5KQAAgNpUrTlEgwcP1ogRI1ReXq6bb75ZkrR8+XKNHj2aO1UDAIALTrUC0ahRo/Tjjz/qL3/5i8rKyiRJ/v7+GjNmjMaOHVujDQIAAJxv1QpEXl5e+uc//6l//OMf2rFjhwICAtS6dWv5+fnVdH8AAADnXbUCUZWgoCBdffXVNdULAACAR1RrUjUAAMDFhEAEAABsz6OBaMaMGWrfvr0cDoccDoecTqcWL15sjR89elSpqalq3LixgoKC1Lt3bxUVFbnto6CgQElJSWrQoIHCwsI0atQoHTt2zK1m1apV6tixo/z8/NSqVStlZGTUxuEBAIALhEcDUfPmzfXMM88oOztbn332mW6++Wb17NlT27dvlySNHDlSCxYs0LvvvqvVq1dr3759uuuuu6ztKyoqlJSUpLKyMq1fv16vv/66MjIylJ6ebtXk5+crKSlJN910k3JycjRixAgNGjRIS5YsqfXjBQAAdZOXMcZ4uokThYaGatKkSerTp4+aNm2quXPnqk+fPpKknTt3KjY2VllZWerSpYsWL16sO+64Q/v27VN4eLgkaebMmRozZoy+//57+fr6asyYMVq0aJG2bdtmPce9996r4uJiZWZmnlVPLpdLwcHBKikpkcPhqPmDhpv4UW94uoU6IXtS/3PeB+fyuJo4lwAuPL/l/bvOzCGqqKjQ22+/rcOHD8vpdCo7O1vl5eVKSEiwatq2basWLVooKytLkpSVlaV27dpZYUiSEhMT5XK5rKtMWVlZbvuoqqnaBwAAwDl97b4mbN26VU6nU0ePHlVQUJDmz5+vuLg45eTkyNfXVyEhIW714eHhKiwslHT8N9VODENV41Vjp6txuVw6cuSIAgICTuqptLRUpaWl1mOXy3XOxwkAAOouj18hatOmjXJycrRx40YNHTpUycnJys3N9WhPEyZMUHBwsLVERUV5tB8AAHB+eTwQ+fr6qlWrVoqPj9eECRPUoUMHTZkyRRERESorK1NxcbFbfVFRkSIiIiRJERERJ33rrOrxmWocDscprw5J0tixY1VSUmIte/furYlDBQAAdZTHA9H/qqysVGlpqeLj41W/fn0tX77cGsvLy1NBQYGcTqckyel0auvWrTpw4IBVs2zZMjkcDsXFxVk1J+6jqqZqH6fi5+dn3QqgagEAABcvj84hGjt2rG677Ta1aNFCP//8s+bOnatVq1ZpyZIlCg4OVkpKitLS0hQaGiqHw6GHHnpITqdTXbp0kST16NFDcXFx6tevnyZOnKjCwkI9+uijSk1NtX5X7cEHH9RLL72k0aNHa+DAgVqxYoXeeecdLVq0yJOHDgAA6hCPBqIDBw6of//+2r9/v4KDg9W+fXstWbJE3bt3lyRNnjxZ3t7e6t27t0pLS5WYmKjp06db29erV08LFy7U0KFD5XQ6FRgYqOTkZI0fP96qiYmJ0aJFizRy5EhNmTJFzZs31yuvvKLExMRaP14AAFA31bn7ENVF3IeodnHvnOO4D1HN4T5EgD1dkPchAgAA8BQCEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD2P/nTHxYa7Ah/HXYEBABcarhABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADbIxABAADb8/F0AwAA+4gf9YanW6gTsif193QL+B9cIQIAALZHIAIAALZHIAIAALZHIAIAALZHIAIAALZHIAIAALZHIAIAALZHIAIAALZHIAIAALZHIAIAALZHIAIAALZHIAIAALZHIAIAALZHIAIAALbn0UA0YcIEXX311WrYsKHCwsLUq1cv5eXludUcPXpUqampaty4sYKCgtS7d28VFRW51RQUFCgpKUkNGjRQWFiYRo0apWPHjrnVrFq1Sh07dpSfn59atWqljIyM8314AADgAuHRQLR69WqlpqZqw4YNWrZsmcrLy9WjRw8dPnzYqhk5cqQWLFigd999V6tXr9a+fft01113WeMVFRVKSkpSWVmZ1q9fr9dff10ZGRlKT0+3avLz85WUlKSbbrpJOTk5GjFihAYNGqQlS5bU6vECAIC6yceTT56Zmen2OCMjQ2FhYcrOzla3bt1UUlKiV199VXPnztXNN98sSZo9e7ZiY2O1YcMGdenSRUuXLlVubq4++eQThYeH68orr9QTTzyhMWPGaNy4cfL19dXMmTMVExOj5557TpIUGxurtWvXavLkyUpMTKz14wYAAHVLnZpDVFJSIkkKDQ2VJGVnZ6u8vFwJCQlWTdu2bdWiRQtlZWVJkrKystSuXTuFh4dbNYmJiXK5XNq+fbtVc+I+qmqq9gEAAOzNo1eITlRZWakRI0aoa9euuuKKKyRJhYWF8vX1VUhIiFtteHi4CgsLrZoTw1DVeNXY6WpcLpeOHDmigIAAt7HS0lKVlpZaj10u17kfIAAAqLPqzBWi1NRUbdu2TW+//banW9GECRMUHBxsLVFRUZ5uCQAAnEd1IhANGzZMCxcu1MqVK9W8eXNrfUREhMrKylRcXOxWX1RUpIiICKvmf791VvX4TDUOh+Okq0OSNHbsWJWUlFjL3r17z/kYAQBA3eXRQGSM0bBhwzR//nytWLFCMTExbuPx8fGqX7++li9fbq3Ly8tTQUGBnE6nJMnpdGrr1q06cOCAVbNs2TI5HA7FxcVZNSfuo6qmah//y8/PTw6Hw20BAAAXL4/OIUpNTdXcuXP10UcfqWHDhtacn+DgYAUEBCg4OFgpKSlKS0tTaGioHA6HHnroITmdTnXp0kWS1KNHD8XFxalfv36aOHGiCgsL9eijjyo1NVV+fn6SpAcffFAvvfSSRo8erYEDB2rFihV65513tGjRIo8dOwAAqDs8eoVoxowZKikp0Y033qhmzZpZy7x586yayZMn64477lDv3r3VrVs3RURE6IMPPrDG69Wrp4ULF6pevXpyOp3605/+pP79+2v8+PFWTUxMjBYtWqRly5apQ4cOeu655/TKK6/wlXsAACDJw1eIjDFnrPH399e0adM0bdq0X62Jjo7Wf/7zn9Pu58Ybb9QXX3zxm3sEAAAXvzoxqRoAAMCTCEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2PPrjrgBwoYgf9YanW6gTsif193QLwHnBFSIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7Hg1Ea9as0Z133qnIyEh5eXnpww8/dBs3xig9PV3NmjVTQECAEhIStGvXLrean376SX379pXD4VBISIhSUlJ06NAht5ovv/xS119/vfz9/RUVFaWJEyee70MDAAAXEI8GosOHD6tDhw6aNm3aKccnTpyoqVOnaubMmdq4caMCAwOVmJioo0ePWjV9+/bV9u3btWzZMi1cuFBr1qzRkCFDrHGXy6UePXooOjpa2dnZmjRpksaNG6dZs2ad9+MDAAAXBh9PPvltt92m22677ZRjxhi98MILevTRR9WzZ09J0htvvKHw8HB9+OGHuvfee7Vjxw5lZmZq8+bN6tSpkyTpxRdf1O23365nn31WkZGRmjNnjsrKyvTaa6/J19dXl19+uXJycvT888+7BScAAGBfdXYOUX5+vgoLC5WQkGCtCw4OVufOnZWVlSVJysrKUkhIiBWGJCkhIUHe3t7auHGjVdOtWzf5+vpaNYmJicrLy9PBgwdr6WgAAEBd5tErRKdTWFgoSQoPD3dbHx4ebo0VFhYqLCzMbdzHx0ehoaFuNTExMSfto2qsUaNGJz13aWmpSktLrccul+scjwYAANRldfYKkSdNmDBBwcHB1hIVFeXplgAAwHlUZwNRRESEJKmoqMhtfVFRkTUWERGhAwcOuI0fO3ZMP/30k1vNqfZx4nP8r7Fjx6qkpMRa9u7de+4HBAAA6qw6G4hiYmIUERGh5cuXW+tcLpc2btwop9MpSXI6nSouLlZ2drZVs2LFClVWVqpz585WzZo1a1ReXm7VLFu2TG3atDnlx2WS5OfnJ4fD4bYAAICLl0cD0aFDh5STk6OcnBxJxydS5+TkqKCgQF5eXhoxYoSefPJJffzxx9q6dav69++vyMhI9erVS5IUGxurW2+9VYMHD9amTZu0bt06DRs2TPfee68iIyMlSffff798fX2VkpKi7du3a968eZoyZYrS0tI8dNQAAKCu8eik6s8++0w33XST9bgqpCQnJysjI0OjR4/W4cOHNWTIEBUXF+u6665TZmam/P39rW3mzJmjYcOG6ZZbbpG3t7d69+6tqVOnWuPBwcFaunSpUlNTFR8fryZNmig9PZ2v3AMAAItHA9GNN94oY8yvjnt5eWn8+PEaP378r9aEhoZq7ty5p32e9u3b69NPP612nwAA4OJWZ+cQAQAA1BYCEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD0CEQAAsD2P3pgRAABUT/yoNzzdQp2QPal/jeyHK0QAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2CEQAAMD2bBWIpk2bpksuuUT+/v7q3LmzNm3a5OmWAABAHWCbQDRv3jylpaXpscce0+eff64OHTooMTFRBw4c8HRrAADAw2wTiJ5//nkNHjxYDzzwgOLi4jRz5kw1aNBAr732mqdbAwAAHmaLQFRWVqbs7GwlJCRY67y9vZWQkKCsrCwPdgYAAOoCH083UBt++OEHVVRUKDw83G19eHi4du7ceVJ9aWmpSktLrcclJSWSJJfLddrnqSg9UgPdXvjOdJ7OhPN43LmeR4lzWYVzWXP4/7tm8DdZc053LqvGjDFn3pGxge+++85IMuvXr3dbP2rUKHPNNdecVP/YY48ZSSwsLCwsLCwXwbJ3794zZgVbXCFq0qSJ6tWrp6KiIrf1RUVFioiIOKl+7NixSktLsx5XVlbqp59+UuPGjeXl5XXe+60ul8ulqKgo7d27Vw6Hw9PtXLA4jzWHc1lzOJc1g/NYcy6Ec2mM0c8//6zIyMgz1toiEPn6+io+Pl7Lly9Xr169JB0POcuXL9ewYcNOqvfz85Ofn5/bupCQkFrotGY4HI46+8d5IeE81hzOZc3hXNYMzmPNqevnMjg4+KzqbBGIJCktLU3Jycnq1KmTrrnmGr3wwgs6fPiwHnjgAU+3BgAAPMw2geiee+7R999/r/T0dBUWFurKK69UZmbmSROtAQCA/dgmEEnSsGHDTvkR2cXCz89Pjz322Ekf9+G34TzWHM5lzeFc1gzOY8252M6llzFn8100AACAi5ctbswIAABwOgQiAABgewQiAABgewQi2JqXl5c+/PBDT7dxQRswYIB1fy+cnRtvvFEjRozwdBsXDWOMhgwZotDQUHl5eSknJ8fTLeEE48aN05VXXunpNs7IVt8yA1DzpkyZcna/EwScJ5mZmcrIyNCqVavUsmVLNWnSxNMt4QQPP/ywHnroIU+3cUYEIvyq8vJy1a9f39NtoI4727vAAufLnj171KxZM1177bXn7TnKysrk6+t73vZfl1X32I0xqqioUFBQkIKCgs5DZzWLj8zqgMzMTF133XUKCQlR48aNdccdd2jPnj2SpG+++UZeXl764IMPdNNNN6lBgwbq0KGDsrKy3Pbx8ssvKyoqSg0aNNAf/vAHPf/88yf93MhHH32kjh07yt/fXy1bttTjjz+uY8eOWeNeXl6aMWOGfv/73yswMFBPPfXUeT/23+q9995Tu3btFBAQoMaNGyshIUGHDx/W5s2b1b17dzVp0kTBwcG64YYb9Pnnn7ttu2vXLnXr1k3+/v6Ki4vTsmXL3MbP9lyvXbtW119/vQICAhQVFaXhw4fr8OHD1vj06dPVunVr+fv7Kzw8XH369Dlj/xeyEz8yKy0t1fDhwxUWFiZ/f39dd9112rx5s6TjL46tWrXSs88+67Z9Tk6OvLy8tHv37tpuvU44ePCg+vfvr0aNGqlBgwa67bbbtGvXLknHfysqICBAixcvdttm/vz5atiwoX755RdJ0t69e/XHP/5RISEhCg0NVc+ePfXNN9/U9qF4xIABA/TQQw+poKBAXl5euuSSS1RZWakJEyYoJiZGAQEB6tChg9577z1rm4qKCqWkpFjjbdq00ZQpU07ab69evfTUU08pMjJSbdq0qe1DOye/9lpzqo9re/XqpQEDBliPL7nkEj3xxBPq37+/HA6HhgwZYr0+vv3227r22mvl7++vK664QqtXr7a2W7Vqlby8vLR48WLFx8fLz89Pa9euPekjs1WrVumaa65RYGCgQkJC1LVrV3377bfW+Jneq86bc/8teZyr9957z7z//vtm165d5osvvjB33nmnadeunamoqDD5+flGkmnbtq1ZuHChycvLM3369DHR0dGmvLzcGGPM2rVrjbe3t5k0aZLJy8sz06ZNM6GhoSY4ONh6jjVr1hiHw2EyMjLMnj17zNKlS80ll1xixo0bZ9VIMmFhYea1114ze/bsMd9++21tn4rT2rdvn/Hx8THPP/+8yc/PN19++aWZNm2a+fnnn83y5cvNm2++aXbs2GFyc3NNSkqKCQ8PNy6XyxhjTEVFhbniiivMLbfcYnJycszq1avNVVddZSSZ+fPnG2PMWZ3r3bt3m8DAQDN58mTz1VdfmXXr1pmrrrrKDBgwwBhjzObNm029evXM3LlzzTfffGM+//xzM2XKlDP2fyFLTk42PXv2NMYYM3z4cBMZGWn+85//mO3bt5vk5GTTqFEj8+OPPxpjjHnqqadMXFyc2/bDhw833bp1q+22PeqGG24wf/3rX40xxvz+9783sbGxZs2aNSYnJ8ckJiaaVq1ambKyMmOMMX369DF/+tOf3Lbv3bu3ta6srMzExsaagQMHmi+//NLk5uaa+++/37Rp08aUlpbW6nF5QnFxsRk/frxp3ry52b9/vzlw4IB58sknTdu2bU1mZqbZs2ePmT17tvHz8zOrVq0yxhw/Z+np6Wbz5s3m66+/Nm+99ZZp0KCBmTdvnrXf5ORkExQUZPr162e2bdtmtm3b5qlD/M1O91pz4t9elZ49e5rk5GTrcXR0tHE4HObZZ581u3fvNrt377ZeH5s3b27ee+89k5ubawYNGmQaNmxofvjhB2OMMStXrjSSTPv27c3SpUvN7t27zY8//mgee+wx06FDB2OMMeXl5SY4ONg8/PDDZvfu3SY3N9dkZGRY7zdn8151vhCI6qDvv//eSDJbt261/ghfeeUVa3z79u1GktmxY4cxxph77rnHJCUlue2jb9++boHolltuMU8//bRbzZtvvmmaNWtmPZZkRowYcR6OqGZkZ2cbSeabb745Y21FRYVp2LChWbBggTHGmCVLlhgfHx/z3XffWTWLFy8+ZSA63blOSUkxQ4YMcXuuTz/91Hh7e5sjR46Y999/3zgcDiuIVbf/C0lVIDp06JCpX7++mTNnjjVWVlZmIiMjzcSJE40xxnz33XemXr16ZuPGjdZ4kyZNTEZGhkd695SqN6WvvvrKSDLr1q2zxn744QcTEBBg3nnnHWOMMfPnzzdBQUHm8OHDxhhjSkpKjL+/v1m8eLEx5vj/x23atDGVlZXWPkpLS01AQIBZsmRJLR6V50yePNlER0cbY4w5evSoadCggVm/fr1bTUpKirnvvvt+dR+pqammd+/e1uPk5GQTHh5+QYbK073WnG0g6tWrl1tN1evjM888Y60rLy83zZs3N//85z+NMf8fiD788EO3bU8MRD/++KORZIXT/3U271XnCx+Z1QG7du3Sfffdp5YtW8rhcOiSSy6RJBUUFFg17du3t/7drFkzSdKBAwckSXl5ebrmmmvc9vm/j7ds2aLx48dbn+UGBQVp8ODB2r9/v3XZXZI6depUo8dWkzp06KBbbrlF7dq10913362XX35ZBw8elCQVFRVp8ODBat26tYKDg+VwOHTo0CHrHO7YsUNRUVGKjIy09ud0Ok/5PKc711u2bFFGRobbeUxMTFRlZaXy8/PVvXt3RUdHq2XLlurXr5/mzJljnd/T9X8x2LNnj8rLy9W1a1drXf369XXNNddox44dkqTIyEglJSXptddekyQtWLBApaWluvvuuz3Ss6ft2LFDPj4+6ty5s7WucePGatOmjXXObr/9dtWvX18ff/yxJOn999+Xw+FQQkKCpON/k7t371bDhg2tv8nQ0FAdPXrU+ujdTnbv3q1ffvlF3bt3d/v/9I033nA7H9OmTVN8fLyaNm2qoKAgzZo1y+01V5LatWt3Qc4bqonXml97LzjxddPHx0edOnWy/lbPtK0khYaGasCAAUpMTNSdd96pKVOmaP/+/db42b5XnQ8Eojrgzjvv1E8//aSXX35ZGzdu1MaNGyUdn8hW5cTJzV5eXpKkysrKs36OQ4cO6fHHH1dOTo61bN26Vbt27ZK/v79VFxgYeK6Hc97Uq1dPy5Yt0+LFixUXF6cXX3xRbdq0UX5+vpKTk5WTk6MpU6Zo/fr1ysnJUePGjd3O4dk63bk+dOiQ/vznP7udxy1btmjXrl269NJL1bBhQ33++ef697//rWbNmik9PV0dOnRQcXHxafu3k0GDBuntt9/WkSNHNHv2bN1zzz1q0KCBp9uqs3x9fdWnTx/NnTtXkjR37lzdc8898vE5/p2YQ4cOKT4+3u1vMicnR1999ZXuv/9+T7buEYcOHZIkLVq0yO185ObmWvOI3n77bT388MNKSUnR0qVLlZOTowceeOCk14u6/Hp4Oqd7rfH29j7pW6Hl5eUn7eNcjv1M286ePVtZWVm69tprNW/ePF122WXasGGDpLN/rzof+JaZh/3444/Ky8vTyy+/rOuvv17S8Um7v0WbNm2siatV/vdxx44dlZeXp1atWp1bwx7m5eWlrl27qmvXrkpPT1d0dLTmz5+vdevWafr06br99tslHZ9k+sMPP1jbxcbGau/evdq/f7911afqf8DfomPHjsrNzT3tefTx8VFCQoISEhL02GOPKSQkRCtWrNBdd931q/2npaX95l7qmksvvVS+vr5at26doqOjJR1/od28ebPbJM7bb79dgYGBmjFjhjIzM7VmzRoPdex5sbGxOnbsmDZu3Gh9Q6rqNSEuLs6q69u3r7p3767t27drxYoVevLJJ62xjh07at68eQoLC5PD4aj1Y6hr4uLi5Ofnp4KCAt1www2nrFm3bp2uvfZa/eUvf7HWXWxX037ttaZp06ZuV2QqKiq0bds23XTTTWe13w0bNqhbt26SpGPHjik7O7taP5p+1VVX6aqrrtLYsWPldDo1d+5cdenSxaPvVQQiD2vUqJEaN26sWbNmqVmzZiooKNAjjzzym/bx0EMPqVu3bnr++ed15513asWKFVq8eLF1dUOS0tPTdccdd6hFixbq06ePvL29tWXLFm3bts3txbUu27hxo5YvX64ePXooLCxMGzdu1Pfff6/Y2Fi1bt1ab775pjp16iSXy6VRo0YpICDA2jYhIUGXXXaZkpOTNWnSJLlcLv3973//zT2MGTNGXbp00bBhwzRo0CAFBgYqNzdXy5Yt00svvaSFCxfq66+/Vrdu3dSoUSP95z//UWVlpdq0aXPa/i8GgYGBGjp0qEaNGqXQ0FC1aNFCEydO1C+//KKUlBSrrl69ehowYIDGjh2r1q1b/+pHl3bQunVr9ezZU4MHD9a//vUvNWzYUI888oh+97vfqWfPnlZdt27dFBERob59+yomJsbtI7a+fftq0qRJ6tmzp8aPH6/mzZvr22+/1QcffKDRo0erefPmnjg0j2nYsKEefvhhjRw5UpWVlbruuutUUlKidevWyeFwKDk5Wa1bt9Ybb7yhJUuWKCYmRm+++aY2b96smJgYT7dfI073WhMYGKi0tDQtWrRIl156qZ5//nkVFxef9b6nTZum1q1bKzY2VpMnT9bBgwc1cODAs94+Pz9fs2bN0u9//3tFRkYqLy9Pu3btUv/+/SV5+L3qvM9SwhktW7bMxMbGGj8/P9O+fXuzatUqa7Jv1US2L774wqo/ePCgkWRWrlxprZs1a5b53e9+ZwICAkyvXr3Mk08+aSIiItyeJzMz01x77bUmICDAOBwOc80115hZs2ZZ4zphgnFdlJubaxITE03Tpk2Nn5+fueyyy8yLL75ojDHm888/N506dTL+/v6mdevW5t133zXR0dFm8uTJ1vZ5eXnmuuuuM76+vuayyy4zmZmZp5xUfaZzvWnTJtO9e3cTFBRkAgMDTfv27c1TTz1ljDk+wfqGG24wjRo1MgEBAaZ9+/bWN1dO1/+F7MRvmR05csQ89NBDpkmTJsbPz8907drVbNq06aRt9uzZYyRZk63t5sSJrT/99JPp16+fCQ4ONgEBASYxMdF89dVXJ20zevRoI8mkp6efNLZ//37Tv39/67y3bNnSDB482JSUlJzvQ6kTTpxUbYwxlZWV5oUXXjBt2rQx9evXN02bNjWJiYlm9erVxpjjE68HDBhggoODTUhIiBk6dKh55JFHrIm/xrj/XV9oTvdaU1ZWZoYOHWpCQ0NNWFiYmTBhwiknVZ/42mnM/78+zp0711xzzTXG19fXxMXFmRUrVlg1VZOqDx486LbtiZOqCwsLTa9evUyzZs2Mr6+viY6ONunp6aaiosKqP9N71fniZQy3mL0YDR48WDt37tSnn37q6VZwkbvvvvtUr149vfXWW2e9zaeffqpbbrlFe/fuVXh4+HnsDkBN+OabbxQTE6MvvvjigvgZjupgUvVF4tlnn7W+bfLiiy/q9ddfV3JysqfbwkXs2LFjys3NVVZWli6//PKz2qa0tFT//e9/NW7cON19992EIQB1BoHoIrFp0yZ1795d7dq108yZMzV16lQNGjTI023hIrZt2zZ16tRJl19+uR588MGz2ubf//63oqOjVVxcrIkTJ57nDgHg7PGRGQAAsD2uEAEAANsjEAEAANsjEAEAANsjEAEAANsjEAG4KNx4441uPxFyOqtWrZKXl9dvukPvqVxyySV64YUXzmkfAOoGAhEAALA9AhEAALA9AhGAi07VD/02bNhQERERuv/++3XgwIGT6tatW6f27dvL399fXbp00bZt29zG165dq+uvv14BAQGKiorS8OHDdfjw4do6DAC1iEAE4KJTXl6uJ554Qlu2bNGHH36ob775RgMGDDipbtSoUXruuee0efNmNW3aVHfeeafKy8slSXv27NGtt96q3r1768svv9S8efO0du1aDRs2rJaPBkBt8PF0AwBQ0wYOHGj9u2XLlpo6daquvvpqHTp0SEFBQdbYY489pu7du0uSXn/9dTVv3lzz58/XH//4R02YMEF9+/a1Jmq3bt1aU6dO1Q033KAZM2bI39+/Vo8JwPnFFSIAF53s7GzdeeedatGihRo2bKgbbrhBklRQUOBW53Q6rX+HhoaqTZs22rFjhyRpy5YtysjIUFBQkLUkJiaqsrJS+fn5tXcwAGoFV4gAXFQOHz6sxMREJSYmas6cOWratKkKCgqUmJiosrKys97PoUOH9Oc//1nDhw8/aaxFixY12TKAOoBABOCisnPnTv3444965plnFBUVJUn67LPPTlm7YcMGK9wcPHhQX331lWJjYyVJHTt2VG5urlq1alU7jQPwKD4yA3BRadGihXx9ffXiiy/q66+/1scff6wnnnjilLXjx4/X8uXLtW3bNg0YMEBNmjRRr169JEljxozR+vXrNWzYMOXk5GjXrl366KOPmFQNXKQIRAAuKk2bNlVGRobeffddxcXF6ZlnntGzzz57ytpnnnlGf/3rXxUfH6/CwkItWLBAvr6+kqT27dtr9erV+uqrr3T99dfrqquuUnp6uiIjI2vzcADUEi9jjPF0EwAAAJ7EFSIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7BCIAAGB7/wcMreqrqDM/eAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = get_file()\n",
    "\n",
    "sns.countplot(plot, x='label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to separate these 6 emotions into 3 categories because some of the feelings are similar. These categories are positive, negative and neutral. We want to group the emotions with corresponding category. \n",
    "This means we can re-label to new categories:\n",
    "\n",
    "* Negative\n",
    "    - 'anger'\n",
    "    - 'sadness'\n",
    "    - 'fear'\n",
    "\n",
    "* Neutral\n",
    "    - 'surprise'\n",
    "\n",
    "* Positive\n",
    "    - 'joy'\n",
    "    - 'love'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df =  get_file()\n",
    "\n",
    "label_map = {\n",
    "    'anger': 'Negative',\n",
    "    'sadness': 'Negative',\n",
    "    'fear': 'Negative',\n",
    "    'surprise': 'Neutral',\n",
    "    'joy': 'Positive',\n",
    "    'love': 'Positive',\n",
    "}\n",
    "\n",
    "df['label'] = df['label'].map(label_map) # changes the names of the labels to suit the 3 classes\n",
    "\n",
    "sns.countplot(df, x='label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the dataset is not balanced. So this might be an issue that might need to be investigated further if the models tends to be biased towards negative feelings. It might also introduce some challenges with the evaluation since it is not even. \n",
    "\n",
    "\n",
    "\n",
    "* TODO: Maybe check what the shortest amount of words is, and the most amount of the words in the data is and number of occurences. This is to know that padding the data is not introducing any dilation to my network and messes with the tanh acitvation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel so pissed off over an old friend and some friends</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i also feel it is unfortunate that nearly all the readers of going to meet the man will be african americans unlike myself</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel petty a clairee</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i used to believe that a feel like fear was to be ignored or suppressed right away more on this in a moment</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>i was i might be buying stuff from there but i feel the clothes are too casual</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>i like sonam deepika and genelia who i feel are very talented and beautiful</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>i feel pathetic that i can hardly go a whole day not talking to him</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>i would have spent more time with her on reading i feel a bit guilty about that</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>i do however feel like one of those pathetic girls who make up excuses because of a guy</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0                                                                                      i feel so pissed off over an old friend and some friends   \n",
       "1      i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated   \n",
       "2                    i also feel it is unfortunate that nearly all the readers of going to meet the man will be african americans unlike myself   \n",
       "3                                                                                                                        i feel petty a clairee   \n",
       "4                                   i used to believe that a feel like fear was to be ignored or suppressed right away more on this in a moment   \n",
       "...                                                                                                                                         ...   \n",
       "19995                                                            i was i might be buying stuff from there but i feel the clothes are too casual   \n",
       "19996                                                               i like sonam deepika and genelia who i feel are very talented and beautiful   \n",
       "19997                                                                       i feel pathetic that i can hardly go a whole day not talking to him   \n",
       "19998                                                           i would have spent more time with her on reading i feel a bit guilty about that   \n",
       "19999                                                   i do however feel like one of those pathetic girls who make up excuses because of a guy   \n",
       "\n",
       "          label  \n",
       "0      Negative  \n",
       "1      Negative  \n",
       "2      Negative  \n",
       "3      Negative  \n",
       "4      Negative  \n",
       "...         ...  \n",
       "19995  Positive  \n",
       "19996  Positive  \n",
       "19997  Negative  \n",
       "19998  Negative  \n",
       "19999  Negative  \n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_clean_text(df):\n",
    "    \"\"\"\"\n",
    "    Takes in a df column \"text\" and:\n",
    "    - converts everything to lowercase\n",
    "    - removes punctuation\n",
    "    - removes extra whitespaces\n",
    "    - adjustes words or removes them\n",
    "    \n",
    "    Returns: cleaned df\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    full_word_mapping = {\n",
    "        r'\\bid\\b': 'i would',\n",
    "        r'\\bim\\b': 'i am',\n",
    "        r'\\bive\\b': 'i have',\n",
    "        r'\\bill\\b': 'i will',\n",
    "        r'\\bdont\\b': 'do not',\n",
    "        r'\\bwoulde\\b': 'would have',\n",
    "        r'\\bfeeling\\b': 'feel',\n",
    "        r'\\bfelt\\b': 'feel',\n",
    "        r'\\bi havent\\b': 'i have not',\n",
    "        r'\\bamportant\\b': 'important',\n",
    "    }\n",
    "\n",
    "    words_to_remove = {\n",
    "        'href',\n",
    "        'ti',\n",
    "        'wi',\n",
    "        'hi',\n",
    "        'sti',\n",
    "        'ames',\n",
    "        'le',\n",
    "        'gi',\n",
    "        'si',\n",
    "        'http',\n",
    "        'href',\n",
    "    }\n",
    "\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(word for word in x.split() if word not in words_to_remove)) # applies and removes the words in the set that are not needed because they are shit\n",
    "\n",
    "    df['text'] = df['text'].replace(full_word_mapping, regex=True) # replaces the words to its full form\n",
    "\n",
    "    df['text'] = df['text'].str.lower() # makes everything lowercase\n",
    "\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(x.split())) # removes the extra whitespaces\n",
    "\n",
    "    df['text'] = df['text'].str.replace(f'[{string.punctuation}]', '', regex=True) # removes the punctuation\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "get_clean_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# # Combine the lists of tokenized text into a single string\n",
    "# combined_text = \" \".join(df[\"tokenized_text\"].apply(\" \".join))\n",
    "\n",
    "# # Use Counter to count the most common words\n",
    "# common_words = Counter(combined_text.split()).most_common(100)\n",
    "\n",
    "# # Print the most common words and their counts\n",
    "# print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# # Combine the lists of tokenized text into a single string\n",
    "# combined_text = \" \".join(df[\"tokenized_text\"].apply(\" \".join))\n",
    "\n",
    "# # Use Counter to count the occurrences of words\n",
    "# word_counts = Counter(combined_text.split())\n",
    "\n",
    "# # Sort the items in ascending order based on count\n",
    "# least_common_words = sorted(word_counts.items(), key=lambda x: x[1])[:100]\n",
    "\n",
    "# # Print the least common words and their counts\n",
    "# print(least_common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel so pissed off over an old friend and some friends</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, feel, so, pissed, off, over, an, old, friend, and, some, friends]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, have, found, it, has, made, a, huge, difference, especially, on, the, finger, with, my, ring, and, the, my, skin, feels, so, much, softer, and, less, irritated]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i also feel it is unfortunate that nearly all the readers of going to meet the man will be african americans unlike myself</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, also, feel, it, is, unfortunate, that, nearly, all, the, readers, of, going, to, meet, the, man, will, be, african, americans, unlike, myself]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel petty a clairee</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, feel, petty, a, clairee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i used to believe that a feel like fear was to be ignored or suppressed right away more on this in a moment</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, used, to, believe, that, a, feel, like, fear, was, to, be, ignored, or, suppressed, right, away, more, on, this, in, a, moment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>i was i might be buying stuff from there but i feel the clothes are too casual</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[i, was, i, might, be, buying, stuff, from, there, but, i, feel, the, clothes, are, too, casual]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>i like sonam deepika and genelia who i feel are very talented and beautiful</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[i, like, sonam, deepika, and, genelia, who, i, feel, are, very, talented, and, beautiful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>i feel pathetic that i can hardly go a whole day not talking to him</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, feel, pathetic, that, i, can, hardly, go, a, whole, day, not, talking, to, him]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>i would have spent more time with her on reading i feel a bit guilty about that</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, would, have, spent, more, time, with, her, on, reading, i, feel, a, bit, guilty, about, that]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>i do however feel like one of those pathetic girls who make up excuses because of a guy</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, do, however, feel, like, one, of, those, pathetic, girls, who, make, up, excuses, because, of, a, guy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0                                                                                      i feel so pissed off over an old friend and some friends   \n",
       "1      i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated   \n",
       "2                    i also feel it is unfortunate that nearly all the readers of going to meet the man will be african americans unlike myself   \n",
       "3                                                                                                                        i feel petty a clairee   \n",
       "4                                   i used to believe that a feel like fear was to be ignored or suppressed right away more on this in a moment   \n",
       "...                                                                                                                                         ...   \n",
       "19995                                                            i was i might be buying stuff from there but i feel the clothes are too casual   \n",
       "19996                                                               i like sonam deepika and genelia who i feel are very talented and beautiful   \n",
       "19997                                                                       i feel pathetic that i can hardly go a whole day not talking to him   \n",
       "19998                                                           i would have spent more time with her on reading i feel a bit guilty about that   \n",
       "19999                                                   i do however feel like one of those pathetic girls who make up excuses because of a guy   \n",
       "\n",
       "          label  \\\n",
       "0      Negative   \n",
       "1      Negative   \n",
       "2      Negative   \n",
       "3      Negative   \n",
       "4      Negative   \n",
       "...         ...   \n",
       "19995  Positive   \n",
       "19996  Positive   \n",
       "19997  Negative   \n",
       "19998  Negative   \n",
       "19999  Negative   \n",
       "\n",
       "                                                                                                                                                             tokenized_text  \n",
       "0                                                                                                     [i, feel, so, pissed, off, over, an, old, friend, and, some, friends]  \n",
       "1      [i, have, found, it, has, made, a, huge, difference, especially, on, the, finger, with, my, ring, and, the, my, skin, feels, so, much, softer, and, less, irritated]  \n",
       "2                        [i, also, feel, it, is, unfortunate, that, nearly, all, the, readers, of, going, to, meet, the, man, will, be, african, americans, unlike, myself]  \n",
       "3                                                                                                                                              [i, feel, petty, a, clairee]  \n",
       "4                                       [i, used, to, believe, that, a, feel, like, fear, was, to, be, ignored, or, suppressed, right, away, more, on, this, in, a, moment]  \n",
       "...                                                                                                                                                                     ...  \n",
       "19995                                                                      [i, was, i, might, be, buying, stuff, from, there, but, i, feel, the, clothes, are, too, casual]  \n",
       "19996                                                                            [i, like, sonam, deepika, and, genelia, who, i, feel, are, very, talented, and, beautiful]  \n",
       "19997                                                                                   [i, feel, pathetic, that, i, can, hardly, go, a, whole, day, not, talking, to, him]  \n",
       "19998                                                                     [i, would, have, spent, more, time, with, her, on, reading, i, feel, a, bit, guilty, about, that]  \n",
       "19999                                                            [i, do, however, feel, like, one, of, those, pathetic, girls, who, make, up, excuses, because, of, a, guy]  \n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(df):\n",
    "    \"\"\"\"\n",
    "    Takes in a df column\n",
    "\n",
    "    Returns: column with the text tokenized and each row is corresponding to correct index\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    df['tokenized_text'] = df['text'].apply(nltk.word_tokenize) # tokenizes the text\n",
    "\n",
    "    # tokenized_text = df['tokenized_text'].explode().reset_index() # transform each element and replicate index values to match each row. \n",
    "\n",
    "\n",
    "    # display(tokenized_text['index'].unique()) # this is to see that the index matches the original df rows (20K)\n",
    "\n",
    "    return df\n",
    "\n",
    "tokenize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df, STOPWORDS_DEFAULT):\n",
    "    \"\"\" Remove stopwords from the tokenized text (df) \"\"\"\n",
    "\n",
    "    df['tokenized_text'] = df['tokenized_text'].apply(lambda words: [word for word in words if word not in STOPWORDS_DEFAULT])\n",
    "\n",
    "    return\n",
    "\n",
    "remove_stopwords(df, STOPWORDS_DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel so pissed off over an old friend and some friends</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[feel, pissed, old, friend, friends]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[found, made, huge, difference, especially, finger, ring, skin, feels, much, softer, less, irritated]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i also feel it is unfortunate that nearly all the readers of going to meet the man will be african americans unlike myself</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[also, feel, unfortunate, nearly, readers, going, meet, man, african, americans, unlike]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel petty a clairee</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[feel, petty, clairee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i used to believe that a feel like fear was to be ignored or suppressed right away more on this in a moment</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[used, believe, feel, like, fear, ignored, suppressed, right, away, moment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>i was i might be buying stuff from there but i feel the clothes are too casual</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[might, buying, stuff, feel, clothes, casual]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>i like sonam deepika and genelia who i feel are very talented and beautiful</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[like, sonam, deepika, genelia, feel, talented, beautiful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>i feel pathetic that i can hardly go a whole day not talking to him</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[feel, pathetic, hardly, go, whole, day, talking]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>i would have spent more time with her on reading i feel a bit guilty about that</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[would, spent, time, reading, feel, bit, guilty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>i do however feel like one of those pathetic girls who make up excuses because of a guy</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[however, feel, like, one, pathetic, girls, make, excuses, guy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0                                                                                      i feel so pissed off over an old friend and some friends   \n",
       "1      i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated   \n",
       "2                    i also feel it is unfortunate that nearly all the readers of going to meet the man will be african americans unlike myself   \n",
       "3                                                                                                                        i feel petty a clairee   \n",
       "4                                   i used to believe that a feel like fear was to be ignored or suppressed right away more on this in a moment   \n",
       "...                                                                                                                                         ...   \n",
       "19995                                                            i was i might be buying stuff from there but i feel the clothes are too casual   \n",
       "19996                                                               i like sonam deepika and genelia who i feel are very talented and beautiful   \n",
       "19997                                                                       i feel pathetic that i can hardly go a whole day not talking to him   \n",
       "19998                                                           i would have spent more time with her on reading i feel a bit guilty about that   \n",
       "19999                                                   i do however feel like one of those pathetic girls who make up excuses because of a guy   \n",
       "\n",
       "          label  \\\n",
       "0      Negative   \n",
       "1      Negative   \n",
       "2      Negative   \n",
       "3      Negative   \n",
       "4      Negative   \n",
       "...         ...   \n",
       "19995  Positive   \n",
       "19996  Positive   \n",
       "19997  Negative   \n",
       "19998  Negative   \n",
       "19999  Negative   \n",
       "\n",
       "                                                                                              tokenized_text  \n",
       "0                                                                       [feel, pissed, old, friend, friends]  \n",
       "1      [found, made, huge, difference, especially, finger, ring, skin, feels, much, softer, less, irritated]  \n",
       "2                   [also, feel, unfortunate, nearly, readers, going, meet, man, african, americans, unlike]  \n",
       "3                                                                                     [feel, petty, clairee]  \n",
       "4                                [used, believe, feel, like, fear, ignored, suppressed, right, away, moment]  \n",
       "...                                                                                                      ...  \n",
       "19995                                                          [might, buying, stuff, feel, clothes, casual]  \n",
       "19996                                             [like, sonam, deepika, genelia, feel, talented, beautiful]  \n",
       "19997                                                      [feel, pathetic, hardly, go, whole, day, talking]  \n",
       "19998                                                       [would, spent, time, reading, feel, bit, guilty]  \n",
       "19999                                        [however, feel, like, one, pathetic, girls, make, excuses, guy]  \n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "* Reduce the words to their base form. for instance \"running\" -> \"run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel so pissed off over an old friend and some friends</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[feel, pissed, old, friend, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[found, made, huge, difference, especially, finger, ring, skin, feel, much, softer, le, irritated]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i also feel it is unfortunate that nearly all the readers of going to meet the man will be african americans unlike myself</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[also, feel, unfortunate, nearly, reader, going, meet, man, african, american, unlike]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel petty a clairee</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[feel, petty, clairee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i used to believe that a feel like fear was to be ignored or suppressed right away more on this in a moment</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[used, believe, feel, like, fear, ignored, suppressed, right, away, moment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>i was i might be buying stuff from there but i feel the clothes are too casual</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[might, buying, stuff, feel, clothes, casual]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>i like sonam deepika and genelia who i feel are very talented and beautiful</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[like, sonam, deepika, genelia, feel, talented, beautiful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>i feel pathetic that i can hardly go a whole day not talking to him</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[feel, pathetic, hardly, go, whole, day, talking]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>i would have spent more time with her on reading i feel a bit guilty about that</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[would, spent, time, reading, feel, bit, guilty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>i do however feel like one of those pathetic girls who make up excuses because of a guy</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[however, feel, like, one, pathetic, girl, make, excuse, guy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0                                                                                      i feel so pissed off over an old friend and some friends   \n",
       "1      i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated   \n",
       "2                    i also feel it is unfortunate that nearly all the readers of going to meet the man will be african americans unlike myself   \n",
       "3                                                                                                                        i feel petty a clairee   \n",
       "4                                   i used to believe that a feel like fear was to be ignored or suppressed right away more on this in a moment   \n",
       "...                                                                                                                                         ...   \n",
       "19995                                                            i was i might be buying stuff from there but i feel the clothes are too casual   \n",
       "19996                                                               i like sonam deepika and genelia who i feel are very talented and beautiful   \n",
       "19997                                                                       i feel pathetic that i can hardly go a whole day not talking to him   \n",
       "19998                                                           i would have spent more time with her on reading i feel a bit guilty about that   \n",
       "19999                                                   i do however feel like one of those pathetic girls who make up excuses because of a guy   \n",
       "\n",
       "          label  \\\n",
       "0      Negative   \n",
       "1      Negative   \n",
       "2      Negative   \n",
       "3      Negative   \n",
       "4      Negative   \n",
       "...         ...   \n",
       "19995  Positive   \n",
       "19996  Positive   \n",
       "19997  Negative   \n",
       "19998  Negative   \n",
       "19999  Negative   \n",
       "\n",
       "                                                                                           tokenized_text  \n",
       "0                                                                     [feel, pissed, old, friend, friend]  \n",
       "1      [found, made, huge, difference, especially, finger, ring, skin, feel, much, softer, le, irritated]  \n",
       "2                  [also, feel, unfortunate, nearly, reader, going, meet, man, african, american, unlike]  \n",
       "3                                                                                  [feel, petty, clairee]  \n",
       "4                             [used, believe, feel, like, fear, ignored, suppressed, right, away, moment]  \n",
       "...                                                                                                   ...  \n",
       "19995                                                       [might, buying, stuff, feel, clothes, casual]  \n",
       "19996                                          [like, sonam, deepika, genelia, feel, talented, beautiful]  \n",
       "19997                                                   [feel, pathetic, hardly, go, whole, day, talking]  \n",
       "19998                                                    [would, spent, time, reading, feel, bit, guilty]  \n",
       "19999                                       [however, feel, like, one, pathetic, girl, make, excuse, guy]  \n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize(df, lemmatizer):\n",
    "    \"\"\" Lemmatize list of words in the tokenized_text column df \"\"\"\n",
    "\n",
    "    df['tokenized_text'] = df['tokenized_text'].apply(lambda words: [lemmatizer.lemmatize(word) for word in words])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "lemmatize(df, LEMMATIZER)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign a unique number to each unique word\n",
    "\n",
    "- We need to assign a unique number to each unique word to vectorize it.\n",
    "- Also i change the labels positive/neutral/negative to numerical numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in the text: 15091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clicked': 0,\n",
       " 'ditched': 1,\n",
       " 'whipping': 2,\n",
       " 'risk': 3,\n",
       " 'notebook': 4,\n",
       " 'torned': 5,\n",
       " 'ounce': 6,\n",
       " 'midst': 7,\n",
       " 'caffeined': 8,\n",
       " 'bowler': 9,\n",
       " 'semester': 10,\n",
       " 'mindful': 11,\n",
       " 'admit': 12,\n",
       " 'mumbai': 13,\n",
       " 'crotch': 14,\n",
       " 'glitter': 15,\n",
       " 'psychologically': 16,\n",
       " 'rundown': 17,\n",
       " 'glum': 18,\n",
       " 'graphic': 19,\n",
       " 'seeming': 20,\n",
       " 'introvert': 21,\n",
       " 'resort': 22,\n",
       " 'ebony': 23,\n",
       " 'porcupine': 24,\n",
       " 'retrospect': 25,\n",
       " 'farther': 26,\n",
       " 'nude': 27,\n",
       " 'mill': 28,\n",
       " 'sorbet': 29,\n",
       " 'descend': 30,\n",
       " 'residential': 31,\n",
       " 'remote': 32,\n",
       " 'south': 33,\n",
       " 'bigged': 34,\n",
       " 'euphoria': 35,\n",
       " 'farm': 36,\n",
       " 'reiterate': 37,\n",
       " 'oyyy': 38,\n",
       " 'awkwardly': 39,\n",
       " 'waffle': 40,\n",
       " 'portia': 41,\n",
       " 'internet': 42,\n",
       " 'quo': 43,\n",
       " 'laugh': 44,\n",
       " 'categorie': 45,\n",
       " 'suburb': 46,\n",
       " 'flutter': 47,\n",
       " 'repetition': 48,\n",
       " 'pipsqueak': 49,\n",
       " 'downwards': 50,\n",
       " 'anti': 51,\n",
       " 'beautifully': 52,\n",
       " 'culture': 53,\n",
       " 'oddly': 54,\n",
       " 'consuming': 55,\n",
       " 'supposedly': 56,\n",
       " 'terror': 57,\n",
       " 'permitting': 58,\n",
       " 'bikini': 59,\n",
       " 'bryangregorylewis': 60,\n",
       " 'stumble': 61,\n",
       " 'bummin': 62,\n",
       " 'esque': 63,\n",
       " 'trauma': 64,\n",
       " 'skinned': 65,\n",
       " 'vt': 66,\n",
       " 'needy': 67,\n",
       " 'complete': 68,\n",
       " 'hedge': 69,\n",
       " 'mag': 70,\n",
       " 'waxed': 71,\n",
       " 'slighted': 72,\n",
       " 'bat': 73,\n",
       " 'unlikeable': 74,\n",
       " 'communicat': 75,\n",
       " 'doesnt': 76,\n",
       " 'lollipop': 77,\n",
       " 'hiring': 78,\n",
       " 'brassed': 79,\n",
       " 'jtwoo': 80,\n",
       " 'veryy': 81,\n",
       " 'upholstered': 82,\n",
       " 'dance': 83,\n",
       " 'masochist': 84,\n",
       " 'skate': 85,\n",
       " 'dayummm': 86,\n",
       " 'suffocate': 87,\n",
       " 'clearer': 88,\n",
       " 'undertaking': 89,\n",
       " 'hyperthyroidism': 90,\n",
       " 'snapback': 91,\n",
       " 'appreciates': 92,\n",
       " 'succumb': 93,\n",
       " 'allthingsbucks': 94,\n",
       " 'bogged': 95,\n",
       " 'platform': 96,\n",
       " 'logarythmic': 97,\n",
       " 'u': 98,\n",
       " 'etape': 99,\n",
       " 'cam': 100,\n",
       " 'valuable': 101,\n",
       " 'musicjuzz': 102,\n",
       " 'whiff': 103,\n",
       " 'website': 104,\n",
       " 'faced': 105,\n",
       " 'potter': 106,\n",
       " 'freind': 107,\n",
       " 'seeped': 108,\n",
       " 'rosa': 109,\n",
       " 'ppl': 110,\n",
       " 'hmm': 111,\n",
       " 'tempo': 112,\n",
       " 'subtly': 113,\n",
       " 'grieving': 114,\n",
       " 'reasonably': 115,\n",
       " 'muttering': 116,\n",
       " 'curriculum': 117,\n",
       " 'delhi': 118,\n",
       " 'drinking': 119,\n",
       " 'droppings': 120,\n",
       " 'disinfected': 121,\n",
       " 'jared': 122,\n",
       " 'abruptly': 123,\n",
       " 'steinbeck': 124,\n",
       " 'proudest': 125,\n",
       " 'feelingof': 126,\n",
       " 'hindu': 127,\n",
       " 'jeff': 128,\n",
       " 'moderately': 129,\n",
       " 'born': 130,\n",
       " 'powerless': 131,\n",
       " 'corpse': 132,\n",
       " 'joey': 133,\n",
       " 'guilted': 134,\n",
       " 'thicker': 135,\n",
       " 'dennis': 136,\n",
       " 'psyched': 137,\n",
       " 'clutched': 138,\n",
       " 'validation': 139,\n",
       " 'accusing': 140,\n",
       " 'rip': 141,\n",
       " 'lumia': 142,\n",
       " 'tower': 143,\n",
       " 'post': 144,\n",
       " 'provide': 145,\n",
       " 'prank': 146,\n",
       " 'dressing': 147,\n",
       " 'marketer': 148,\n",
       " 'lewis': 149,\n",
       " 'dominate': 150,\n",
       " 'usa': 151,\n",
       " 'dissassociated': 152,\n",
       " 'hotshot': 153,\n",
       " 'entrance': 154,\n",
       " 'gfs': 155,\n",
       " 'odor': 156,\n",
       " 'unkind': 157,\n",
       " 'wealth': 158,\n",
       " 'rmb': 159,\n",
       " 'ended': 160,\n",
       " 'podcast': 161,\n",
       " 'ep': 162,\n",
       " 'manifesting': 163,\n",
       " 'yearned': 164,\n",
       " 'boost': 165,\n",
       " 'devin': 166,\n",
       " 'plagued': 167,\n",
       " 'indulging': 168,\n",
       " 'predicted': 169,\n",
       " 'alliance': 170,\n",
       " 'whats': 171,\n",
       " 'regain': 172,\n",
       " 'rightful': 173,\n",
       " 'everyone': 174,\n",
       " 'freaked': 175,\n",
       " 'vow': 176,\n",
       " 'usual': 177,\n",
       " 'legitimately': 178,\n",
       " 'hospitalized': 179,\n",
       " 'principle': 180,\n",
       " 'http': 181,\n",
       " 'tweeting': 182,\n",
       " 'shakiness': 183,\n",
       " 'sum': 184,\n",
       " 'brown': 185,\n",
       " 'revising': 186,\n",
       " 'dependency': 187,\n",
       " 'programme': 188,\n",
       " 'profoundly': 189,\n",
       " 'respected': 190,\n",
       " 'akin': 191,\n",
       " 'unpaid': 192,\n",
       " 'fed': 193,\n",
       " 'possessing': 194,\n",
       " 'bombed': 195,\n",
       " 'supervisor': 196,\n",
       " 'weekly': 197,\n",
       " 'cutie': 198,\n",
       " 'production': 199,\n",
       " 'heard': 200,\n",
       " 'musician': 201,\n",
       " 'resentful': 202,\n",
       " 'tad': 203,\n",
       " 'recess': 204,\n",
       " 'clot': 205,\n",
       " 'backache': 206,\n",
       " 'restroom': 207,\n",
       " 'extremely': 208,\n",
       " 'simulation': 209,\n",
       " 'glanced': 210,\n",
       " 'fire': 211,\n",
       " 'constrained': 212,\n",
       " 'meltdown': 213,\n",
       " 'irc': 214,\n",
       " 'anyhow': 215,\n",
       " 'alternate': 216,\n",
       " 'padding': 217,\n",
       " 'carbohydrate': 218,\n",
       " 'woefully': 219,\n",
       " 'approx': 220,\n",
       " 'prominently': 221,\n",
       " 'beneficial': 222,\n",
       " 'weepy': 223,\n",
       " 'exposing': 224,\n",
       " 'bunny': 225,\n",
       " 'pacified': 226,\n",
       " 'beaubronz': 227,\n",
       " 'progesterone': 228,\n",
       " 'horizontal': 229,\n",
       " 'otherwise': 230,\n",
       " 'hormone': 231,\n",
       " 'funded': 232,\n",
       " 'myslef': 233,\n",
       " 'educator': 234,\n",
       " 'select': 235,\n",
       " 'contemplating': 236,\n",
       " 'mummy': 237,\n",
       " 'tribal': 238,\n",
       " 'jeanette': 239,\n",
       " 'inescapable': 240,\n",
       " 'flias': 241,\n",
       " 'rafael': 242,\n",
       " 'puzzled': 243,\n",
       " 'hurting': 244,\n",
       " 'legitimate': 245,\n",
       " 'eqafe': 246,\n",
       " 'sids': 247,\n",
       " 'conquer': 248,\n",
       " 'jazzed': 249,\n",
       " 'feedlinks': 250,\n",
       " 'wind': 251,\n",
       " 'homage': 252,\n",
       " 'mohammed': 253,\n",
       " 'emerges': 254,\n",
       " 'rain': 255,\n",
       " 'mechanism': 256,\n",
       " 'inequality': 257,\n",
       " 'quaker': 258,\n",
       " 'bye': 259,\n",
       " 'poorly': 260,\n",
       " 'hobby': 261,\n",
       " 'analyze': 262,\n",
       " 'tweedy': 263,\n",
       " 'pg': 264,\n",
       " 'upcoming': 265,\n",
       " 'disjointed': 266,\n",
       " 'loos': 267,\n",
       " 'democrat': 268,\n",
       " 'sarawak': 269,\n",
       " 'remembers': 270,\n",
       " 'swing': 271,\n",
       " 'hinterland': 272,\n",
       " 'deader': 273,\n",
       " 'surged': 274,\n",
       " 'omission': 275,\n",
       " 'shinae': 276,\n",
       " 'harmonious': 277,\n",
       " 'none': 278,\n",
       " 'sephora': 279,\n",
       " 'committed': 280,\n",
       " 'lovecraft': 281,\n",
       " 'unexpected': 282,\n",
       " 'gerling': 283,\n",
       " 'cherishing': 284,\n",
       " 'isi': 285,\n",
       " 'orissa': 286,\n",
       " 'weak': 287,\n",
       " 'date': 288,\n",
       " 'paratrooper': 289,\n",
       " 'privilege': 290,\n",
       " 'recless': 291,\n",
       " 'thaliad': 292,\n",
       " 'petal': 293,\n",
       " 'relocation': 294,\n",
       " 'richie': 295,\n",
       " 'enable': 296,\n",
       " 'treassured': 297,\n",
       " 'stumped': 298,\n",
       " 'persecuted': 299,\n",
       " 'depending': 300,\n",
       " 'defiled': 301,\n",
       " 'equation': 302,\n",
       " 'birthday': 303,\n",
       " 'punk': 304,\n",
       " 'childrens': 305,\n",
       " 'offset': 306,\n",
       " 'giver': 307,\n",
       " 'asasoulawakens': 308,\n",
       " 'rightness': 309,\n",
       " 'poppy': 310,\n",
       " 'torment': 311,\n",
       " 'signing': 312,\n",
       " 'fortnight': 313,\n",
       " 'visable': 314,\n",
       " 'bibliography': 315,\n",
       " 'burrowing': 316,\n",
       " 'bright': 317,\n",
       " 'projected': 318,\n",
       " 'mustached': 319,\n",
       " 'ally': 320,\n",
       " 'swag': 321,\n",
       " 'diagnosis': 322,\n",
       " 'dittmar': 323,\n",
       " 'breeder': 324,\n",
       " 'resilience': 325,\n",
       " 'sucker': 326,\n",
       " 'approximately': 327,\n",
       " 'insightful': 328,\n",
       " 'lathi': 329,\n",
       " 'patrimony': 330,\n",
       " 'decrease': 331,\n",
       " 'rang': 332,\n",
       " 'suicidal': 333,\n",
       " 'hardening': 334,\n",
       " 'tired': 335,\n",
       " 'incheswhyinches': 336,\n",
       " 'plate': 337,\n",
       " 'mechanical': 338,\n",
       " 'tartlet': 339,\n",
       " 'amish': 340,\n",
       " 'photoshoots': 341,\n",
       " 'numb': 342,\n",
       " 'teasing': 343,\n",
       " 'aryan': 344,\n",
       " 'bound': 345,\n",
       " 'methodically': 346,\n",
       " 'wrestle': 347,\n",
       " 'karen': 348,\n",
       " 'vivre': 349,\n",
       " 'wondered': 350,\n",
       " 'displayed': 351,\n",
       " 'gabapentin': 352,\n",
       " 'negotiator': 353,\n",
       " 'repression': 354,\n",
       " 'horned': 355,\n",
       " 'mae': 356,\n",
       " 'honestly': 357,\n",
       " 'keiko': 358,\n",
       " 'necessity': 359,\n",
       " 'sheri': 360,\n",
       " 'tween': 361,\n",
       " 'summed': 362,\n",
       " 'pairing': 363,\n",
       " 'sickened': 364,\n",
       " 'flung': 365,\n",
       " 'turtle': 366,\n",
       " 'chart': 367,\n",
       " 'watcher': 368,\n",
       " 'rec': 369,\n",
       " 'personal': 370,\n",
       " 'stiffen': 371,\n",
       " 'thrifty': 372,\n",
       " 'newrhinegargoyle': 373,\n",
       " 'prima': 374,\n",
       " 'meditative': 375,\n",
       " 'deny': 376,\n",
       " 'weewee': 377,\n",
       " 'gerry': 378,\n",
       " 'fangirls': 379,\n",
       " 'confidence': 380,\n",
       " 'eepctqlhiafjwnrrmas': 381,\n",
       " 'hubby': 382,\n",
       " 'swan': 383,\n",
       " 'paired': 384,\n",
       " 'bottom': 385,\n",
       " 'asa': 386,\n",
       " 'jim': 387,\n",
       " 'unloveable': 388,\n",
       " 'utter': 389,\n",
       " 'searched': 390,\n",
       " 'muas': 391,\n",
       " 'haagen': 392,\n",
       " 'cumberland': 393,\n",
       " 'stylings': 394,\n",
       " 'temporary': 395,\n",
       " 'reached': 396,\n",
       " 'von': 397,\n",
       " 'revered': 398,\n",
       " 'memorial': 399,\n",
       " 'sticking': 400,\n",
       " 'pagetitle': 401,\n",
       " 'age': 402,\n",
       " 'complication': 403,\n",
       " 'dismissing': 404,\n",
       " 'nd': 405,\n",
       " 'skdd': 406,\n",
       " 'michael': 407,\n",
       " 'anxiety': 408,\n",
       " 'persevere': 409,\n",
       " 'misunderstood': 410,\n",
       " 'stub': 411,\n",
       " 'counselor': 412,\n",
       " 'allah': 413,\n",
       " 'dismay': 414,\n",
       " 'headlock': 415,\n",
       " 'campus': 416,\n",
       " 'millennium': 417,\n",
       " 'susan': 418,\n",
       " 'becomes': 419,\n",
       " 'unreal': 420,\n",
       " 'poignant': 421,\n",
       " 'grader': 422,\n",
       " 'pokemon': 423,\n",
       " 'detatched': 424,\n",
       " 'turkish': 425,\n",
       " 'stsm': 426,\n",
       " 'known': 427,\n",
       " 'white': 428,\n",
       " 'weighing': 429,\n",
       " 'unused': 430,\n",
       " 'meditated': 431,\n",
       " 'trainable': 432,\n",
       " 'innovative': 433,\n",
       " 'seb': 434,\n",
       " 'mainstream': 435,\n",
       " 'icon': 436,\n",
       " 'bear': 437,\n",
       " 'yelling': 438,\n",
       " 'unfinished': 439,\n",
       " 'rockette': 440,\n",
       " 'adapted': 441,\n",
       " 'boomer': 442,\n",
       " 'dropped': 443,\n",
       " 'spazzing': 444,\n",
       " 'wale': 445,\n",
       " 'war': 446,\n",
       " 'temptation': 447,\n",
       " 'rainboots': 448,\n",
       " 'opiate': 449,\n",
       " 'lean': 450,\n",
       " 'tactic': 451,\n",
       " 'kinison': 452,\n",
       " 'fedotenko': 453,\n",
       " 'bastard': 454,\n",
       " 'cruise': 455,\n",
       " 'deep': 456,\n",
       " 'seek': 457,\n",
       " 'testosterone': 458,\n",
       " 'intimidate': 459,\n",
       " 'relying': 460,\n",
       " 'zack': 461,\n",
       " 'unless': 462,\n",
       " 'get': 463,\n",
       " 'eerily': 464,\n",
       " 'satisfy': 465,\n",
       " 'trinket': 466,\n",
       " 'banjo': 467,\n",
       " 'custom': 468,\n",
       " 'terrible': 469,\n",
       " 'terbutaline': 470,\n",
       " 'prohibited': 471,\n",
       " 'hesei': 472,\n",
       " 'muttered': 473,\n",
       " 'liv': 474,\n",
       " 'staying': 475,\n",
       " 'addictive': 476,\n",
       " 'dagger': 477,\n",
       " 'dannika': 478,\n",
       " 'invocation': 479,\n",
       " 'conscientious': 480,\n",
       " 'crib': 481,\n",
       " 'locking': 482,\n",
       " 'squirrel': 483,\n",
       " 'uwilnevrknow': 484,\n",
       " 'nicer': 485,\n",
       " 'forward': 486,\n",
       " 'anime': 487,\n",
       " 'chore': 488,\n",
       " 'army': 489,\n",
       " 'being': 490,\n",
       " 'blindly': 491,\n",
       " 'typically': 492,\n",
       " 'moist': 493,\n",
       " 'notcied': 494,\n",
       " 'gallivanting': 495,\n",
       " 'lightened': 496,\n",
       " 'cabbage': 497,\n",
       " 'cost': 498,\n",
       " 'rarely': 499,\n",
       " 'commodity': 500,\n",
       " 'illicit': 501,\n",
       " 'duck': 502,\n",
       " 'ink': 503,\n",
       " 'idealized': 504,\n",
       " 'merely': 505,\n",
       " 'beneath': 506,\n",
       " 'swarming': 507,\n",
       " 'majestic': 508,\n",
       " 'greeting': 509,\n",
       " 'neglecting': 510,\n",
       " 'sloth': 511,\n",
       " 'murder': 512,\n",
       " 'celeen': 513,\n",
       " 'swishing': 514,\n",
       " 'person': 515,\n",
       " 'tattoo': 516,\n",
       " 'subdue': 517,\n",
       " 'despise': 518,\n",
       " 'yarn': 519,\n",
       " 'photograph': 520,\n",
       " 'snarky': 521,\n",
       " 'oftentimes': 522,\n",
       " 'carapace': 523,\n",
       " 'sek': 524,\n",
       " 'arrival': 525,\n",
       " 'annual': 526,\n",
       " 'sluggish': 527,\n",
       " 'overthink': 528,\n",
       " 'lining': 529,\n",
       " 'chuffed': 530,\n",
       " 'gaia': 531,\n",
       " 'blubbering': 532,\n",
       " 'freshen': 533,\n",
       " 'hope': 534,\n",
       " 'pavement': 535,\n",
       " 'barbeque': 536,\n",
       " 'unbelievably': 537,\n",
       " 'listed': 538,\n",
       " 'commodore': 539,\n",
       " 'intention': 540,\n",
       " 'birth': 541,\n",
       " 'occasion': 542,\n",
       " 'three': 543,\n",
       " 'bitsy': 544,\n",
       " 'fergie': 545,\n",
       " 'duplex': 546,\n",
       " 'integrate': 547,\n",
       " 'gloss': 548,\n",
       " 'digg': 549,\n",
       " 'grinned': 550,\n",
       " 'travelling': 551,\n",
       " 'dabble': 552,\n",
       " 'japan': 553,\n",
       " 'spiteful': 554,\n",
       " 'stuff': 555,\n",
       " 'total': 556,\n",
       " 'united': 557,\n",
       " 'disease': 558,\n",
       " 'kidding': 559,\n",
       " 'macabre': 560,\n",
       " 'el': 561,\n",
       " 'clover': 562,\n",
       " 'alignment': 563,\n",
       " 'judy': 564,\n",
       " 'mihm': 565,\n",
       " 'relegated': 566,\n",
       " 'habitual': 567,\n",
       " 'unsatisfying': 568,\n",
       " 'consecutive': 569,\n",
       " 'female': 570,\n",
       " 'cat': 571,\n",
       " 'championed': 572,\n",
       " 'cooler': 573,\n",
       " 'lash': 574,\n",
       " 'inarticulate': 575,\n",
       " 'irresistable': 576,\n",
       " 'paper': 577,\n",
       " 'argument': 578,\n",
       " 'misunderstanding': 579,\n",
       " 'fin': 580,\n",
       " 'chlorine': 581,\n",
       " 'monogamous': 582,\n",
       " 'betraying': 583,\n",
       " 'chatting': 584,\n",
       " 'submits': 585,\n",
       " 'addison': 586,\n",
       " 'lick': 587,\n",
       " 'integrity': 588,\n",
       " 'barrier': 589,\n",
       " 'synthwave': 590,\n",
       " 'flex': 591,\n",
       " 'flinched': 592,\n",
       " 'newtown': 593,\n",
       " 'cyber': 594,\n",
       " 'computer': 595,\n",
       " 'depict': 596,\n",
       " 'nonstop': 597,\n",
       " 'tag': 598,\n",
       " 'glad': 599,\n",
       " 'complimented': 600,\n",
       " 'sabotaging': 601,\n",
       " 'hakodesh': 602,\n",
       " 'karate': 603,\n",
       " 'animated': 604,\n",
       " 'decoration': 605,\n",
       " 'nipple': 606,\n",
       " 'wealthy': 607,\n",
       " 'tightness': 608,\n",
       " 'plain': 609,\n",
       " 'reassured': 610,\n",
       " 'unintended': 611,\n",
       " 'thinking': 612,\n",
       " 'articulated': 613,\n",
       " 'bauer': 614,\n",
       " 'exited': 615,\n",
       " 'spendy': 616,\n",
       " 'author': 617,\n",
       " 'unwillingly': 618,\n",
       " 'buoied': 619,\n",
       " 'pervasive': 620,\n",
       " 'aint': 621,\n",
       " 'premature': 622,\n",
       " 'encounter': 623,\n",
       " 'minute': 624,\n",
       " 'foment': 625,\n",
       " 'oliver': 626,\n",
       " 'traveler': 627,\n",
       " 'mellow': 628,\n",
       " 'cherished': 629,\n",
       " 'wo': 630,\n",
       " 'homeschooled': 631,\n",
       " 'vertigo': 632,\n",
       " 'invented': 633,\n",
       " 'bothersome': 634,\n",
       " 'cuddle': 635,\n",
       " 'liver': 636,\n",
       " 'bent': 637,\n",
       " 'avoidance': 638,\n",
       " 'polar': 639,\n",
       " 'broodiness': 640,\n",
       " 'cast': 641,\n",
       " 'backpack': 642,\n",
       " 'cake': 643,\n",
       " 'ball': 644,\n",
       " 'believ': 645,\n",
       " 'sunset': 646,\n",
       " 'troubled': 647,\n",
       " 'dearwendy': 648,\n",
       " 'yknow': 649,\n",
       " 'chocolate': 650,\n",
       " 'acted': 651,\n",
       " 'eintrag': 652,\n",
       " 'dipped': 653,\n",
       " 'fragrance': 654,\n",
       " 'maker': 655,\n",
       " 'wig': 656,\n",
       " 'hijack': 657,\n",
       " 'deceived': 658,\n",
       " 'decreasing': 659,\n",
       " 'patel': 660,\n",
       " 'occurring': 661,\n",
       " 'vulturous': 662,\n",
       " 'butt': 663,\n",
       " 'vj': 664,\n",
       " 'craves': 665,\n",
       " 'couldve': 666,\n",
       " 'opinons': 667,\n",
       " 'advertorial': 668,\n",
       " 'hairdryer': 669,\n",
       " 'performance': 670,\n",
       " 'sho': 671,\n",
       " 'tension': 672,\n",
       " 'clothing': 673,\n",
       " 'tightly': 674,\n",
       " 'strongest': 675,\n",
       " 'multitasking': 676,\n",
       " 'communal': 677,\n",
       " 'repentant': 678,\n",
       " 'dickens': 679,\n",
       " 'depends': 680,\n",
       " 'strongly': 681,\n",
       " 'building': 682,\n",
       " 'crone': 683,\n",
       " 'frazzle': 684,\n",
       " 'blake': 685,\n",
       " 'linguistics': 686,\n",
       " 'hugging': 687,\n",
       " 'opposed': 688,\n",
       " 'lurking': 689,\n",
       " 'triangle': 690,\n",
       " 'owe': 691,\n",
       " 'conducive': 692,\n",
       " 'runny': 693,\n",
       " 'ward': 694,\n",
       " 'pervert': 695,\n",
       " 'lyman': 696,\n",
       " 'nutritional': 697,\n",
       " 'duvet': 698,\n",
       " 'gracious': 699,\n",
       " 'princess': 700,\n",
       " 'ayumi': 701,\n",
       " 'chill': 702,\n",
       " 'femme': 703,\n",
       " 'biofuels': 704,\n",
       " 'flab': 705,\n",
       " 'cx': 706,\n",
       " 'becuase': 707,\n",
       " 'ed': 708,\n",
       " 'liam': 709,\n",
       " 'uh': 710,\n",
       " 'sukkah': 711,\n",
       " 'benevolent': 712,\n",
       " 'jeannie': 713,\n",
       " 'vinegary': 714,\n",
       " 'siting': 715,\n",
       " 'bareminerals': 716,\n",
       " 'many': 717,\n",
       " 'downstream': 718,\n",
       " 'hay': 719,\n",
       " 'competing': 720,\n",
       " 'flying': 721,\n",
       " 'unbreakable': 722,\n",
       " 'raini': 723,\n",
       " 'dearest': 724,\n",
       " 'unspeakable': 725,\n",
       " 'feminine': 726,\n",
       " 'ann': 727,\n",
       " 'publicly': 728,\n",
       " 'tampon': 729,\n",
       " 'alumninium': 730,\n",
       " 'fluffed': 731,\n",
       " 'fuss': 732,\n",
       " 'verizon': 733,\n",
       " 'assuming': 734,\n",
       " 'illustrator': 735,\n",
       " 'tirade': 736,\n",
       " 'converting': 737,\n",
       " 'sailed': 738,\n",
       " 'unconvinced': 739,\n",
       " 'marjane': 740,\n",
       " 'stephanie': 741,\n",
       " 'molly': 742,\n",
       " 'happening': 743,\n",
       " 'max': 744,\n",
       " 'truthful': 745,\n",
       " 'suspicious': 746,\n",
       " 'autonomy': 747,\n",
       " 'umbrella': 748,\n",
       " 'sifting': 749,\n",
       " 'nellie': 750,\n",
       " 'joining': 751,\n",
       " 'imminent': 752,\n",
       " 'donning': 753,\n",
       " 'unfurling': 754,\n",
       " 'insanity': 755,\n",
       " 'outdid': 756,\n",
       " 'denying': 757,\n",
       " 'superdrug': 758,\n",
       " 'inundated': 759,\n",
       " 'excel': 760,\n",
       " 'zonisamide': 761,\n",
       " 'idiotic': 762,\n",
       " 'ardmore': 763,\n",
       " 'hardworking': 764,\n",
       " 'marge': 765,\n",
       " 'maturity': 766,\n",
       " 'please': 767,\n",
       " 'scad': 768,\n",
       " 'undeservingly': 769,\n",
       " 'dazs': 770,\n",
       " 'igniting': 771,\n",
       " 'wailing': 772,\n",
       " 'l': 773,\n",
       " 'zhu': 774,\n",
       " 'circle': 775,\n",
       " 'copious': 776,\n",
       " 'admittance': 777,\n",
       " 'geek': 778,\n",
       " 'gimmick': 779,\n",
       " 'reusable': 780,\n",
       " 'resolving': 781,\n",
       " 'regulated': 782,\n",
       " 'hurry': 783,\n",
       " 'smidgen': 784,\n",
       " 'raw': 785,\n",
       " 'allows': 786,\n",
       " 'jump': 787,\n",
       " 'takeing': 788,\n",
       " 'sanity': 789,\n",
       " 'ultimate': 790,\n",
       " 'correct': 791,\n",
       " 'pac': 792,\n",
       " 'irritate': 793,\n",
       " 'starring': 794,\n",
       " 'wangxuehai': 795,\n",
       " 'laser': 796,\n",
       " 'embarked': 797,\n",
       " 'ivy': 798,\n",
       " 'echoing': 799,\n",
       " 'richness': 800,\n",
       " 'lunchroom': 801,\n",
       " 'goodnight': 802,\n",
       " 'marilla': 803,\n",
       " 'repeat': 804,\n",
       " 'superduperreally': 805,\n",
       " 'audrey': 806,\n",
       " 'breed': 807,\n",
       " 'crew': 808,\n",
       " 'irma': 809,\n",
       " 'unruly': 810,\n",
       " 'invade': 811,\n",
       " 'ghetto': 812,\n",
       " 'church': 813,\n",
       " 'loathe': 814,\n",
       " 'vulgar': 815,\n",
       " 'dense': 816,\n",
       " 'fiesty': 817,\n",
       " 'app': 818,\n",
       " 'bad': 819,\n",
       " 'exponentially': 820,\n",
       " 'eight': 821,\n",
       " 'stop': 822,\n",
       " 'crushingly': 823,\n",
       " 'coupled': 824,\n",
       " 'faceplate': 825,\n",
       " 'conquering': 826,\n",
       " 'side': 827,\n",
       " 'convey': 828,\n",
       " 'shocking': 829,\n",
       " 'manic': 830,\n",
       " 'doorstep': 831,\n",
       " 'tuck': 832,\n",
       " 'holli': 833,\n",
       " 'finality': 834,\n",
       " 'ashley': 835,\n",
       " 'kingdom': 836,\n",
       " 'discontented': 837,\n",
       " 'report': 838,\n",
       " 'venison': 839,\n",
       " 'slash': 840,\n",
       " 'samuel': 841,\n",
       " 'deferential': 842,\n",
       " 'atom': 843,\n",
       " 'lindo': 844,\n",
       " 'milligram': 845,\n",
       " 'yoga': 846,\n",
       " 'caught': 847,\n",
       " 'occassional': 848,\n",
       " 'pistil': 849,\n",
       " 'deepika': 850,\n",
       " 'festive': 851,\n",
       " 'biopsy': 852,\n",
       " 'brittneys': 853,\n",
       " 'sprog': 854,\n",
       " 'cave': 855,\n",
       " 'euphoric': 856,\n",
       " 'belmont': 857,\n",
       " 'maclaine': 858,\n",
       " 'baht': 859,\n",
       " 'stolen': 860,\n",
       " 'wimbledon': 861,\n",
       " 'interactive': 862,\n",
       " 'worthless': 863,\n",
       " 'encoding': 864,\n",
       " 'blunt': 865,\n",
       " 'plead': 866,\n",
       " 'borrow': 867,\n",
       " 'tie': 868,\n",
       " 'lion': 869,\n",
       " 'harmony': 870,\n",
       " 'itv': 871,\n",
       " 'zum': 872,\n",
       " 'manure': 873,\n",
       " 'painstakingly': 874,\n",
       " 'system': 875,\n",
       " 'mopping': 876,\n",
       " 'indian': 877,\n",
       " 'loosen': 878,\n",
       " 'pressed': 879,\n",
       " 'kg': 880,\n",
       " 'bunnysuit': 881,\n",
       " 'pub': 882,\n",
       " 'dint': 883,\n",
       " 'grue': 884,\n",
       " 'infusion': 885,\n",
       " 'feelingless': 886,\n",
       " 'walt': 887,\n",
       " 'frame': 888,\n",
       " 'goal': 889,\n",
       " 'vacationer': 890,\n",
       " 'learned': 891,\n",
       " 'floor': 892,\n",
       " 'clench': 893,\n",
       " 'daunting': 894,\n",
       " 'dsp': 895,\n",
       " 'huddled': 896,\n",
       " 'result': 897,\n",
       " 'antonios': 898,\n",
       " 'operating': 899,\n",
       " 'floated': 900,\n",
       " 'land': 901,\n",
       " 'internship': 902,\n",
       " 'uncaring': 903,\n",
       " 'philosopher': 904,\n",
       " 'modification': 905,\n",
       " 'regardless': 906,\n",
       " 'sep': 907,\n",
       " 'chip': 908,\n",
       " 'gorge': 909,\n",
       " 'implement': 910,\n",
       " 'bon': 911,\n",
       " 'educationg': 912,\n",
       " 'dlk': 913,\n",
       " 'deepens': 914,\n",
       " 'murmur': 915,\n",
       " 'fleeting': 916,\n",
       " 'offending': 917,\n",
       " 'creeped': 918,\n",
       " 'procrastinating': 919,\n",
       " 'spirit': 920,\n",
       " 'orgasm': 921,\n",
       " 'gel': 922,\n",
       " 'zen': 923,\n",
       " 'chinatown': 924,\n",
       " 'ineffective': 925,\n",
       " 'xvi': 926,\n",
       " 'scariest': 927,\n",
       " 'forefront': 928,\n",
       " 'valued': 929,\n",
       " 'uttering': 930,\n",
       " 'hsp': 931,\n",
       " 'electric': 932,\n",
       " 'distrust': 933,\n",
       " 'go': 934,\n",
       " 'xanax': 935,\n",
       " 'footbed': 936,\n",
       " 'disclaimer': 937,\n",
       " 'witnessed': 938,\n",
       " 'destiny': 939,\n",
       " 'billiards': 940,\n",
       " 'heed': 941,\n",
       " 'reproduce': 942,\n",
       " 'snuggled': 943,\n",
       " 'nightlife': 944,\n",
       " 'kava': 945,\n",
       " 'mailbox': 946,\n",
       " 'enmayi': 947,\n",
       " 'mortality': 948,\n",
       " 'slave': 949,\n",
       " 'romantic': 950,\n",
       " 'steamy': 951,\n",
       " 'pooper': 952,\n",
       " 'akward': 953,\n",
       " 'practiced': 954,\n",
       " 'warner': 955,\n",
       " 'sob': 956,\n",
       " 'complainy': 957,\n",
       " 'putrid': 958,\n",
       " 'beoryeo': 959,\n",
       " 'kite': 960,\n",
       " 'kinship': 961,\n",
       " 'coke': 962,\n",
       " 'scout': 963,\n",
       " 'salon': 964,\n",
       " 'diazepam': 965,\n",
       " 'filmfare': 966,\n",
       " 'disappearance': 967,\n",
       " 'wired': 968,\n",
       " 'heartburn': 969,\n",
       " 'dress': 970,\n",
       " 'indicated': 971,\n",
       " 'plugin': 972,\n",
       " 'braved': 973,\n",
       " 'luscious': 974,\n",
       " 'earnestness': 975,\n",
       " 'pity': 976,\n",
       " 'vibrant': 977,\n",
       " 'lh': 978,\n",
       " 'sooooo': 979,\n",
       " 'devoured': 980,\n",
       " 'relaxing': 981,\n",
       " 'braeden': 982,\n",
       " 'porcelain': 983,\n",
       " 'nailart': 984,\n",
       " 'detoured': 985,\n",
       " 'published': 986,\n",
       " 'horrific': 987,\n",
       " 'collapsing': 988,\n",
       " 'ano': 989,\n",
       " 'temperature': 990,\n",
       " 'referee': 991,\n",
       " 'kevin': 992,\n",
       " 'eminent': 993,\n",
       " 'taryn': 994,\n",
       " 'faking': 995,\n",
       " 'display': 996,\n",
       " 'okc': 997,\n",
       " 'school': 998,\n",
       " 'psalm': 999,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def id_each_unique_word(df):\n",
    "    \"\"\"\" \n",
    "    Mapping each unique word to an id\n",
    "    \n",
    "    Returns: \n",
    "    - word_to_id (maps the words to an id) \n",
    "    - id_to_word (maps the id to a word)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    index_each_word = df['tokenized_text'].explode().unique()\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "\n",
    "\n",
    "    for i, token in enumerate(set(index_each_word)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "\n",
    "    print(f'Unique words in the text: {len(word_to_id)}')\n",
    "    display(word_to_id)\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "\n",
    "word_to_id, id_to_word = id_each_unique_word(df)\n",
    "\n",
    "\n",
    "def change_label_to_int(df):\n",
    "    \"\"\" Change the labels column positive, neutral, negative to ints \"\"\"\n",
    "\n",
    "    df['label'] = df['label'].replace({'Neutral': 0, 'Negative': 1, 'Positive': 2})\n",
    "\n",
    "    return\n",
    "\n",
    "change_label_to_int(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ['feel', 'pissed', 'old', 'friend', 'friend']\n",
      "Converted to Sequence: 1: [1280, 2850, 10667, 1836, 1836]\n",
      "======================================================\n",
      "Sentence 2: ['found', 'made', 'huge', 'difference', 'especially', 'finger', 'ring', 'skin', 'feel', 'much', 'softer', 'le', 'irritated']\n",
      "Converted to Sequence: 2: [11224, 13002, 9155, 13715, 12593, 13070, 6769, 7139, 1280, 6647, 3002, 13091, 13061]\n",
      "======================================================\n",
      "Sentence 3: ['also', 'feel', 'unfortunate', 'nearly', 'reader', 'going', 'meet', 'man', 'african', 'american', 'unlike']\n",
      "Converted to Sequence: 3: [1488, 1280, 10523, 6020, 4970, 10904, 1996, 10534, 3355, 13783, 10701]\n",
      "======================================================\n",
      "Sentence 4: ['feel', 'petty', 'clairee']\n",
      "Converted to Sequence: 4: [1280, 1251, 14364]\n",
      "======================================================\n",
      "Sentence 5: ['used', 'believe', 'feel', 'like', 'fear', 'ignored', 'suppressed', 'right', 'away', 'moment']\n",
      "Converted to Sequence: 5: [13417, 4021, 1280, 9866, 10099, 3441, 14507, 9873, 2078, 11373]\n",
      "======================================================\n",
      "Amount of text sequences (rows): 20000\n"
     ]
    }
   ],
   "source": [
    "def text_to_sequence(word_to_id):\n",
    "    \"\"\" \n",
    "    Convert sentences to sequences from word_of_id mapping\n",
    "      \n",
    "    Returns: sequence of ints where each sequence corresponds to a sentence\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    sequences = []\n",
    "\n",
    "\n",
    "    for index, sentence in enumerate(df['tokenized_text']):\n",
    "\n",
    "        # for each word in the sentence get the corresponding id from mapping and put in list, if not found return None\n",
    "        seq = [word_to_id.get(word, None) for word in sentence]\n",
    "        sequences.append(seq)\n",
    "\n",
    "        if index < 5: # 5 examples to make sure its correct converted\n",
    "            print(f\"Sentence {index+1}: {sentence}\")\n",
    "            get_separator\n",
    "            print(f\"Converted to Sequence: {index+1}: {seq}\")\n",
    "            get_separator()\n",
    "\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "sequences = text_to_sequence(word_to_id)\n",
    "\n",
    "\n",
    "print(f'Amount of text sequences (rows): {len(sequences)}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 35\n",
      "Min sequence length: 1\n",
      "Mean sequence length: 9.07685\n",
      "======================================================\n",
      "Max sequence length: 35\n",
      "Min sequence length: 35\n",
      "Mean sequence length: 35.0\n"
     ]
    }
   ],
   "source": [
    "def get_seq_length(sequences):\n",
    "    \"\"\" Get the length of the maximum sequence in the sequences list to determine padding \"\"\"\n",
    "    \n",
    "    max_length = max(len(sequence) for sequence in sequences)\n",
    "    min_length = min(len(sequence) for sequence in sequences)\n",
    "    mean_length = sum(map(len, sequences)) / len(sequences)\n",
    "\n",
    "    print(f'Max sequence length: {max_length}')\n",
    "    print(f'Min sequence length: {min_length}')\n",
    "    print(f'Mean sequence length: {mean_length}')\n",
    "\n",
    "    return max_length, min_length, mean_length\n",
    "\n",
    "\n",
    "def pad_data(sequences, max_length):\n",
    "    \"\"\" Pad the sequences to the max length \"\"\"\n",
    "\n",
    "    padded_sequences = []\n",
    "\n",
    "    for sequence in sequences:\n",
    "        padded_sequence = sequence + [0] * (max_length - len(sequence)) # padding the sequences with 0 (neutral label)\n",
    "        padded_sequences.append(padded_sequence)\n",
    "\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "# investigating so the padding works as intented\n",
    "og_max_length, og_min_length, og_mean_length = get_seq_length(sequences)\n",
    "\n",
    "get_separator()\n",
    "\n",
    "padded_sequences = pad_data(sequences, og_max_length)\n",
    "max_length, min_length, mean_length = get_seq_length(padded_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the paddding might affect the overall model and further deep diving into the text data itself is needed. But it's not the priority for me in this assignment, the goal is to follow the whole workflow and create a model from scratch for learning. For instance some words that are misspelled i haven't done anything about either, and this will also affect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 35, 1)\n",
      "(20000, 3)\n",
      "X_train shape: (12000, 35, 1)\n",
      "X_test shape: (4000, 35, 1)\n",
      "X_val shape: (4000, 35, 1)\n",
      "y_train shape: (12000, 3)\n",
      "y_test shape: (4000, 3)\n",
      "y_val shape: (4000, 3)\n"
     ]
    }
   ],
   "source": [
    "def get_train_test_val(df, padded_sequences):\n",
    "    \"\"\" \n",
    "    Converts and aligns the padded sequence and the labels to a numpy array.\n",
    "    Then splits up the data. One-hot encodes the labels.\n",
    "    \n",
    "    Takes in:\n",
    "    - padded_sequence(list)\n",
    "    - df['labels'] \n",
    "\n",
    "    Returns: train, test, val split of the data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # # Converting to numpy arrays, these lists are in the same order\n",
    "    # np_seq = np.array(padded_sequences)\n",
    "    # np_labels = np.array(df['label'].values)\n",
    "\n",
    "    # np_seq = np_seq.reshape((np_seq.shape[0], np_seq.shape[1], 1))  # Reshape to (samples, timesteps, features)\n",
    "\n",
    "    # Converting to numpy arrays\n",
    "    np_seq = np.array(padded_sequences)\n",
    "    np_seq = np_seq.reshape((np_seq.shape[0], np_seq.shape[1], 1))  # Reshape to (samples, timesteps, features)\n",
    "\n",
    "\n",
    "\n",
    "    # One-hot encoding the labels\n",
    "    np_labels = to_categorical(df['label'].values)\n",
    "\n",
    "    \n",
    "\n",
    "    print(np_seq.shape)\n",
    "    print(np_labels.shape)\n",
    "\n",
    "\n",
    "    # Splitting the data into:\n",
    "    # - train (60%)\n",
    "    # - test (20%)\n",
    "    # - validation (20% of train data)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np_seq, np_labels, test_size=0.2, random_state=137)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=137)\n",
    "\n",
    "\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = get_train_test_val(df, padded_sequences)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_x_shape(X_train, X_test, X_val, y_train, y_test, y_val):\n",
    "    \"\"\" Check the shape of the arrays \"\"\"\n",
    "\n",
    "    print(f'X_train shape: {X_train.shape}')\n",
    "    print(f'X_test shape: {X_test.shape}')\n",
    "    print(f'X_val shape: {X_val.shape}')\n",
    "    print(f'y_train shape: {y_train.shape}')\n",
    "    print(f'y_test shape: {y_test.shape}')\n",
    "    print(f'y_val shape: {y_val.shape}')\n",
    "\n",
    "\n",
    "def check_alignment():\n",
    "    \"\"\" Check and inspect the array \"\"\"\n",
    "    for i in range(5):\n",
    "        get_separator()\n",
    "        \n",
    "        sequence_str = ' '.join(map(str, X_train[i].flatten()))\n",
    "\n",
    "        print(f'Sequence: [{sequence_str}]')\n",
    "        print(f\"Label: {y_train[i]}\")\n",
    "    \n",
    "    get_separator()\n",
    "\n",
    "check_x_shape(X_train, X_test, X_val, y_train, y_test, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "Sequence: [1280 681 6133 9675 3793 190 1408 10351 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Label: [0. 0. 1.]\n",
      "======================================================\n",
      "Sequence: [1280 9087 13850 11373 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Label: [0. 1. 0.]\n",
      "======================================================\n",
      "Sequence: [1280 13095 12676 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Label: [0. 1. 0.]\n",
      "======================================================\n",
      "Sequence: [1280 10867 14495 8461 6390 11689 11681 9515 7570 6038 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Label: [0. 0. 1.]\n",
      "======================================================\n",
      "Sequence: [1280 10058 4834 9859 13713 4834 144 11741 488 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Label: [0. 0. 1.]\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "check_alignment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def rnn_model(num_classes):\n",
    "#     \"\"\"\n",
    "#     This script creates the RNN model using tensorflow.keras.\n",
    "#     \"\"\"\n",
    "\n",
    "#     model = Sequential([\n",
    "#         SimpleRNN(400, input_shape=(35, 1)),  # Corrected input_shape\n",
    "#         Dense(9, activation='relu'),\n",
    "#         Dense(27, activation='relu'),\n",
    "#         Dense(243, activation='relu'),\n",
    "#         Dense(27, activation='relu'),\n",
    "#         Dense(num_classes, activation='softmax')  # Changed to softmax for classification\n",
    "#     ])\n",
    "\n",
    "#     model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# model = rnn_model(NUM_CLASSES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 35, 128)           2048000   \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 128)               99072     \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 3)                 387       \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 9)                 36        \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 9)                 90        \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 3)                 30        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2147615 (8.19 MB)\n",
      "Trainable params: 2147615 (8.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 25s 60ms/step - loss: 0.8314 - accuracy: 0.5362 - val_loss: 0.8194 - val_accuracy: 0.5405\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 23s 61ms/step - loss: 0.8146 - accuracy: 0.5465 - val_loss: 0.8196 - val_accuracy: 0.5405\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 24s 63ms/step - loss: 0.8141 - accuracy: 0.5465 - val_loss: 0.8233 - val_accuracy: 0.5405\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 24s 64ms/step - loss: 0.8148 - accuracy: 0.5465 - val_loss: 0.8202 - val_accuracy: 0.5405\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 24s 63ms/step - loss: 0.5727 - accuracy: 0.7159 - val_loss: 0.1651 - val_accuracy: 0.9528\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 25s 67ms/step - loss: 0.0940 - accuracy: 0.9734 - val_loss: 0.1051 - val_accuracy: 0.9672\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 23s 63ms/step - loss: 0.0608 - accuracy: 0.9825 - val_loss: 0.1060 - val_accuracy: 0.9650\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.0474 - accuracy: 0.9865 - val_loss: 0.1039 - val_accuracy: 0.9682\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 26s 68ms/step - loss: 0.0426 - accuracy: 0.9885 - val_loss: 0.1057 - val_accuracy: 0.9668\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.0370 - accuracy: 0.9910 - val_loss: 0.1253 - val_accuracy: 0.9670\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.0376 - accuracy: 0.9901 - val_loss: 0.1203 - val_accuracy: 0.9655\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.0327 - accuracy: 0.9914 - val_loss: 0.1152 - val_accuracy: 0.9643\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 26s 70ms/step - loss: 0.0284 - accuracy: 0.9927 - val_loss: 0.1203 - val_accuracy: 0.9672\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 24s 63ms/step - loss: 0.0244 - accuracy: 0.9940 - val_loss: 0.1336 - val_accuracy: 0.9663\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 24s 64ms/step - loss: 0.0232 - accuracy: 0.9950 - val_loss: 0.1338 - val_accuracy: 0.9685\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 24s 64ms/step - loss: 0.0200 - accuracy: 0.9959 - val_loss: 0.1327 - val_accuracy: 0.9688\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.0179 - accuracy: 0.9968 - val_loss: 0.1442 - val_accuracy: 0.9643\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 24s 64ms/step - loss: 0.0175 - accuracy: 0.9967 - val_loss: 0.1552 - val_accuracy: 0.9670\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 26s 69ms/step - loss: 0.0201 - accuracy: 0.9958 - val_loss: 0.1412 - val_accuracy: 0.9672\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.0160 - accuracy: 0.9969 - val_loss: 0.1688 - val_accuracy: 0.9638\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 25s 67ms/step - loss: 0.0184 - accuracy: 0.9962 - val_loss: 0.1334 - val_accuracy: 0.9693\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.0186 - accuracy: 0.9966 - val_loss: 0.1655 - val_accuracy: 0.9622\n",
      "Epoch 23/30\n",
      " 34/375 [=>............................] - ETA: 22s - loss: 0.0180 - accuracy: 0.9972"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 53\u001b[0m\n\u001b[1;32m     48\u001b[0m     save_history(history)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 30\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, X_train, y_train, X_val, y_val, epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, X_train, y_train, X_val, y_val, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    This scripts trains the model and saves the weights and history. \u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_history\u001b[39m(history):\n\u001b[1;32m     36\u001b[0m         time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/www/rnn-assignment/venv-rnn/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/www/rnn-assignment/venv-rnn/lib/python3.9/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Documents/www/rnn-assignment/venv-rnn/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/www/rnn-assignment/venv-rnn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Documents/www/rnn-assignment/venv-rnn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Documents/www/rnn-assignment/venv-rnn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/www/rnn-assignment/venv-rnn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Documents/www/rnn-assignment/venv-rnn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Documents/www/rnn-assignment/venv-rnn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/www/rnn-assignment/venv-rnn/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/Documents/www/rnn-assignment/venv-rnn/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def rnn_model_gru(num_classes, learning_rate=0.0009):\n",
    "    \"\"\"\n",
    "    This script creates an RNN model with GRU units using tensorflow.keras.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(16000, 128, input_length=35),  # Corrected input_shape\n",
    "        GRU(128, input_shape=(35, 1), dropout=0.2, recurrent_dropout=0.2),  # Using GRU instead of SimpleRNN\n",
    "        Dense(3, activation='tanh'),\n",
    "        Dense(9, activation='tanh'),\n",
    "        Dense(9, activation='tanh'),\n",
    "        Dense(num_classes, activation='softmax')  # Softmax for classification\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = rnn_model_gru(NUM_CLASSES, learning_rate=0.0009)\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=2):\n",
    "    \"\"\" \n",
    "    This scripts trains the model and saves the weights and history. \n",
    "    \n",
    "    \"\"\"\n",
    "    history = model.fit(X_train, y_train,\n",
    "                            batch_size=32,\n",
    "                            validation_data=(X_val, y_val), \n",
    "                            epochs=epochs)\n",
    "\n",
    "    def save_history(history):\n",
    "        time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        _file = ((f'history_{epochs}_{time}.npy'))\n",
    "        file = os.path.join(LOGS_PATH, _file)\n",
    "\n",
    "        np.save(file, history.history)\n",
    "\n",
    "        model.save(f'model/model_{time}.keras')  # generalize this solution\n",
    "\n",
    "        get_separator()\n",
    "        print(f'Saved training history to: {file}')\n",
    "        get_separator()\n",
    "\n",
    "    save_history(history)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "train_model(model, X_train, y_train, X_val, y_val, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HÃ„R Ã„R JAG NEDANFÃ–R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 498ms/step\n",
      "Prediction: 1\n",
      "Label: negative\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "text = \"happy happy happy happy\"\n",
    "# Load the model\n",
    "model_path = 'model/model_20240112-223414.keras'\n",
    "model = load_model(model_path)\n",
    "\n",
    "\n",
    "\n",
    "def predict(model, text, word_to_id, max_length=35):\n",
    "    \"\"\"\n",
    "    Predict the sentiment of a given text using a pre-trained model.\n",
    "    \"\"\"\n",
    "    # Define the sentiment labels\n",
    "    sentiment_labels = ['neutral', 'negative', 'positive']\n",
    "\n",
    "    # Preprocess the text\n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{string.punctuation}]', '', text)\n",
    "    text = nltk.word_tokenize(text)\n",
    "\n",
    "    # Convert words to indices\n",
    "    text_indices = [word_to_id.get(word, 0) for word in text]\n",
    "\n",
    "    # Pad the sequence\n",
    "    if len(text_indices) < max_length:\n",
    "        text_indices += [0] * (max_length - len(text_indices))\n",
    "\n",
    "    # Convert to numpy array and reshape\n",
    "    text_array = np.array([text_indices])\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(text_array)\n",
    "    prediction = np.argmax(prediction, axis=1)\n",
    "\n",
    "    # Output\n",
    "    print(f'Prediction: {prediction[0]}')\n",
    "    print(f'Label: {sentiment_labels[prediction[0]]}')\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# Example usage\n",
    "predict(model, text, word_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
    "\n",
    "\n",
    "def decode_sentiment(score, include_neutral=True):\n",
    "    if include_neutral:        \n",
    "        label = neutral\n",
    "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
    "            label = negative\n",
    "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
    "            label = positive\n",
    "\n",
    "        return label\n",
    "    else:\n",
    "        return negative if score < 0.5 else positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
    "\n",
    "\n",
    "def decode_sentiment(score, include_neutral=True):\n",
    "    if include_neutral:        \n",
    "        label = neutral\n",
    "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
    "            label = negative\n",
    "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
    "            label = positive\n",
    "\n",
    "        return label\n",
    "    else:\n",
    "        return negative if score < 0.5 else positive\n",
    "\n",
    "\n",
    "def predict(text, include_neutral=True, X_test, y_test):\n",
    "    start_at = time.time()\n",
    "    \n",
    "\n",
    "    \n",
    "    # Predict\n",
    "    score = model.predict([X_test])\n",
    "    \n",
    "    # Decode sentiment\n",
    "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
    "\n",
    "    return {\"label\": label, \"score\": float(score),\n",
    "       \"elapsed_time\": time.time()-start_at}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 29\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: label,\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m: score,\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m'\u001b[39m: elapsed_time\n\u001b[1;32m     26\u001b[0m     }\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI love the music\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m start_at \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming `model` is already loaded and `preprocess` is a function you've defined \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# to format your text input (like tokenization, padding, etc.)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m processed_text \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m([text])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Making the prediction\u001b[39;00m\n\u001b[1;32m     12\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(processed_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def predict(text):\n",
    "    \"\"\" Predict the sentiment of a given text \"\"\"\n",
    "    start_at = time.time()\n",
    "\n",
    "    # Assuming `model` is already loaded and `preprocess` is a function you've defined \n",
    "    # to format your text input (like tokenization, padding, etc.)\n",
    "    processed_text = preprocess([text])\n",
    "    \n",
    "    # Making the prediction\n",
    "    prediction = model.predict(processed_text)\n",
    "\n",
    "    # Assuming your model returns a probability distribution for classes\n",
    "    # and you're interested in the class with the highest probability\n",
    "    label = \"positive\" if prediction[0][1] > 0.5 else \"negative\"\n",
    "    score = max(prediction[0])\n",
    "\n",
    "    end_at = time.time()\n",
    "    elapsed_time = end_at - start_at\n",
    "\n",
    "    return {\n",
    "        'label': label,\n",
    "        'score': score,\n",
    "        'elapsed_time': elapsed_time\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result = predict(\"I love the music\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HÃ„R Ã„R JAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1451506294.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[24], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    eewewdef train_model(model, X_train, y_train, X_val, y_val, epochs=30):\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eewewdef train_model(model, X_train, y_train, X_val, y_val, epochs=30):\n",
    "    \"\"\" \n",
    "    This scripts trains the model and saves the weights and history. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    history = model.fit(X_train, y_train, \n",
    "                            validation_data=(X_val, y_val), \n",
    "                            epochs=epochs)\n",
    "\n",
    "    def save_history(history):\n",
    "        time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        _file = ((f'history_{epochs}_{time}.npy'))\n",
    "        file = os.path.join(LOGS_PATH, _file)\n",
    "\n",
    "        np.save(file, history.history)\n",
    "\n",
    "        rnn_model.save('model/model_1.h5')  # generalize this solution\n",
    "\n",
    "        get_separator()\n",
    "        print(f'Saved training history to: {file}')\n",
    "        get_separator()\n",
    "\n",
    "\n",
    "    return history, save_history(history)\n",
    "\n",
    "\n",
    "train_model(rnn_model_gru, X_train, y_train, X_val, y_val, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: input_dim is equal 38 ( equal to the length padded_sequence) TODO: Vocab_size is the len of word_to_id TODO: hidden_dim (????)\n",
    "\n",
    "# TODO: Split up the data and so they correspond with the label from the dataframe\n",
    "\n",
    "# Next is padding the data?\n",
    "\n",
    "# But what do i with the labels? And split up the data and labels (And i need to make sure that the labels follow correct sentence)\n",
    "\n",
    "# use constant padding since its 0 and affects the model as little as possible.\n",
    "\n",
    "# My choice in the model is gonna be an RNN network. Since i think even though its a multiclass classficiation problem. \n",
    "# I only have 3 classes so its not that complex i hope. And also i am combating the vanishing gradient problem by using tanh as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim = 38 \n",
    "# hidden_dim = vocab_size (word_to_id)\n",
    "\n",
    "\n",
    "# TODO: Construct the RNN cell with the correct class conventions \n",
    "# TODO: Construct the RNN model\n",
    "\n",
    "\n",
    "\n",
    "# class RnnCell(tf.Module):\n",
    "\n",
    "#     hidden_dim = len(word_to_id) # takes in dict of each unique words in the text\n",
    "\n",
    "#     def __init__(self, input_dim=38, hidden_dim=hidden_dim):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "        \n",
    "#         # weights and biases\n",
    "#         self.Wxh = tf.Variable(tf.random.normal([self.input_dim, self.hidden_dim])) # TODO: What is Wxh in the formula? \n",
    "#         self.Whh = tf.Variable(tf.random.normal([self.hidden_dim, self.hidden_dim]))\n",
    "#         self.bh = tf.Variable(tf.zeros([self.hidden_dim]))\n",
    "\n",
    "#     def __call__(self, x, h):\n",
    "\n",
    "#         # Forward pass\n",
    "#         h_next = tf.tanh(tf.matmul(x, self.Wxh) + tf.matmul(h, self.Whh) + self.bh)\n",
    "\n",
    "#         return h_next\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RnnModel' object has no attribute 'sequence_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_inputs, batch_targets \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_train, y_train):\n\u001b[0;32m---> 93\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     96\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m rnn_model(batch_inputs)\n",
      "Cell \u001b[0;32mIn[49], line 76\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(rnn_model, inputs, targets)\u001b[0m\n\u001b[1;32m     74\u001b[0m clip_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 76\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     loss \u001b[38;5;241m=\u001b[39m rnn_model\u001b[38;5;241m.\u001b[39mloss(targets, predictions) \n\u001b[1;32m     79\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, rnn_model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "Cell \u001b[0;32mIn[49], line 53\u001b[0m, in \u001b[0;36mRnnModel.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m h \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros([batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn_cell\u001b[38;5;241m.\u001b[39mhidden_dim])\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Process each timestep\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_length\u001b[49m):\n\u001b[1;32m     54\u001b[0m     x_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x[:, t])\n\u001b[1;32m     55\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn_cell(x_t, h)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RnnModel' object has no attribute 'sequence_length'"
     ]
    }
   ],
   "source": [
    "eweww\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "class RnnCell(tf.Module):\n",
    "\n",
    "    hidden_dim = len(word_to_id) # takes in dict of each unique words in the text\n",
    "\n",
    "    def __init__(self, input_dim=38, hidden_dim=hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # weights and biases\n",
    "        self.Wxh = tf.Variable(tf.random.normal([self.input_dim, self.hidden_dim])) # TODO: What is Wxh in the formula? \n",
    "        self.Whh = tf.Variable(tf.random.normal([self.hidden_dim, self.hidden_dim]))\n",
    "        self.bh = tf.Variable(tf.zeros([self.hidden_dim]))\n",
    "\n",
    "    def __call__(self, x, h):\n",
    "\n",
    "        # Forward pass\n",
    "        h_next = tf.tanh(tf.matmul(x, self.Wxh) + tf.matmul(h, self.Whh) + self.bh)\n",
    "\n",
    "        return h_next\n",
    "\n",
    "\n",
    "\n",
    "class RnnModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=50, hidden_dim=50, output_dim=3, sequence_length=38):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # embedding layer\n",
    "\n",
    "        self.rnn_cell = RnnCell(embedding_dim, hidden_dim) # rnn cell\n",
    "\n",
    "        # output layer weights and biases\n",
    "        self.Why = tf.Variable(tf.random.normal([hidden_dim, output_dim]))\n",
    "        self.by = tf.Variable(tf.zeros([output_dim]))\n",
    "\n",
    "\n",
    "        self.dense_layers = [\n",
    "            tf.keras.layers.Dense(9, activation='tanh'),\n",
    "            tf.keras.layers.Dense(27, activation='tanh'),\n",
    "            tf.keras.layers.Dense(output_dim, activation='tanh')\n",
    "        ]\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Initialize hidden state\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        h = tf.zeros([batch_size, self.rnn_cell.hidden_dim])\n",
    "\n",
    "        # Process each timestep\n",
    "        for t in range(self.sequence_length):\n",
    "            x_t = self.embedding(x[:, t])\n",
    "            h = self.rnn_cell(x_t, h)\n",
    "\n",
    "        # Output layer\n",
    "        y = tf.matmul(h, self.Why) + self.by\n",
    "        return y\n",
    "\n",
    "\n",
    "    def compile(self, optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']):\n",
    "        super().compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=metrics)\n",
    "\n",
    "\n",
    "rnn_model = RnnModel()\n",
    "\n",
    "\n",
    "\n",
    "def train_step(rnn_model, inputs, targets):\n",
    "    \"\"\" This script is used to train the model \"\"\"\n",
    "    clip_norm = 1.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = rnn_model(inputs)\n",
    "        loss = rnn_model.loss(targets, predictions) \n",
    "\n",
    "    gradients = tape.gradient(loss, rnn_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, rnn_model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    # Training\n",
    "    for batch_inputs, batch_targets in zip(X_train, y_train):\n",
    "        loss = train_step(rnn_model, batch_inputs, batch_targets)\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "        predictions = rnn_model(batch_inputs)\n",
    "        accuracy = calculate_accuracy(batch_targets, predictions)\n",
    "        epoch_accuracy += accuracy.numpy()\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    avg_accuracy = epoch_accuracy / total_batches\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Training Loss: {avg_loss:.4f}, Training Accuracy: {avg_accuracy:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    val_batches = 0\n",
    "    for batch_inputs, batch_targets in zip(X_val, y_val):\n",
    "        predictions = rnn_model(batch_inputs)\n",
    "        val_loss += rnn_model.loss(batch_targets, predictions).numpy()  # Assuming rnn_model.loss is defined\n",
    "\n",
    "        val_accuracy += calculate_accuracy(batch_targets, predictions).numpy()\n",
    "        val_batches += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    avg_val_accuracy = val_accuracy / val_batches\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(rnn_model, X_train, y_train, X_val, y_val, epochs=30):\n",
    "    \"\"\" \n",
    "    This scripts trains the model and saves the weights and history. \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    history = rnn_model.fit(X_train, y_train, \n",
    "                            validation_data=(X_val, y_val), \n",
    "                            epochs=epochs)\n",
    "\n",
    "    def save_history(history):\n",
    "        time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        _file = ((f'history_{epochs}_{time}.npy'))\n",
    "        file = os.path.join(LOGS_PATH, _file)\n",
    "\n",
    "        np.save(file, history.history)\n",
    "\n",
    "        rnn_model.save('model/model_1.h5')  # generalize this solution\n",
    "\n",
    "        get_separator()\n",
    "        print(f'Saved training history to: {file}')\n",
    "        get_separator()\n",
    "\n",
    "\n",
    "    return history, save_history(history)\n",
    "\n",
    "\n",
    "train_model(rnn_model, X_train, y_train, X_val, y_val, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RnnModel(vocab_size)\n",
    "rnn_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming rnn_model is your RnnModel instance and already defined\n",
    "rnn_model = RnnModel(vocab_size)\n",
    "\n",
    "# Choose an optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Compile the model\n",
    "rnn_model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',  # since labels are not one-hot encoded but ints\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Note: If your labels (y_train, y_test) are not one-hot encoded, use 'sparse_categorical_crossentropy' as the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = rnn_model.fit(X_train, y_train, \n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=number_of_epochs, \n",
    "                        batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next is padding the data?\n",
    "But what do i with the labels? And split up the data and labels (And i need to make sure that the labels follow correct sentence)\n",
    "\n",
    "\n",
    "# use constant padding since its 0 and affects the model as little as possible.\n",
    "\n",
    "\n",
    "\n",
    "# My choice in the model is gonna be an RNN network. Since i think even though its a multiclass classficiation problem. I only have 3 classes so its not that complex i hope. And also i am combating the vanishing gradient problem by using tanh as the activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label', 'tokenized_text'], dtype='object')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'www' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwww\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'www' is not defined"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "www"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class RNNCell(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Initialize weights and biases\n",
    "        self.Wxh = tf.Variable(tf.random.normal([input_dim, hidden_dim]))\n",
    "        self.Whh = tf.Variable(tf.random.normal([hidden_dim, hidden_dim]))\n",
    "        self.bh = tf.Variable(tf.zeros([hidden_dim]))\n",
    "\n",
    "    def __call__(self, x, h):\n",
    "        # Compute the next hidden state\n",
    "        h_next = tf.tanh(tf.matmul(x, self.Wxh) + tf.matmul(h, self.Whh) + self.bh)\n",
    "        return h_next\n",
    "\n",
    "class MyRNNModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=1, sequence_length=100):\n",
    "        super().__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = tf.Variable(tf.random.normal([vocab_size, embedding_dim]))\n",
    "        # RNN Cell\n",
    "        self.rnn_cell = RNNCell(embedding_dim, hidden_dim)\n",
    "        # Output layer weights and biases\n",
    "        self.Why = tf.Variable(tf.random.normal([hidden_dim, output_dim]))\n",
    "        self.by = tf.Variable(tf.zeros([output_dim]))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Embedding lookup\n",
    "        x = tf.nn.embedding_lookup(self.embedding, x)\n",
    "        # Initial hidden state (zeros)\n",
    "        h = tf.zeros([x.shape[0], self.rnn_cell.Whh.shape[0]])\n",
    "\n",
    "        # Process the input sequence\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h = self.rnn_cell(x_t, h)\n",
    "\n",
    "        # Compute the output\n",
    "        y = tf.matmul(h, self.Why) + self.by\n",
    "        return tf.sigmoid(y)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = tf.losses.BinaryCrossentropy()\n",
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "def train_step(model, inputs, targets, clip_norm=1.0):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(targets, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    gradients = [tf.clip_by_norm(g, clip_norm) for g in gradients]  # Gradient clipping\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Example usage\n",
    "# Define your model, dataset, and training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 17:03:28.307898: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def pad_data(sequences):\n",
    "    # Find the maximum sequence length\n",
    "    max_seq_length = max(len(s) for s in sequences)\n",
    "\n",
    "    # Pad the sequences to ensure uniform length\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "    return padded_sequences, max_seq_length\n",
    "\n",
    "\n",
    "padded_sequences, max_seq_length = pad_data(sequences)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define model parameters\n",
    "vocab_size = len(word_to_id)  # Total number of unique words\n",
    "embed_size = 100  # Size of the embeddings\n",
    "\n",
    "# Adjust the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=max_seq_length))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(27, activation='tanh'))\n",
    "model.add(Dense(9, activation='tanh'))\n",
    "model.add(Dense(3, activation='tanh'))\n",
    "\n",
    "\n",
    "# I am using tanh since activation since it introduces non-linearity to the model and it is good for classification problems. The curve goes from -1 to 1 and it is good considering positive, neutral and negative.\n",
    "# S\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorial_crossentrop', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into train and test sets (e.g., 80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting the training data into train and validation sets (e.g., 80% train, 20% validation of the original training data)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "375/375 [==============================] - 12s 27ms/step - loss: 0.6872 - accuracy: 0.1715 - val_loss: 0.2195 - val_accuracy: 0.3918\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 9s 24ms/step - loss: 0.1148 - accuracy: 0.4134 - val_loss: 0.1535 - val_accuracy: 0.4175\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 9s 25ms/step - loss: 0.0456 - accuracy: 0.4352 - val_loss: 0.1185 - val_accuracy: 0.4150\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.0268 - accuracy: 0.4420 - val_loss: 0.1103 - val_accuracy: 0.4353\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.0256 - accuracy: 0.4454 - val_loss: 0.2796 - val_accuracy: 0.4358\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.1423 - accuracy: 0.3992 - val_loss: 0.2348 - val_accuracy: 0.4142\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 9s 25ms/step - loss: 0.0748 - accuracy: 0.4212 - val_loss: 0.1984 - val_accuracy: 0.4035\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 9s 25ms/step - loss: 0.0481 - accuracy: 0.4336 - val_loss: 0.1631 - val_accuracy: 0.4230\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.0258 - accuracy: 0.4421 - val_loss: 0.1422 - val_accuracy: 0.4235\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 11s 30ms/step - loss: 0.0134 - accuracy: 0.4481 - val_loss: 0.1455 - val_accuracy: 0.4327\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.0089 - accuracy: 0.4506 - val_loss: 0.1542 - val_accuracy: 0.4360\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.0157 - accuracy: 0.4495 - val_loss: 0.1411 - val_accuracy: 0.4338\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 9s 24ms/step - loss: 0.0215 - accuracy: 0.4471 - val_loss: 0.1576 - val_accuracy: 0.4280\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 9s 25ms/step - loss: 0.0141 - accuracy: 0.4498 - val_loss: 0.1659 - val_accuracy: 0.4280\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 9s 24ms/step - loss: 0.0156 - accuracy: 0.4492 - val_loss: 0.1704 - val_accuracy: 0.4275\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 9s 25ms/step - loss: 0.0188 - accuracy: 0.4471 - val_loss: 0.1856 - val_accuracy: 0.4243\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.0135 - accuracy: 0.4493 - val_loss: 0.1774 - val_accuracy: 0.4268\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.0240 - accuracy: 0.4471 - val_loss: 0.1526 - val_accuracy: 0.4195\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 11s 29ms/step - loss: 0.0164 - accuracy: 0.4487 - val_loss: 0.1429 - val_accuracy: 0.4230\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 11s 30ms/step - loss: 0.0093 - accuracy: 0.4499 - val_loss: 0.1374 - val_accuracy: 0.4342\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 12s 32ms/step - loss: 0.0077 - accuracy: 0.4498 - val_loss: 0.1391 - val_accuracy: 0.4280\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 9s 24ms/step - loss: 0.0052 - accuracy: 0.4515 - val_loss: 0.1446 - val_accuracy: 0.4397\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.0236 - accuracy: 0.4468 - val_loss: 0.1476 - val_accuracy: 0.4207\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.0155 - accuracy: 0.4489 - val_loss: 0.1804 - val_accuracy: 0.4212\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.0191 - accuracy: 0.4493 - val_loss: 0.1391 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 8s 22ms/step - loss: 0.0122 - accuracy: 0.4495 - val_loss: 0.1372 - val_accuracy: 0.4405\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 9s 25ms/step - loss: 0.0100 - accuracy: 0.4488 - val_loss: 0.1437 - val_accuracy: 0.4295\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 9s 24ms/step - loss: 0.0110 - accuracy: 0.4493 - val_loss: 0.1752 - val_accuracy: 0.4250\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.0124 - accuracy: 0.4501 - val_loss: 0.1491 - val_accuracy: 0.4367\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 12s 31ms/step - loss: 0.0109 - accuracy: 0.4504 - val_loss: 0.1441 - val_accuracy: 0.4448\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=NUM_EPOCHS,              # Number of epochs\n",
    "                    batch_size=32,          # Size of each batch\n",
    "                    validation_data=(X_val, y_val))  # Validation data to monitor performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_history(history, X_train, y_train, X_val, y_val, NUM_EPOCHS, NUM_EPOCHS):\n",
    "    time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    file = (f'history_{NUM_EPOCHS}_{time}.npy')\n",
    "    file = os.path.join(LOGS_PATH, file)\n",
    "\n",
    "    \n",
    "    # Create logs folder if it does not exist\n",
    "    if not LOGS_PATH.exists:\n",
    "        os.makedirs('logs')\n",
    "\n",
    "\n",
    "    np.save(file, history.history) # saves history as a dict with key values\n",
    "\n",
    "    model.save('model/model_1.h5')  #TODO generalize this solution\n",
    "    \n",
    "    # np.save('logs/history.npy', history.history)\n",
    "    \n",
    "    get_separator()\n",
    "    print(f'Saved training history to: {file}')\n",
    "    get_separator()\n",
    "    return history, save_history(history)\n",
    "\n",
    "# train_model(model, X_train, y_train, X_val, y_val, NUM_EPOCHS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots metrics over training history\n",
    "def plot_metrics(metrics):\n",
    "    _, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    metrics[['loss', 'val_loss']].plot(ax=ax[0], title='Loss', grid=True)\n",
    "    metrics[['accuracy', 'val_accuracy']].plot(ax=ax[1], title='Accuracy', grid=True) \n",
    "\n",
    "plot_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 1s 8ms/step - loss: 0.1333 - accuracy: 0.4378\n",
      "Test Loss: 0.13325612246990204\n",
      "Test Accuracy: 0.4377500116825104\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-rnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
