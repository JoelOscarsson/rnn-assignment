{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "The goal for this assignment is to classify feelings in a text corpus. Each row has a sentence with corresponding emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:50:57.145428: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joeloscarsson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joeloscarsson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/joeloscarsson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import ssl \n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_DEFAULT = nltk.corpus.stopwords.words('english')\n",
    "LEMMATIZER = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "FILE = \"./data/emotions.csv\"\n",
    "LOGS_PATH = (\"./logs\")\n",
    "\n",
    "\n",
    "EPOCHS = 30\n",
    "INPUT_DIM = 3 # Size of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_separator():\n",
    "    \"\"\" Returns a separator \"\"\"\n",
    "    print(\"======================================================\")\n",
    "    \n",
    "\n",
    "def get_file():\n",
    "    \"\"\" Get the file and return it as a pandas dataframe\"\"\"\n",
    "    data = pd.read_csv(FILE)\n",
    "    pd.set_option('display.max_colwidth', 200) # too be able to see more of the text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel so pissed off over an old friend and some friends</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ive found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i also feel it is unfortunate that nearly all the readers of going to meet the man will be african americans unlike myself</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel petty a href http clairee</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i used to believe that a feeling like fear was to be ignored or suppressed right away more on this in a moment</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>i was i might be buying stuff from there but i feel the clothes are too casual</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>i like sonam deepika and genelia who i feel are very talented and beautiful</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>i feel pathetic that i can hardly go a whole day not talking to him</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>id have spent more time with her on reading i feel a bit guilty about that</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>i do however feel like one of those pathetic girls who make up excuses because of a guy</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        text  \\\n",
       "0                                                                                   i feel so pissed off over an old friend and some friends   \n",
       "1      ive found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated   \n",
       "2                 i also feel it is unfortunate that nearly all the readers of going to meet the man will be african americans unlike myself   \n",
       "3                                                                                                           i feel petty a href http clairee   \n",
       "4                             i used to believe that a feeling like fear was to be ignored or suppressed right away more on this in a moment   \n",
       "...                                                                                                                                      ...   \n",
       "19995                                                         i was i might be buying stuff from there but i feel the clothes are too casual   \n",
       "19996                                                            i like sonam deepika and genelia who i feel are very talented and beautiful   \n",
       "19997                                                                    i feel pathetic that i can hardly go a whole day not talking to him   \n",
       "19998                                                             id have spent more time with her on reading i feel a bit guilty about that   \n",
       "19999                                                i do however feel like one of those pathetic girls who make up excuses because of a guy   \n",
       "\n",
       "         label  \n",
       "0        anger  \n",
       "1        anger  \n",
       "2      sadness  \n",
       "3        anger  \n",
       "4      sadness  \n",
       "...        ...  \n",
       "19995      joy  \n",
       "19996      joy  \n",
       "19997  sadness  \n",
       "19998  sadness  \n",
       "19999  sadness  \n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    20000 non-null  object\n",
      " 1   label   20000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 312.6+ KB\n"
     ]
    }
   ],
   "source": [
    "get_file().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'sadness', 'joy', 'love', 'fear', 'surprise'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file()['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = get_file()\n",
    "\n",
    "sns.countplot(plot, x='label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to separate these 6 emotions into 3 categories because some of the feelings are similar. These categories are positive, negative and neutral. We want to group the emotions with corresponding category. \n",
    "This means we can re-label to new categories:\n",
    "\n",
    "* Negative\n",
    "    - 'anger'\n",
    "    - 'sadness'\n",
    "    - 'fear'\n",
    "\n",
    "* Neutral\n",
    "    - 'surprise'\n",
    "\n",
    "* Positive\n",
    "    - 'joy'\n",
    "    - 'love'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAGwCAYAAAC0HlECAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuoklEQVR4nO3de1hU9aL/8Q+IXLwAXhAkR2Wnx0uRJnYUSysjxzIf3bkrk9KSJA0ytdTcu8gyw0uaaR3J9il1HzxZ7a2ZlkmYuFXyQt7vp2hrKVApTGICyvr90Wb9nLD8iuQM+n49zzyPs9Z31nwXz2p4t2bN4GNZliUAAAD8Jl9PTwAAAKAmIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGDAz9MTuFyUl5fryJEjql+/vnx8fDw9HQAAYMCyLP3444+KjIyUr+9vn0simqrJkSNH5HA4PD0NAABQBYcPH1azZs1+cwzRVE3q168v6ecfenBwsIdnAwAATLhcLjkcDvv3+G8hmqpJxVtywcHBRBMAADWMyaU1XAgOAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAE/T08A7mLGLvT0FOBFcqYP9vQUAAD/xpkmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABjwaTWvXrlXfvn0VGRkpHx8fLV261G29ZVlKSUlR06ZNFRQUpLi4OB08eNBtzLFjxxQfH6/g4GCFhoYqISFBJ06ccBuzY8cOde/eXYGBgXI4HJo2bVqlubz33ntq27atAgMDFR0drY8++qja9xcAANRcHo2m4uJidejQQa+//vo510+bNk2zZ89WWlqaNm7cqLp168rpdOrUqVP2mPj4eO3evVsZGRlavny51q5dq8TERHu9y+VSr1691KJFC+Xk5Gj69OmaOHGi5s2bZ4/ZsGGD7r//fiUkJGjr1q3q37+/+vfvr127dv1+Ow8AAGoUH8uyLE9PQpJ8fHy0ZMkS9e/fX9LPZ5kiIyP15JNP6qmnnpIkFRUVKTw8XPPnz9fAgQO1d+9etW/fXps3b1bnzp0lSStXrtSdd96pb775RpGRkZo7d67+8pe/KC8vT/7+/pKkp59+WkuXLtW+ffskSffdd5+Ki4u1fPlyez5du3ZVx44dlZaWds75lpSUqKSkxL7vcrnkcDhUVFSk4ODgKv8cYsYurPJjcfnJmT7Y01MAgMuay+VSSEiI0e9vr72mKTc3V3l5eYqLi7OXhYSEqEuXLsrOzpYkZWdnKzQ01A4mSYqLi5Ovr682btxoj+nRo4cdTJLkdDq1f/9+HT9+3B5z9vNUjKl4nnNJTU1VSEiIfXM4HBe/0wAAwGt5bTTl5eVJksLDw92Wh4eH2+vy8vLUpEkTt/V+fn5q2LCh25hzbePs5/i1MRXrz2XChAkqKiqyb4cPH77QXQQAADWIn6cnUFMFBAQoICDA09MAAACXiNeeaYqIiJAk5efnuy3Pz8+310VERKigoMBt/enTp3Xs2DG3MefaxtnP8WtjKtYDAAB4bTRFRUUpIiJCmZmZ9jKXy6WNGzcqNjZWkhQbG6vCwkLl5OTYY1avXq3y8nJ16dLFHrN27VqVlZXZYzIyMtSmTRs1aNDAHnP281SMqXgeAAAAj0bTiRMntG3bNm3btk3Szxd/b9u2TYcOHZKPj49GjRqlF198UcuWLdPOnTs1ePBgRUZG2p+wa9eunXr37q1hw4Zp06ZNWr9+vZKTkzVw4EBFRkZKkgYNGiR/f38lJCRo9+7dWrx4sV599VWNGTPGnscTTzyhlStXasaMGdq3b58mTpyoLVu2KDk5+VL/SAAAgJfy6DVNW7Zs0a233mrfrwiZIUOGaP78+Ro3bpyKi4uVmJiowsJC3XTTTVq5cqUCAwPtx6Snpys5OVm33XabfH19NWDAAM2ePdteHxISolWrVikpKUkxMTFq3LixUlJS3L7LqVu3blq0aJGeeeYZ/fnPf1br1q21dOlSXXvttZfgpwAAAGoCr/meppruQr7n4bfwPU04G9/TBAC/r8vie5oAAAC8CdEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGPDz9AQAeLeYsQs9PQV4kZzpgz09BcBjONMEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMCAV0fTmTNn9OyzzyoqKkpBQUG6+uqrNWnSJFmWZY+xLEspKSlq2rSpgoKCFBcXp4MHD7pt59ixY4qPj1dwcLBCQ0OVkJCgEydOuI3ZsWOHunfvrsDAQDkcDk2bNu2S7CMAAKgZvDqapk6dqrlz5+q1117T3r17NXXqVE2bNk1z5syxx0ybNk2zZ89WWlqaNm7cqLp168rpdOrUqVP2mPj4eO3evVsZGRlavny51q5dq8TERHu9y+VSr1691KJFC+Xk5Gj69OmaOHGi5s2bd0n3FwAAeC+v/jMqGzZsUL9+/dSnTx9JUsuWLfW///u/2rRpk6SfzzLNmjVLzzzzjPr16ydJWrhwocLDw7V06VINHDhQe/fu1cqVK7V582Z17txZkjRnzhzdeeedevnllxUZGan09HSVlpbqrbfekr+/v6655hpt27ZNM2fOdIsrAABw5fLqM03dunVTZmamDhw4IEnavn271q1bpzvuuEOSlJubq7y8PMXFxdmPCQkJUZcuXZSdnS1Jys7OVmhoqB1MkhQXFydfX19t3LjRHtOjRw/5+/vbY5xOp/bv36/jx4+fc24lJSVyuVxuNwAAcPny6jNNTz/9tFwul9q2batatWrpzJkzmjx5suLj4yVJeXl5kqTw8HC3x4WHh9vr8vLy1KRJE7f1fn5+atiwoduYqKioStuoWNegQYNKc0tNTdXzzz9fDXsJAABqAq8+0/Tuu+8qPT1dixYt0hdffKEFCxbo5Zdf1oIFCzw9NU2YMEFFRUX27fDhw56eEgAA+B159ZmmsWPH6umnn9bAgQMlSdHR0frXv/6l1NRUDRkyRBEREZKk/Px8NW3a1H5cfn6+OnbsKEmKiIhQQUGB23ZPnz6tY8eO2Y+PiIhQfn6+25iK+xVjfikgIEABAQEXv5MAAKBG8OozTSdPnpSvr/sUa9WqpfLycklSVFSUIiIilJmZaa93uVzauHGjYmNjJUmxsbEqLCxUTk6OPWb16tUqLy9Xly5d7DFr165VWVmZPSYjI0Nt2rQ551tzAADgyuPV0dS3b19NnjxZK1as0Ndff60lS5Zo5syZ+uMf/yhJ8vHx0ahRo/Tiiy9q2bJl2rlzpwYPHqzIyEj1799fktSuXTv17t1bw4YN06ZNm7R+/XolJydr4MCBioyMlCQNGjRI/v7+SkhI0O7du7V48WK9+uqrGjNmjKd2HQAAeBmvfntuzpw5evbZZ/XYY4+poKBAkZGRevTRR5WSkmKPGTdunIqLi5WYmKjCwkLddNNNWrlypQIDA+0x6enpSk5O1m233SZfX18NGDBAs2fPtteHhIRo1apVSkpKUkxMjBo3bqyUlBS+bgAAANh8rLO/XhtV5nK5FBISoqKiIgUHB1d5OzFjF1bjrFDT5Uwf7OkpcEzCjTcck0B1upDf31799hwAAIC3IJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMVCmaevbsqcLCwkrLXS6XevbsebFzAgAA8DpViqY1a9aotLS00vJTp07pn//850VPCgAAwNv4XcjgHTt22P/es2eP8vLy7PtnzpzRypUrddVVV1Xf7AAAALzEBUVTx44d5ePjIx8fn3O+DRcUFKQ5c+ZU2+QAAAC8xQW9PZebm6svv/xSlmVp06ZNys3NtW/ffvutXC6Xhg4dWq0T/Pbbb/XAAw+oUaNGCgoKUnR0tLZs2WKvtyxLKSkpatq0qYKCghQXF6eDBw+6bePYsWOKj49XcHCwQkNDlZCQoBMnTriN2bFjh7p3767AwEA5HA5NmzatWvcDAADUbBd0pqlFixaSpPLy8t9lMr90/Phx3Xjjjbr11lv18ccfKywsTAcPHlSDBg3sMdOmTdPs2bO1YMECRUVF6dlnn5XT6dSePXsUGBgoSYqPj9fRo0eVkZGhsrIyPfzww0pMTNSiRYsk/XwBe69evRQXF6e0tDTt3LlTQ4cOVWhoqBITEy/JvgIAAO92QdF0toMHD+qzzz5TQUFBpYhKSUm56IlJ0tSpU+VwOPT222/by6Kioux/W5alWbNm6ZlnnlG/fv0kSQsXLlR4eLiWLl2qgQMHau/evVq5cqU2b96szp07S5LmzJmjO++8Uy+//LIiIyOVnp6u0tJSvfXWW/L399c111yjbdu2aebMmUQTAACQVMVPz7355ptq166dUlJS9P7772vJkiX2benSpdU2uWXLlqlz586655571KRJE11//fV688037fW5ubnKy8tTXFycvSwkJERdunRRdna2JCk7O1uhoaF2MElSXFycfH19tXHjRntMjx495O/vb49xOp3av3+/jh8/fs65lZSUyOVyud0AAMDlq0rR9OKLL2ry5MnKy8vTtm3btHXrVvv2xRdfVNvkvvrqK82dO1etW7fWJ598ohEjRmjkyJFasGCBJNmf3gsPD3d7XHh4uL0uLy9PTZo0cVvv5+enhg0buo051zbOfo5fSk1NVUhIiH1zOBwXubcAAMCbVSmajh8/rnvuuae651JJeXm5OnXqpJdeeknXX3+9EhMTNWzYMKWlpf3uz30+EyZMUFFRkX07fPiwp6cEAAB+R1WKpnvuuUerVq2q7rlU0rRpU7Vv395tWbt27XTo0CFJUkREhCQpPz/fbUx+fr69LiIiQgUFBW7rT58+rWPHjrmNOdc2zn6OXwoICFBwcLDbDQAAXL6qdCF4q1at9Oyzz+rzzz9XdHS0ateu7bZ+5MiR1TK5G2+8Ufv373dbduDAAftTfFFRUYqIiFBmZqY6duwo6edPwm3cuFEjRoyQJMXGxqqwsFA5OTmKiYmRJK1evVrl5eXq0qWLPeYvf/mLysrK7H3JyMhQmzZt3D6pBwAArlxViqZ58+apXr16ysrKUlZWlts6Hx+faoum0aNHq1u3bnrppZd07733atOmTZo3b57mzZtnP9eoUaP04osvqnXr1vZXDkRGRqp///6Sfj4z1bt3b/ttvbKyMiUnJ2vgwIGKjIyUJA0aNEjPP/+8EhISNH78eO3atUuvvvqqXnnllWrZDwAAUPNVKZpyc3Orex7ndMMNN2jJkiWaMGGCXnjhBUVFRWnWrFmKj4+3x4wbN07FxcVKTExUYWGhbrrpJq1cudL+jiZJSk9PV3Jysm677Tb5+vpqwIABmj17tr0+JCREq1atUlJSkmJiYtS4cWOlpKTwdQMAAMDmY1mW5elJXA5cLpdCQkJUVFR0Udc3xYxdWI2zQk2XM32wp6fAMQk33nBMAtXpQn5/V+lM0/n+VMpbb71Vlc0CAAB4rSpF0y+/8LGsrEy7du1SYWHhOf+QLwAAQE1XpWhasmRJpWXl5eUaMWKErr766oueFAAAgLep0vc0nXNDvr4aM2YMnzgDAACXpWqLJkn68ssvdfr06ercJAAAgFeo0ttzY8aMcbtvWZaOHj2qFStWaMiQIdUyMQAAAG9SpWjaunWr231fX1+FhYVpxowZ5/1kHQAAQE1UpWj67LPPqnseAAAAXq1K0VThu+++s/82XJs2bRQWFlYtkwIAAPA2VboQvLi4WEOHDlXTpk3Vo0cP9ejRQ5GRkUpISNDJkyere44AAAAeV6VoGjNmjLKysvThhx+qsLBQhYWF+uCDD5SVlaUnn3yyuucIAADgcVV6e+7vf/+73n//fd1yyy32sjvvvFNBQUG69957NXfu3OqaHwAAgFeo0pmmkydPKjw8vNLyJk2a8PYcAAC4LFUpmmJjY/Xcc8/p1KlT9rKffvpJzz//vGJjY6ttcgAAAN6iSm/PzZo1S71791azZs3UoUMHSdL27dsVEBCgVatWVesEAQAAvEGVoik6OloHDx5Uenq69u3bJ0m6//77FR8fr6CgoGqdIAAAgDeoUjSlpqYqPDxcw4YNc1v+1ltv6bvvvtP48eOrZXIAAADeokrXNL3xxhtq27ZtpeXXXHON0tLSLnpSAAAA3qZK0ZSXl6emTZtWWh4WFqajR49e9KQAAAC8TZWiyeFwaP369ZWWr1+/XpGRkRc9KQAAAG9TpWuahg0bplGjRqmsrEw9e/aUJGVmZmrcuHF8IzgAALgsVSmaxo4dqx9++EGPPfaYSktLJUmBgYEaP368JkyYUK0TBAAA8AZViiYfHx9NnTpVzz77rPbu3augoCC1bt1aAQEB1T0/AAAAr1ClaKpQr1493XDDDdU1FwAAAK9VpQvBAQAArjREEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGKhR0TRlyhT5+Pho1KhR9rJTp04pKSlJjRo1Ur169TRgwADl5+e7Pe7QoUPq06eP6tSpoyZNmmjs2LE6ffq025g1a9aoU6dOCggIUKtWrTR//vxLsEcAAKCmqDHRtHnzZr3xxhu67rrr3JaPHj1aH374od577z1lZWXpyJEjuvvuu+31Z86cUZ8+fVRaWqoNGzZowYIFmj9/vlJSUuwxubm56tOnj2699VZt27ZNo0aN0iOPPKJPPvnkku0fAADwbjUimk6cOKH4+Hi9+eabatCggb28qKhI//3f/62ZM2eqZ8+eiomJ0dtvv60NGzbo888/lyStWrVKe/bs0f/8z/+oY8eOuuOOOzRp0iS9/vrrKi0tlSSlpaUpKipKM2bMULt27ZScnKw//elPeuWVV351TiUlJXK5XG43AABw+aoR0ZSUlKQ+ffooLi7ObXlOTo7Kysrclrdt21bNmzdXdna2JCk7O1vR0dEKDw+3xzidTrlcLu3evdse88ttO51OexvnkpqaqpCQEPvmcDguej8BAID38vpoeuedd/TFF18oNTW10rq8vDz5+/srNDTUbXl4eLjy8vLsMWcHU8X6inW/Ncblcumnn34657wmTJigoqIi+3b48OEq7R8AAKgZ/Dw9gd9y+PBhPfHEE8rIyFBgYKCnp+MmICBAAQEBnp4GAAC4RLz6TFNOTo4KCgrUqVMn+fn5yc/PT1lZWZo9e7b8/PwUHh6u0tJSFRYWuj0uPz9fERERkqSIiIhKn6aruH++McHBwQoKCvqd9g4AANQkXh1Nt912m3bu3Klt27bZt86dOys+Pt7+d+3atZWZmWk/Zv/+/Tp06JBiY2MlSbGxsdq5c6cKCgrsMRkZGQoODlb79u3tMWdvo2JMxTYAAAC8+u25+vXr69prr3VbVrduXTVq1MhenpCQoDFjxqhhw4YKDg7W448/rtjYWHXt2lWS1KtXL7Vv314PPvigpk2bpry8PD3zzDNKSkqy314bPny4XnvtNY0bN05Dhw7V6tWr9e6772rFihWXdocBAIDX8upoMvHKK6/I19dXAwYMUElJiZxOp/7rv/7LXl+rVi0tX75cI0aMUGxsrOrWrashQ4bohRdesMdERUVpxYoVGj16tF599VU1a9ZMf/3rX+V0Oj2xSwAAwAv5WJZleXoSlwOXy6WQkBAVFRUpODi4ytuJGbuwGmeFmi5n+mBPT4FjEm684ZgEqtOF/P726muaAAAAvAXRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADHh1NKWmpuqGG25Q/fr11aRJE/Xv31/79+93G3Pq1CklJSWpUaNGqlevngYMGKD8/Hy3MYcOHVKfPn1Up04dNWnSRGPHjtXp06fdxqxZs0adOnVSQECAWrVqpfnz5//euwcAAGoQr46mrKwsJSUl6fPPP1dGRobKysrUq1cvFRcX22NGjx6tDz/8UO+9956ysrJ05MgR3X333fb6M2fOqE+fPiotLdWGDRu0YMECzZ8/XykpKfaY3Nxc9enTR7feequ2bdumUaNG6ZFHHtEnn3xySfcXAAB4Lx/LsixPT8LUd999pyZNmigrK0s9evRQUVGRwsLCtGjRIv3pT3+SJO3bt0/t2rVTdna2unbtqo8//lh33XWXjhw5ovDwcElSWlqaxo8fr++++07+/v4aP368VqxYoV27dtnPNXDgQBUWFmrlypVGc3O5XAoJCVFRUZGCg4OrvI8xYxdW+bG4/ORMH+zpKXBMwo03HJNAdbqQ399efabpl4qKiiRJDRs2lCTl5OSorKxMcXFx9pi2bduqefPmys7OliRlZ2crOjraDiZJcjqdcrlc2r17tz3m7G1UjKnYxrmUlJTI5XK53QAAwOWrxkRTeXm5Ro0apRtvvFHXXnutJCkvL0/+/v4KDQ11GxseHq68vDx7zNnBVLG+Yt1vjXG5XPrpp5/OOZ/U1FSFhITYN4fDcdH7CAAAvFeNiaakpCTt2rVL77zzjqenIkmaMGGCioqK7Nvhw4c9PSUAAPA78vP0BEwkJydr+fLlWrt2rZo1a2Yvj4iIUGlpqQoLC93ONuXn5ysiIsIes2nTJrftVXy67uwxv/zEXX5+voKDgxUUFHTOOQUEBCggIOCi9w0AANQMXn2mybIsJScna8mSJVq9erWioqLc1sfExKh27drKzMy0l+3fv1+HDh1SbGysJCk2NlY7d+5UQUGBPSYjI0PBwcFq3769PebsbVSMqdgGAACAV59pSkpK0qJFi/TBBx+ofv369jVIISEhCgoKUkhIiBISEjRmzBg1bNhQwcHBevzxxxUbG6uuXbtKknr16qX27dvrwQcf1LRp05SXl6dnnnlGSUlJ9pmi4cOH67XXXtO4ceM0dOhQrV69Wu+++65WrFjhsX0HAADexavPNM2dO1dFRUW65ZZb1LRpU/u2ePFie8wrr7yiu+66SwMGDFCPHj0UERGhf/zjH/b6WrVqafny5apVq5ZiY2P1wAMPaPDgwXrhhRfsMVFRUVqxYoUyMjLUoUMHzZgxQ3/961/ldDov6f4CAADv5dVnmky+QiowMFCvv/66Xn/99V8d06JFC3300Ue/uZ1bbrlFW7duveA5AgCAK4NXn2kCAADwFkQTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABP09PAACACxEzdqGnpwAvkzN98CV5Hs40AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIpl94/fXX1bJlSwUGBqpLly7atGmTp6cEAAC8ANF0lsWLF2vMmDF67rnn9MUXX6hDhw5yOp0qKCjw9NQAAICHEU1nmTlzpoYNG6aHH35Y7du3V1pamurUqaO33nrL01MDAAAe5ufpCXiL0tJS5eTkaMKECfYyX19fxcXFKTs7u9L4kpISlZSU2PeLiookSS6X66Lmcabkp4t6PC4vF3s8VQeOSZyNYxLe6GKOy4rHWpZ13rFE0799//33OnPmjMLDw92Wh4eHa9++fZXGp6am6vnnn6+03OFw/G5zxJUnZM5wT08BcMMxCW9UHcfljz/+qJCQkN8cQzRV0YQJEzRmzBj7fnl5uY4dO6ZGjRrJx8fHgzOr+VwulxwOhw4fPqzg4GBPTwfgmITX4ZisPpZl6ccff1RkZOR5xxJN/9a4cWPVqlVL+fn5bsvz8/MVERFRaXxAQIACAgLcloWGhv6eU7ziBAcH82IAr8IxCW/DMVk9zneGqQIXgv+bv7+/YmJilJmZaS8rLy9XZmamYmNjPTgzAADgDTjTdJYxY8ZoyJAh6ty5s/7zP/9Ts2bNUnFxsR5++GFPTw0AAHgY0XSW++67T999951SUlKUl5enjh07auXKlZUuDsfvKyAgQM8991yltz8BT+GYhLfhmPQMH8vkM3YAAABXOK5pAgAAMEA0AQAAGCCaAAAADBBNqPFatmypWbNmeXoauAytWbNGPj4+Kiws/M1xHIO4nJge91ciogm/6aGHHpKPj4+mTJnitnzp0qWX/JvP58+ff84vEN28ebMSExMv6VzgXSqOUx8fH/n7+6tVq1Z64YUXdPr06Yvabrdu3XT06FH7i+84BnEhLtXr59dffy0fHx9t27at2raJcyOacF6BgYGaOnWqjh8/7umpnFNYWJjq1Knj6WnAw3r37q2jR4/q4MGDevLJJzVx4kRNnz79orbp7++viIiI8/6C4xjEr/Gm18/S0lJPT6HGI5pwXnFxcYqIiFBqauqvjlm3bp26d++uoKAgORwOjRw5UsXFxfb6o0ePqk+fPgoKClJUVJQWLVpU6S2NmTNnKjo6WnXr1pXD4dBjjz2mEydOSPr5dPHDDz+soqIi+4zCxIkTJbm/NTJo0CDdd999bnMrKytT48aNtXDhQkk/f9N7amqqoqKiFBQUpA4dOuj999+vhp8UPCkgIEARERFq0aKFRowYobi4OC1btkzHjx/X4MGD1aBBA9WpU0d33HGHDh48aD/uX//6l/r27asGDRqobt26uuaaa/TRRx9Jcn+bgmMQVVEdr58+Pj5aunSp22NCQ0M1f/58SVJUVJQk6frrr5ePj49uueUWST+f6erfv78mT56syMhItWnTRpL0t7/9TZ07d1b9+vUVERGhQYMGqaCgoPp2+jJGNOG8atWqpZdeeklz5szRN998U2n9l19+qd69e2vAgAHasWOHFi9erHXr1ik5OdkeM3jwYB05ckRr1qzR3//+d82bN6/Sf6S+vr6aPXu2du/erQULFmj16tUaN26cpJ/fJpk1a5aCg4N19OhRHT16VE899VSlucTHx+vDDz+0Y0uSPvnkE508eVJ//OMfJUmpqalauHCh0tLStHv3bo0ePVoPPPCAsrKyquXnBe8QFBSk0tJSPfTQQ9qyZYuWLVum7OxsWZalO++8U2VlZZKkpKQklZSUaO3atdq5c6emTp2qevXqVdoexyCqojpeP89n06ZNkqRPP/1UR48e1T/+8Q97XWZmpvbv36+MjAwtX75c0s8RP2nSJG3fvl1Lly7V119/rYceeujidvRKYQG/YciQIVa/fv0sy7Ksrl27WkOHDrUsy7KWLFliVRw+CQkJVmJiotvj/vnPf1q+vr7WTz/9ZO3du9eSZG3evNlef/DgQUuS9corr/zqc7/33ntWo0aN7Ptvv/22FRISUmlcixYt7O2UlZVZjRs3thYuXGivv//++6377rvPsizLOnXqlFWnTh1rw4YNbttISEiw7r///t/+YcBrnX2clpeXWxkZGVZAQIDVv39/S5K1fv16e+z3339vBQUFWe+++65lWZYVHR1tTZw48Zzb/eyzzyxJ1vHjxy3L4hjEhamO10/LsixJ1pIlS9zGhISEWG+//bZlWZaVm5trSbK2bt1a6fnDw8OtkpKS35zn5s2bLUnWjz/+aFlW5eMe/x9/RgXGpk6dqp49e1b6v+vt27drx44dSk9Pt5dZlqXy8nLl5ubqwIED8vPzU6dOnez1rVq1UoMGDdy28+mnnyo1NVX79u2Ty+XS6dOnderUKZ08edL4ehE/Pz/de++9Sk9P14MPPqji4mJ98MEHeueddyRJ//d//6eTJ0/q9ttvd3tcaWmprr/++gv6ecC7LF++XPXq1VNZWZnKy8s1aNAg3X333Vq+fLm6dOlij2vUqJHatGmjvXv3SpJGjhypESNGaNWqVYqLi9OAAQN03XXXVXkeHIM4l6q+frZr1+6injc6Olr+/v5uy3JycjRx4kRt375dx48fV3l5uSTp0KFDat++/UU93+WOaIKxHj16yOl0asKECW6nck+cOKFHH31UI0eOrPSY5s2b68CBA+fd9tdff6277rpLI0aM0OTJk9WwYUOtW7dOCQkJKi0tvaCLbOPj43XzzTeroKBAGRkZCgoKUu/eve25StKKFSt01VVXuT2Ov+FUs916662aO3eu/P39FRkZKT8/Py1btuy8j3vkkUfkdDq1YsUKrVq1SqmpqZoxY4Yef/zxKs+FYxC/VNXXT+nna5qsX/zFs4q3l8+nbt26bveLi4vldDrldDqVnp6usLAwHTp0SE6nkwvFDRBNuCBTpkxRx44d7QsKJalTp07as2ePWrVqdc7HtGnTRqdPn9bWrVsVExMj6ef/2z770yQ5OTkqLy/XjBkz5Ov786V27777rtt2/P39debMmfPOsVu3bnI4HFq8eLE+/vhj3XPPPapdu7YkqX379goICNChQ4d08803X9jOw6vVrVu30jHYrl07nT59Whs3blS3bt0kST/88IP279/v9n/UDodDw4cP1/DhwzVhwgS9+eab54wmjkFcjKq8fko/fzrz6NGj9v2DBw/q5MmT9v2KM0kmx+a+ffv0ww8/aMqUKXI4HJKkLVu2XPC+XKmIJlyQ6OhoxcfHa/bs2fay8ePHq2vXrkpOTtYjjzyiunXras+ePcrIyNBrr72mtm3bKi4uTomJiZo7d65q166tJ598UkFBQfZHuVu1aqWysjLNmTNHffv21fr165WWlub23C1bttSJEyeUmZmpDh06qE6dOr96BmrQoEFKS0vTgQMH9Nlnn9nL69evr6eeekqjR49WeXm5brrpJhUVFWn9+vUKDg7WkCFDfoefGjyldevW6tevn4YNG6Y33nhD9evX19NPP62rrrpK/fr1kySNGjVKd9xxh/7jP/5Dx48f12efffarb4lwDOJiVOX1U5J69uyp1157TbGxsTpz5ozGjx9vR7gkNWnSREFBQVq5cqWaNWumwMBA+7vFfql58+by9/fXnDlzNHz4cO3atUuTJk36fXf8cuLha6rg5c6+kLFCbm6u5e/vb519+GzatMm6/fbbrXr16ll169a1rrvuOmvy5Mn2+iNHjlh33HGHFRAQYLVo0cJatGiR1aRJEystLc0eM3PmTKtp06ZWUFCQ5XQ6rYULF1a6GHH48OFWo0aNLEnWc889Z1mW+0W4Ffbs2WNJslq0aGGVl5e7rSsvL7dmzZpltWnTxqpdu7YVFhZmOZ1OKysr6+J+WPCYcx2nFY4dO2Y9+OCDVkhIiH1sHThwwF6fnJxsXX311VZAQIAVFhZmPfjgg9b3339vWda5L4jlGISp6nr9/Pbbb61evXpZdevWtVq3bm199NFHbheCW5Zlvfnmm5bD4bB8fX2tm2+++Vef37Isa9GiRVbLli2tgIAAKzY21lq2bJnbheRcCP7rfCzrF2+UApfAN998I4fDoU8//VS33Xabp6cDAMB5EU24JFavXq0TJ04oOjpaR48e1bhx4/Ttt9/qwIEDbqeZAQDwVlzThEuirKxMf/7zn/XVV1+pfv366tatm9LT0wkmAECNwZkmAAAAA/wZFQAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0Abhi3HLLLRo1apTR2DVr1sjHx0eFhYUX9ZwtW7bUrFmzLmobALwD0QQAAGCAaAIAADBANAG4Iv3tb39T586dVb9+fUVERGjQoEEqKCioNG79+vW67rrrFBgYqK5du2rXrl1u69etW6fu3bsrKChIDodDI0eOVHFx8aXaDQCXENEE4IpUVlamSZMmafv27Vq6dKm+/vprPfTQQ5XGjR07VjNmzNDmzZsVFhamvn37qqysTJL05Zdfqnfv3howYIB27NihxYsXa926dUpOTr7EewPgUuBvzwG4Ig0dOtT+9x/+8AfNnj1bN9xwg06cOKF69erZ65577jndfvvtkqQFCxaoWbNmWrJkie69916lpqYqPj7evri8devWmj17tm6++WbNnTtXgYGBl3SfAPy+ONME4IqUk5Ojvn37qnnz5qpfv75uvvlmSdKhQ4fcxsXGxtr/btiwodq0aaO9e/dKkrZv36758+erXr169s3pdKq8vFy5ubmXbmcAXBKcaQJwxSkuLpbT6ZTT6VR6errCwsJ06NAhOZ1OlZaWGm/nxIkTevTRRzVy5MhK65o3b16dUwbgBYgmAFecffv26YcfftCUKVPkcDgkSVu2bDnn2M8//9wOoOPHj+vAgQNq166dJKlTp07as2ePWrVqdWkmDsCjeHsOwBWnefPm8vf315w5c/TVV19p2bJlmjRp0jnHvvDCC8rMzNSuXbv00EMPqXHjxurfv78kafz48dqwYYOSk5O1bds2HTx4UB988AEXggOXKaIJwBUnLCxM8+fP13vvvaf27dtrypQpevnll885dsqUKXriiScUExOjvLw8ffjhh/L395ckXXfddcrKytKBAwfUvXt3XX/99UpJSVFkZOSl3B0Al4iPZVmWpycBAADg7TjTBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAY+H/vGqf/ORNXxAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df =  get_file()\n",
    "\n",
    "label_map = {\n",
    "    'anger': 'Negative',\n",
    "    'sadness': 'Negative',\n",
    "    'fear': 'Negative',\n",
    "    'surprise': 'Neutral',\n",
    "    'joy': 'Positive',\n",
    "    'love': 'Positive',\n",
    "}\n",
    "\n",
    "df['label'] = df['label'].map(label_map) # changes the names of the labels to suit the 3 classes\n",
    "\n",
    "sns.countplot(df, x='label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the dataset is not balanced. So this might be an issue that might need to be investigated further if the models tends to be biased towards negative feelings. It might also introduce some challenges with the evaluation since it is not even. \n",
    "\n",
    "\n",
    "\n",
    "* TODO: Maybe check what the shortest amount of words is, and the most amount of the words in the data is and number of occurences. This is to know that padding the data is not introducing any dilation to my network and messes with the tanh acitvation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel so pissed off over an old friend and some friends</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i also feel it is unfortunate that nearly all the readers of going to meet the man wi will be african americans unlike myself</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel petty a href http clairee</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i used to believe that a feeling like fear was to be ignored or suppressed right away more on this in a moment</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>i was i might be buying stuff from there but i feel the clothes are too casual</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>i like sonam deepika and genelia who i feel are very talented and beautiful</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>i feel pathetic that i can hardly go a whole day not talking to hi am</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>i would have spent more ti ame with her on reading i feel a bit guilty about that</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>i do however feel like one of those pathetic girls who make up excuses because of a guy</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0                                                                                      i feel so pissed off over an old friend and some friends   \n",
       "1      i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated   \n",
       "2                 i also feel it is unfortunate that nearly all the readers of going to meet the man wi will be african americans unlike myself   \n",
       "3                                                                                                              i feel petty a href http clairee   \n",
       "4                                i used to believe that a feeling like fear was to be ignored or suppressed right away more on this in a moment   \n",
       "...                                                                                                                                         ...   \n",
       "19995                                                            i was i might be buying stuff from there but i feel the clothes are too casual   \n",
       "19996                                                               i like sonam deepika and genelia who i feel are very talented and beautiful   \n",
       "19997                                                                     i feel pathetic that i can hardly go a whole day not talking to hi am   \n",
       "19998                                                         i would have spent more ti ame with her on reading i feel a bit guilty about that   \n",
       "19999                                                   i do however feel like one of those pathetic girls who make up excuses because of a guy   \n",
       "\n",
       "          label  \n",
       "0      Negative  \n",
       "1      Negative  \n",
       "2      Negative  \n",
       "3      Negative  \n",
       "4      Negative  \n",
       "...         ...  \n",
       "19995  Positive  \n",
       "19996  Positive  \n",
       "19997  Negative  \n",
       "19998  Negative  \n",
       "19999  Negative  \n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_clean_text(df):\n",
    "    \"\"\"\"\n",
    "    Takes in a df column \"text\" and:\n",
    "    - converts everything to lowercase\n",
    "    - removes punctuation\n",
    "    - removes extra whitespaces\n",
    "    - adjustes words like \"ive\", \"ill\", im\" to its full form \"i have\", \"i will\", \"i am\"\n",
    "\n",
    "    Returns: cleaned df\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    full_word_mapping = {\n",
    "        'im': 'i am',\n",
    "        'ive': 'i have',\n",
    "        'ill': 'i will',\n",
    "        'id': 'i would',\n",
    "    }\n",
    "\n",
    "    df['text'] = df['text'].replace(full_word_mapping, regex=True) # replaces the words to its full form\n",
    "\n",
    "    df['text'] = df['text'].str.lower() # makes everything lowercase\n",
    "\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(x.split())) # removes the extra whitespaces\n",
    "\n",
    "    df['text'] = df['text'].str.replace(f'[{string.punctuation}]', '', regex=True) # removes the punctuation\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "get_clean_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel so pissed off over an old friend and some friends</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, feel, so, pissed, off, over, an, old, friend, and, some, friends]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, have, found, it, has, made, a, huge, difference, especially, on, the, finger, with, my, ring, and, the, my, skin, feels, so, much, softer, and, less, irritated]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i also feel it is unfortunate that nearly all the readers of going to meet the man wi will be african americans unlike myself</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, also, feel, it, is, unfortunate, that, nearly, all, the, readers, of, going, to, meet, the, man, wi, will, be, african, americans, unlike, myself]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel petty a href http clairee</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, feel, petty, a, href, http, clairee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i used to believe that a feeling like fear was to be ignored or suppressed right away more on this in a moment</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, used, to, believe, that, a, feeling, like, fear, was, to, be, ignored, or, suppressed, right, away, more, on, this, in, a, moment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>i was i might be buying stuff from there but i feel the clothes are too casual</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[i, was, i, might, be, buying, stuff, from, there, but, i, feel, the, clothes, are, too, casual]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>i like sonam deepika and genelia who i feel are very talented and beautiful</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[i, like, sonam, deepika, and, genelia, who, i, feel, are, very, talented, and, beautiful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>i feel pathetic that i can hardly go a whole day not talking to hi am</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, feel, pathetic, that, i, can, hardly, go, a, whole, day, not, talking, to, hi, am]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>i would have spent more ti ame with her on reading i feel a bit guilty about that</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, would, have, spent, more, ti, ame, with, her, on, reading, i, feel, a, bit, guilty, about, that]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>i do however feel like one of those pathetic girls who make up excuses because of a guy</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[i, do, however, feel, like, one, of, those, pathetic, girls, who, make, up, excuses, because, of, a, guy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0                                                                                      i feel so pissed off over an old friend and some friends   \n",
       "1      i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated   \n",
       "2                 i also feel it is unfortunate that nearly all the readers of going to meet the man wi will be african americans unlike myself   \n",
       "3                                                                                                              i feel petty a href http clairee   \n",
       "4                                i used to believe that a feeling like fear was to be ignored or suppressed right away more on this in a moment   \n",
       "...                                                                                                                                         ...   \n",
       "19995                                                            i was i might be buying stuff from there but i feel the clothes are too casual   \n",
       "19996                                                               i like sonam deepika and genelia who i feel are very talented and beautiful   \n",
       "19997                                                                     i feel pathetic that i can hardly go a whole day not talking to hi am   \n",
       "19998                                                         i would have spent more ti ame with her on reading i feel a bit guilty about that   \n",
       "19999                                                   i do however feel like one of those pathetic girls who make up excuses because of a guy   \n",
       "\n",
       "          label  \\\n",
       "0      Negative   \n",
       "1      Negative   \n",
       "2      Negative   \n",
       "3      Negative   \n",
       "4      Negative   \n",
       "...         ...   \n",
       "19995  Positive   \n",
       "19996  Positive   \n",
       "19997  Negative   \n",
       "19998  Negative   \n",
       "19999  Negative   \n",
       "\n",
       "                                                                                                                                                             tokenized_text  \n",
       "0                                                                                                     [i, feel, so, pissed, off, over, an, old, friend, and, some, friends]  \n",
       "1      [i, have, found, it, has, made, a, huge, difference, especially, on, the, finger, with, my, ring, and, the, my, skin, feels, so, much, softer, and, less, irritated]  \n",
       "2                    [i, also, feel, it, is, unfortunate, that, nearly, all, the, readers, of, going, to, meet, the, man, wi, will, be, african, americans, unlike, myself]  \n",
       "3                                                                                                                                  [i, feel, petty, a, href, http, clairee]  \n",
       "4                                    [i, used, to, believe, that, a, feeling, like, fear, was, to, be, ignored, or, suppressed, right, away, more, on, this, in, a, moment]  \n",
       "...                                                                                                                                                                     ...  \n",
       "19995                                                                      [i, was, i, might, be, buying, stuff, from, there, but, i, feel, the, clothes, are, too, casual]  \n",
       "19996                                                                            [i, like, sonam, deepika, and, genelia, who, i, feel, are, very, talented, and, beautiful]  \n",
       "19997                                                                                [i, feel, pathetic, that, i, can, hardly, go, a, whole, day, not, talking, to, hi, am]  \n",
       "19998                                                                  [i, would, have, spent, more, ti, ame, with, her, on, reading, i, feel, a, bit, guilty, about, that]  \n",
       "19999                                                            [i, do, however, feel, like, one, of, those, pathetic, girls, who, make, up, excuses, because, of, a, guy]  \n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(df):\n",
    "    \"\"\"\"\n",
    "    Takes in a df column\n",
    "\n",
    "    Returns: column with the text tokenized and each row is corresponding to correct index\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    df['tokenized_text'] = df['text'].apply(nltk.word_tokenize) # tokenizes the text\n",
    "\n",
    "    # tokenized_text = df['tokenized_text'].explode().reset_index() # transform each element and replicate index values to match each row. \n",
    "\n",
    "\n",
    "    # display(tokenized_text['index'].unique()) # this is to see that the index matches the original df rows (20K)\n",
    "\n",
    "    return df\n",
    "\n",
    "tokenize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df, STOPWORDS_DEFAULT):\n",
    "    \"\"\" Remove stopwords from the tokenized text (df) \"\"\"\n",
    "\n",
    "    df['tokenized_text'] = df['tokenized_text'].apply(lambda words: [word for word in words if word not in STOPWORDS_DEFAULT])\n",
    "\n",
    "    return\n",
    "\n",
    "remove_stopwords(df, STOPWORDS_DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel so pissed off over an old friend and some friends</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[feel, pissed, old, friend, friends]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[found, made, huge, difference, especially, finger, ring, skin, feels, much, softer, less, irritated]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i also feel it is unfortunate that nearly all the readers of going to meet the man wi will be african americans unlike myself</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[also, feel, unfortunate, nearly, readers, going, meet, man, wi, african, americans, unlike]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel petty a href http clairee</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[feel, petty, href, http, clairee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i used to believe that a feeling like fear was to be ignored or suppressed right away more on this in a moment</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[used, believe, feeling, like, fear, ignored, suppressed, right, away, moment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>i was i might be buying stuff from there but i feel the clothes are too casual</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[might, buying, stuff, feel, clothes, casual]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>i like sonam deepika and genelia who i feel are very talented and beautiful</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[like, sonam, deepika, genelia, feel, talented, beautiful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>i feel pathetic that i can hardly go a whole day not talking to hi am</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[feel, pathetic, hardly, go, whole, day, talking, hi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>i would have spent more ti ame with her on reading i feel a bit guilty about that</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[would, spent, ti, ame, reading, feel, bit, guilty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>i do however feel like one of those pathetic girls who make up excuses because of a guy</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[however, feel, like, one, pathetic, girls, make, excuses, guy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0                                                                                      i feel so pissed off over an old friend and some friends   \n",
       "1      i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated   \n",
       "2                 i also feel it is unfortunate that nearly all the readers of going to meet the man wi will be african americans unlike myself   \n",
       "3                                                                                                              i feel petty a href http clairee   \n",
       "4                                i used to believe that a feeling like fear was to be ignored or suppressed right away more on this in a moment   \n",
       "...                                                                                                                                         ...   \n",
       "19995                                                            i was i might be buying stuff from there but i feel the clothes are too casual   \n",
       "19996                                                               i like sonam deepika and genelia who i feel are very talented and beautiful   \n",
       "19997                                                                     i feel pathetic that i can hardly go a whole day not talking to hi am   \n",
       "19998                                                         i would have spent more ti ame with her on reading i feel a bit guilty about that   \n",
       "19999                                                   i do however feel like one of those pathetic girls who make up excuses because of a guy   \n",
       "\n",
       "          label  \\\n",
       "0      Negative   \n",
       "1      Negative   \n",
       "2      Negative   \n",
       "3      Negative   \n",
       "4      Negative   \n",
       "...         ...   \n",
       "19995  Positive   \n",
       "19996  Positive   \n",
       "19997  Negative   \n",
       "19998  Negative   \n",
       "19999  Negative   \n",
       "\n",
       "                                                                                              tokenized_text  \n",
       "0                                                                       [feel, pissed, old, friend, friends]  \n",
       "1      [found, made, huge, difference, especially, finger, ring, skin, feels, much, softer, less, irritated]  \n",
       "2               [also, feel, unfortunate, nearly, readers, going, meet, man, wi, african, americans, unlike]  \n",
       "3                                                                         [feel, petty, href, http, clairee]  \n",
       "4                             [used, believe, feeling, like, fear, ignored, suppressed, right, away, moment]  \n",
       "...                                                                                                      ...  \n",
       "19995                                                          [might, buying, stuff, feel, clothes, casual]  \n",
       "19996                                             [like, sonam, deepika, genelia, feel, talented, beautiful]  \n",
       "19997                                                  [feel, pathetic, hardly, go, whole, day, talking, hi]  \n",
       "19998                                                    [would, spent, ti, ame, reading, feel, bit, guilty]  \n",
       "19999                                        [however, feel, like, one, pathetic, girls, make, excuses, guy]  \n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "* Reduce the words to their base form. for instance \"running\" -> \"run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel so pissed off over an old friend and some friends</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[feel, pissed, old, friend, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[found, made, huge, difference, especially, finger, ring, skin, feel, much, softer, le, irritated]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i also feel it is unfortunate that nearly all the readers of going to meet the man wi will be african americans unlike myself</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[also, feel, unfortunate, nearly, reader, going, meet, man, wi, african, american, unlike]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel petty a href http clairee</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[feel, petty, href, http, clairee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i used to believe that a feeling like fear was to be ignored or suppressed right away more on this in a moment</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[used, believe, feeling, like, fear, ignored, suppressed, right, away, moment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>i was i might be buying stuff from there but i feel the clothes are too casual</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[might, buying, stuff, feel, clothes, casual]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>i like sonam deepika and genelia who i feel are very talented and beautiful</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[like, sonam, deepika, genelia, feel, talented, beautiful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>i feel pathetic that i can hardly go a whole day not talking to hi am</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[feel, pathetic, hardly, go, whole, day, talking, hi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>i would have spent more ti ame with her on reading i feel a bit guilty about that</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[would, spent, ti, ame, reading, feel, bit, guilty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>i do however feel like one of those pathetic girls who make up excuses because of a guy</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[however, feel, like, one, pathetic, girl, make, excuse, guy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0                                                                                      i feel so pissed off over an old friend and some friends   \n",
       "1      i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated   \n",
       "2                 i also feel it is unfortunate that nearly all the readers of going to meet the man wi will be african americans unlike myself   \n",
       "3                                                                                                              i feel petty a href http clairee   \n",
       "4                                i used to believe that a feeling like fear was to be ignored or suppressed right away more on this in a moment   \n",
       "...                                                                                                                                         ...   \n",
       "19995                                                            i was i might be buying stuff from there but i feel the clothes are too casual   \n",
       "19996                                                               i like sonam deepika and genelia who i feel are very talented and beautiful   \n",
       "19997                                                                     i feel pathetic that i can hardly go a whole day not talking to hi am   \n",
       "19998                                                         i would have spent more ti ame with her on reading i feel a bit guilty about that   \n",
       "19999                                                   i do however feel like one of those pathetic girls who make up excuses because of a guy   \n",
       "\n",
       "          label  \\\n",
       "0      Negative   \n",
       "1      Negative   \n",
       "2      Negative   \n",
       "3      Negative   \n",
       "4      Negative   \n",
       "...         ...   \n",
       "19995  Positive   \n",
       "19996  Positive   \n",
       "19997  Negative   \n",
       "19998  Negative   \n",
       "19999  Negative   \n",
       "\n",
       "                                                                                           tokenized_text  \n",
       "0                                                                     [feel, pissed, old, friend, friend]  \n",
       "1      [found, made, huge, difference, especially, finger, ring, skin, feel, much, softer, le, irritated]  \n",
       "2              [also, feel, unfortunate, nearly, reader, going, meet, man, wi, african, american, unlike]  \n",
       "3                                                                      [feel, petty, href, http, clairee]  \n",
       "4                          [used, believe, feeling, like, fear, ignored, suppressed, right, away, moment]  \n",
       "...                                                                                                   ...  \n",
       "19995                                                       [might, buying, stuff, feel, clothes, casual]  \n",
       "19996                                          [like, sonam, deepika, genelia, feel, talented, beautiful]  \n",
       "19997                                               [feel, pathetic, hardly, go, whole, day, talking, hi]  \n",
       "19998                                                 [would, spent, ti, ame, reading, feel, bit, guilty]  \n",
       "19999                                       [however, feel, like, one, pathetic, girl, make, excuse, guy]  \n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize(df, lemmatizer):\n",
    "    \"\"\" Lemmatize list of words in the tokenized_text column df \"\"\"\n",
    "\n",
    "    df['tokenized_text'] = df['tokenized_text'].apply(lambda words: [lemmatizer.lemmatize(word) for word in words])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "lemmatize(df, LEMMATIZER)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign a unique number to each unique word\n",
    "\n",
    "- We need to assign a unique number to each unique word to vectorize it.\n",
    "- Also i change the labels positive/neutral/negative to numerical numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in the text: 14970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'blogger': 0,\n",
       " 'pole': 1,\n",
       " 'prisoner': 2,\n",
       " 'wtf': 3,\n",
       " 'squeezed': 4,\n",
       " 'weighins': 5,\n",
       " 'charlie': 6,\n",
       " 'rep': 7,\n",
       " 'snoozing': 8,\n",
       " 'eaaaat': 9,\n",
       " 'tactic': 10,\n",
       " 'deeply': 11,\n",
       " 'vertigo': 12,\n",
       " 'music': 13,\n",
       " 'lacking': 14,\n",
       " 'destroyer': 15,\n",
       " 'expressing': 16,\n",
       " 'complicated': 17,\n",
       " 'fancy': 18,\n",
       " 'proclai': 19,\n",
       " 'martinez': 20,\n",
       " 'rebuild': 21,\n",
       " 'libi': 22,\n",
       " 'vintage': 23,\n",
       " 'lightly': 24,\n",
       " 'majority': 25,\n",
       " 'ambushed': 26,\n",
       " 'fabulous': 27,\n",
       " 'fate': 28,\n",
       " 'speed': 29,\n",
       " 'overthink': 30,\n",
       " 'hung': 31,\n",
       " 'freakin': 32,\n",
       " 'itge': 33,\n",
       " 'grumbling': 34,\n",
       " 'center': 35,\n",
       " 'roberto': 36,\n",
       " 'crushed': 37,\n",
       " 'reelected': 38,\n",
       " 'messaging': 39,\n",
       " 'club': 40,\n",
       " 'remarkably': 41,\n",
       " 'reed': 42,\n",
       " 'traveling': 43,\n",
       " 'tatsuma': 44,\n",
       " 'continued': 45,\n",
       " 'goin': 46,\n",
       " 'modest': 47,\n",
       " 'birthed': 48,\n",
       " 'representing': 49,\n",
       " 'hsp': 50,\n",
       " 'columbus': 51,\n",
       " 'boss': 52,\n",
       " 'realness': 53,\n",
       " 'semi': 54,\n",
       " 'fir': 55,\n",
       " 'scum': 56,\n",
       " 'collapse': 57,\n",
       " 'formed': 58,\n",
       " 'signal': 59,\n",
       " 'occassionally': 60,\n",
       " 'hahaha': 61,\n",
       " 'proffer': 62,\n",
       " 'harvesting': 63,\n",
       " 'questionable': 64,\n",
       " 'snowy': 65,\n",
       " 'sticking': 66,\n",
       " 'lizard': 67,\n",
       " 'pia': 68,\n",
       " 'bishan': 69,\n",
       " 'yesubais': 70,\n",
       " 'medal': 71,\n",
       " 'besi': 72,\n",
       " 'gland': 73,\n",
       " 'hae': 74,\n",
       " 'responding': 75,\n",
       " 'strasbourg': 76,\n",
       " 'selecti': 77,\n",
       " 'antique': 78,\n",
       " 'tanning': 79,\n",
       " 'bulletin': 80,\n",
       " 'soothe': 81,\n",
       " 'mba': 82,\n",
       " 'wet': 83,\n",
       " 'torturing': 84,\n",
       " 'breeder': 85,\n",
       " 'revisit': 86,\n",
       " 'wednesday': 87,\n",
       " 'abdomen': 88,\n",
       " 'start': 89,\n",
       " 'holly': 90,\n",
       " 'stitching': 91,\n",
       " 'therapeutic': 92,\n",
       " 'moment': 93,\n",
       " 'infact': 94,\n",
       " 'apawa': 95,\n",
       " 'thus': 96,\n",
       " 'pret': 97,\n",
       " 'change': 98,\n",
       " 'linkedin': 99,\n",
       " 'sharply': 100,\n",
       " 'wouldiosyncrasies': 101,\n",
       " 'willicit': 102,\n",
       " 'amaginary': 103,\n",
       " 'sensational': 104,\n",
       " 'serene': 105,\n",
       " 'privileged': 106,\n",
       " 'hows': 107,\n",
       " 'substitute': 108,\n",
       " 'soley': 109,\n",
       " 'bungalow': 110,\n",
       " 'clamoring': 111,\n",
       " 'voting': 112,\n",
       " 'affect': 113,\n",
       " 'chi': 114,\n",
       " 'restrained': 115,\n",
       " 'tgirl': 116,\n",
       " 'phantomwise': 117,\n",
       " 'leaning': 118,\n",
       " 'mudder': 119,\n",
       " 'amage': 120,\n",
       " 'lot': 121,\n",
       " 'clot': 122,\n",
       " 'shock': 123,\n",
       " 'upgrade': 124,\n",
       " 'homesick': 125,\n",
       " 'generous': 126,\n",
       " 'shi': 127,\n",
       " 'underage': 128,\n",
       " 'advance': 129,\n",
       " 'partnership': 130,\n",
       " 'beware': 131,\n",
       " 'pocket': 132,\n",
       " 'coughing': 133,\n",
       " 'flung': 134,\n",
       " 'arsenal': 135,\n",
       " 'ounce': 136,\n",
       " 'ammobile': 137,\n",
       " 'sarcoma': 138,\n",
       " 'mince': 139,\n",
       " 'rum': 140,\n",
       " 'headcold': 141,\n",
       " 'silver': 142,\n",
       " 'distribution': 143,\n",
       " 'grittily': 144,\n",
       " 'expose': 145,\n",
       " 'ei': 146,\n",
       " 'cath': 147,\n",
       " 'fourth': 148,\n",
       " 'chechen': 149,\n",
       " 'woulduals': 150,\n",
       " 'md': 151,\n",
       " 'foodie': 152,\n",
       " 'hatena': 153,\n",
       " 'constipated': 154,\n",
       " 'morning': 155,\n",
       " 'satisfied': 156,\n",
       " 'morbi': 157,\n",
       " 'grudgingly': 158,\n",
       " 'lapse': 159,\n",
       " 'woodstock': 160,\n",
       " 'bareminerals': 161,\n",
       " 'glass': 162,\n",
       " 'tradition': 163,\n",
       " 'pearly': 164,\n",
       " 'swedish': 165,\n",
       " 'vbac': 166,\n",
       " 'action': 167,\n",
       " 'despairing': 168,\n",
       " 'huhuhu': 169,\n",
       " 'ironically': 170,\n",
       " 'asian': 171,\n",
       " 'filter': 172,\n",
       " 'beacuse': 173,\n",
       " 'furniture': 174,\n",
       " 'xelliealicex': 175,\n",
       " 'earlier': 176,\n",
       " 'capturing': 177,\n",
       " 'broadcasting': 178,\n",
       " 'coz': 179,\n",
       " 'framed': 180,\n",
       " 'rockabi': 181,\n",
       " 'peek': 182,\n",
       " 'enmayi': 183,\n",
       " 'unspokenwords': 184,\n",
       " 'manicure': 185,\n",
       " 'certain': 186,\n",
       " 'get': 187,\n",
       " 'tire': 188,\n",
       " 'theologically': 189,\n",
       " 'plight': 190,\n",
       " 'fifth': 191,\n",
       " 'purposeless': 192,\n",
       " 'elashis': 193,\n",
       " 'pop': 194,\n",
       " 'counterproducti': 195,\n",
       " 'flood': 196,\n",
       " 'unhappiness': 197,\n",
       " 'reprieve': 198,\n",
       " 'orthotics': 199,\n",
       " 'kina': 200,\n",
       " 'attainable': 201,\n",
       " 'part': 202,\n",
       " 'whats': 203,\n",
       " 'humbly': 204,\n",
       " 'in': 205,\n",
       " 'approx': 206,\n",
       " 'unloving': 207,\n",
       " 'raping': 208,\n",
       " 'together': 209,\n",
       " 'earthly': 210,\n",
       " 'snobbish': 211,\n",
       " 'tribulation': 212,\n",
       " 'presentation': 213,\n",
       " 'satisfactory': 214,\n",
       " 'tacky': 215,\n",
       " 'kermit': 216,\n",
       " 'largely': 217,\n",
       " 'draw': 218,\n",
       " 'sek': 219,\n",
       " 'thing': 220,\n",
       " 'hatchet': 221,\n",
       " 'jason': 222,\n",
       " 'answered': 223,\n",
       " 'subdued': 224,\n",
       " 'homepage': 225,\n",
       " 'facilitate': 226,\n",
       " 'earliest': 227,\n",
       " 'evaluate': 228,\n",
       " 'fortunately': 229,\n",
       " 'exposed': 230,\n",
       " 'disabled': 231,\n",
       " 'exhaustion': 232,\n",
       " 'unresolved': 233,\n",
       " 'foreign': 234,\n",
       " 'distant': 235,\n",
       " 'deactivateing': 236,\n",
       " 'pilate': 237,\n",
       " 'accomplished': 238,\n",
       " 'refreshed': 239,\n",
       " 'replaced': 240,\n",
       " 'prostitute': 241,\n",
       " 'eartha': 242,\n",
       " 'teetering': 243,\n",
       " 'brings': 244,\n",
       " 'consolation': 245,\n",
       " 'producing': 246,\n",
       " 'sportsbook': 247,\n",
       " 'outlet': 248,\n",
       " 'willustrators': 249,\n",
       " 'lumia': 250,\n",
       " 'photography': 251,\n",
       " 'torned': 252,\n",
       " 'snap': 253,\n",
       " 'cutter': 254,\n",
       " 'left': 255,\n",
       " 'notifying': 256,\n",
       " 'exterior': 257,\n",
       " 'scare': 258,\n",
       " 'spat': 259,\n",
       " 'boudoir': 260,\n",
       " 'flirtiness': 261,\n",
       " 'giardina': 262,\n",
       " 'deemed': 263,\n",
       " 'wen': 264,\n",
       " 'morrow': 265,\n",
       " 'disco': 266,\n",
       " 'doodly': 267,\n",
       " 'pottering': 268,\n",
       " 'series': 269,\n",
       " 'who': 270,\n",
       " 'support': 271,\n",
       " 'crappy': 272,\n",
       " 'tutee': 273,\n",
       " 'confi': 274,\n",
       " 'mastered': 275,\n",
       " 'onel': 276,\n",
       " 'myriad': 277,\n",
       " 'out': 278,\n",
       " 'operation': 279,\n",
       " 'obsessi': 280,\n",
       " 'county': 281,\n",
       " 'cake': 282,\n",
       " 'castle': 283,\n",
       " 'wretched': 284,\n",
       " 'wouldhaire': 285,\n",
       " 'fatigue': 286,\n",
       " 'sparrow': 287,\n",
       " 'nanashi': 288,\n",
       " 'clumsy': 289,\n",
       " 'faithsedge': 290,\n",
       " 'mizuki': 291,\n",
       " 'exceptional': 292,\n",
       " 'hindu': 293,\n",
       " 'belmont': 294,\n",
       " 'sturdy': 295,\n",
       " 'electrical': 296,\n",
       " 'recite': 297,\n",
       " 'unusually': 298,\n",
       " 'threw': 299,\n",
       " 'stuck': 300,\n",
       " 'trusting': 301,\n",
       " 'aci': 302,\n",
       " 'pt': 303,\n",
       " 'examination': 304,\n",
       " 'instinct': 305,\n",
       " 'resentful': 306,\n",
       " 'read': 307,\n",
       " 'thou': 308,\n",
       " 'saloon': 309,\n",
       " 'affirmed': 310,\n",
       " 'consoled': 311,\n",
       " 'cleaned': 312,\n",
       " 'mushroom': 313,\n",
       " 'daiki': 314,\n",
       " 'momma': 315,\n",
       " 'amposed': 316,\n",
       " 'thearchitecturality': 317,\n",
       " 'mormonism': 318,\n",
       " 'inspect': 319,\n",
       " 'review': 320,\n",
       " 'scruncher': 321,\n",
       " 'programmed': 322,\n",
       " 'consists': 323,\n",
       " 'follow': 324,\n",
       " 'connecting': 325,\n",
       " 'apendages': 326,\n",
       " 'upswing': 327,\n",
       " 'mindset': 328,\n",
       " 'ing': 329,\n",
       " 'joining': 330,\n",
       " 'alongsi': 331,\n",
       " 'multiple': 332,\n",
       " 'scientology': 333,\n",
       " 'mindful': 334,\n",
       " 'seb': 335,\n",
       " 'inthewarmholdofyourlovingmind': 336,\n",
       " 'heat': 337,\n",
       " 'laxman': 338,\n",
       " 'hunger': 339,\n",
       " 'vaughn': 340,\n",
       " 'agonised': 341,\n",
       " 'everyone': 342,\n",
       " 'relaxed': 343,\n",
       " 'uninhabited': 344,\n",
       " 'sarcastic': 345,\n",
       " 'defined': 346,\n",
       " 'tortured': 347,\n",
       " 'topping': 348,\n",
       " 'winterpaysforsummer': 349,\n",
       " 'telstra': 350,\n",
       " 'housekeep': 351,\n",
       " 'detracted': 352,\n",
       " 'low': 353,\n",
       " 'sucked': 354,\n",
       " 'edinburgh': 355,\n",
       " 'cri': 356,\n",
       " 'stressing': 357,\n",
       " 'cardboard': 358,\n",
       " 'hugging': 359,\n",
       " 'dramatically': 360,\n",
       " 'location': 361,\n",
       " 'earley': 362,\n",
       " 'depth': 363,\n",
       " 'ended': 364,\n",
       " 'protesting': 365,\n",
       " 'factory': 366,\n",
       " 'surprisingly': 367,\n",
       " 'headless': 368,\n",
       " 'emphasis': 369,\n",
       " 'werent': 370,\n",
       " 'soothes': 371,\n",
       " 'hear': 372,\n",
       " 'lease': 373,\n",
       " 'observed': 374,\n",
       " 'languagedirection': 375,\n",
       " 'compassionate': 376,\n",
       " 'companion': 377,\n",
       " 'contractual': 378,\n",
       " 'encouragement': 379,\n",
       " 'ability': 380,\n",
       " 'meing': 381,\n",
       " 'interacti': 382,\n",
       " 'person': 383,\n",
       " 'foster': 384,\n",
       " 'provoking': 385,\n",
       " 'shhh': 386,\n",
       " 'adomen': 387,\n",
       " 'stressful': 388,\n",
       " 'witness': 389,\n",
       " 'sweater': 390,\n",
       " 'puters': 391,\n",
       " 'slew': 392,\n",
       " 'punishing': 393,\n",
       " 'election': 394,\n",
       " 'soaked': 395,\n",
       " 'dovetail': 396,\n",
       " 'assi': 397,\n",
       " 'seemad': 398,\n",
       " 'firmly': 399,\n",
       " 'disregarding': 400,\n",
       " 'cap': 401,\n",
       " 'theater': 402,\n",
       " 'advancement': 403,\n",
       " 'cami': 404,\n",
       " 'encoding': 405,\n",
       " 'ugliness': 406,\n",
       " 'nip': 407,\n",
       " 'thinker': 408,\n",
       " 'stammer': 409,\n",
       " 'peggy': 410,\n",
       " 'nesting': 411,\n",
       " 'frivolous': 412,\n",
       " 'oversharing': 413,\n",
       " 'mention': 414,\n",
       " 'plowing': 415,\n",
       " 'tremendous': 416,\n",
       " 'conversing': 417,\n",
       " 'ev': 418,\n",
       " 'kindda': 419,\n",
       " 'crawled': 420,\n",
       " 'example': 421,\n",
       " 'executed': 422,\n",
       " 'epers': 423,\n",
       " 'heartache': 424,\n",
       " 'lotte': 425,\n",
       " 'plainly': 426,\n",
       " 'moral': 427,\n",
       " 'dashed': 428,\n",
       " 'ffviii': 429,\n",
       " 'inn': 430,\n",
       " 'mag': 431,\n",
       " 'slut': 432,\n",
       " 'wrong': 433,\n",
       " 'glasberg': 434,\n",
       " 'enjoy': 435,\n",
       " 'geneva': 436,\n",
       " 'subscriber': 437,\n",
       " 'popeye': 438,\n",
       " 'departure': 439,\n",
       " 'lyme': 440,\n",
       " 'skinned': 441,\n",
       " 'complex': 442,\n",
       " 'later': 443,\n",
       " 'recovery': 444,\n",
       " 'apron': 445,\n",
       " 'tense': 446,\n",
       " 'cognize': 447,\n",
       " 'xviith': 448,\n",
       " 'mistake': 449,\n",
       " 'fee': 450,\n",
       " 'fooled': 451,\n",
       " 'wardrode': 452,\n",
       " 'suffered': 453,\n",
       " 'ou': 454,\n",
       " 'frog': 455,\n",
       " 'resolved': 456,\n",
       " 'wouldyllic': 457,\n",
       " 'beaubronz': 458,\n",
       " 'electrified': 459,\n",
       " 'obtain': 460,\n",
       " 'displeasing': 461,\n",
       " 'surreal': 462,\n",
       " 'yea': 463,\n",
       " 'picking': 464,\n",
       " 'leader': 465,\n",
       " 'siege': 466,\n",
       " 'messed': 467,\n",
       " 'shoelace': 468,\n",
       " 'fierceness': 469,\n",
       " 'warmth': 470,\n",
       " 'consumed': 471,\n",
       " 'babble': 472,\n",
       " 'fool': 473,\n",
       " 'coffee': 474,\n",
       " 'spent': 475,\n",
       " 'anansi': 476,\n",
       " 'affend': 477,\n",
       " 'sweaty': 478,\n",
       " 'orcis': 479,\n",
       " 'protection': 480,\n",
       " 'sounded': 481,\n",
       " 'newcomer': 482,\n",
       " 'thumb': 483,\n",
       " 'anymore': 484,\n",
       " 'pestici': 485,\n",
       " 'obedient': 486,\n",
       " 'erupt': 487,\n",
       " 'luar': 488,\n",
       " 'moisturising': 489,\n",
       " 'reminder': 490,\n",
       " 'infrastructure': 491,\n",
       " 'oil': 492,\n",
       " 'apologetic': 493,\n",
       " 'entirelly': 494,\n",
       " 'burlesque': 495,\n",
       " 'geremiafamily': 496,\n",
       " 'exertion': 497,\n",
       " 'ancient': 498,\n",
       " 'takeing': 499,\n",
       " 'defend': 500,\n",
       " 'fff': 501,\n",
       " 'interacting': 502,\n",
       " 'pedal': 503,\n",
       " 'accelerated': 504,\n",
       " 'morganton': 505,\n",
       " 'undertaking': 506,\n",
       " 'boo': 507,\n",
       " 'arched': 508,\n",
       " 'shoulding': 509,\n",
       " 'succumbed': 510,\n",
       " 'rearrange': 511,\n",
       " 'perusal': 512,\n",
       " 'report': 513,\n",
       " 'fug': 514,\n",
       " 'albino': 515,\n",
       " 'snoozel': 516,\n",
       " 'championship': 517,\n",
       " 'newly': 518,\n",
       " 'retarded': 519,\n",
       " 'gud': 520,\n",
       " 'amd': 521,\n",
       " 'alan': 522,\n",
       " 'dharavi': 523,\n",
       " 'inviting': 524,\n",
       " 'whether': 525,\n",
       " 'alll': 526,\n",
       " 'crochet': 527,\n",
       " 'suggested': 528,\n",
       " 'comfortably': 529,\n",
       " 'terrified': 530,\n",
       " 'daydreaming': 531,\n",
       " 'shy': 532,\n",
       " 'unspeakable': 533,\n",
       " 'siting': 534,\n",
       " 'bringing': 535,\n",
       " 'polling': 536,\n",
       " 'forget': 537,\n",
       " 'speaks': 538,\n",
       " 'mechanical': 539,\n",
       " 'qualified': 540,\n",
       " 'wouldori': 541,\n",
       " 'disharmony': 542,\n",
       " 'variety': 543,\n",
       " 'walia': 544,\n",
       " 'website': 545,\n",
       " 'spy': 546,\n",
       " 'munroe': 547,\n",
       " 'restless': 548,\n",
       " 'possibility': 549,\n",
       " 'monitoring': 550,\n",
       " 'damned': 551,\n",
       " 'glint': 552,\n",
       " 'special': 553,\n",
       " 'lee': 554,\n",
       " 'bro': 555,\n",
       " 'trapping': 556,\n",
       " 'kai': 557,\n",
       " 'evolve': 558,\n",
       " 'nuptials': 559,\n",
       " 'entrust': 560,\n",
       " 'donning': 561,\n",
       " 'employment': 562,\n",
       " 'budget': 563,\n",
       " 'flawless': 564,\n",
       " 'amprov': 565,\n",
       " 'niche': 566,\n",
       " 'lippmann': 567,\n",
       " 'alumninium': 568,\n",
       " 'scissors': 569,\n",
       " 'ammature': 570,\n",
       " 'lyric': 571,\n",
       " 'sauna': 572,\n",
       " 'jerked': 573,\n",
       " 'blood': 574,\n",
       " 'n': 575,\n",
       " 'lestat': 576,\n",
       " 'uw': 577,\n",
       " 'generalised': 578,\n",
       " 'lisas': 579,\n",
       " 'darned': 580,\n",
       " 'aoi': 581,\n",
       " 'glistening': 582,\n",
       " 'rightful': 583,\n",
       " 'rabiola': 584,\n",
       " 'doom': 585,\n",
       " 'worship': 586,\n",
       " 'ampotent': 587,\n",
       " 'permission': 588,\n",
       " 'studying': 589,\n",
       " 'course': 590,\n",
       " 'maligned': 591,\n",
       " 'swept': 592,\n",
       " 'relief': 593,\n",
       " 'born': 594,\n",
       " 'bath': 595,\n",
       " 'j': 596,\n",
       " 'unpai': 597,\n",
       " 'doodle': 598,\n",
       " 'yogurt': 599,\n",
       " 'ventured': 600,\n",
       " 'reconciled': 601,\n",
       " 'drowning': 602,\n",
       " 'scared': 603,\n",
       " 'thankk': 604,\n",
       " 'amme': 605,\n",
       " 'exposing': 606,\n",
       " 'mom': 607,\n",
       " 'sake': 608,\n",
       " 'neil': 609,\n",
       " 'marshal': 610,\n",
       " 'armani': 611,\n",
       " 'firearm': 612,\n",
       " 'gave': 613,\n",
       " 'revered': 614,\n",
       " 'director': 615,\n",
       " 'wouldance': 616,\n",
       " 'predicted': 617,\n",
       " 'magzine': 618,\n",
       " 'refrain': 619,\n",
       " 'able': 620,\n",
       " 'retro': 621,\n",
       " 'marius': 622,\n",
       " 'patagonia': 623,\n",
       " 'gal': 624,\n",
       " 'miniscule': 625,\n",
       " 'fed': 626,\n",
       " 'table': 627,\n",
       " 'whine': 628,\n",
       " 'qq': 629,\n",
       " 'alert': 630,\n",
       " 'quirky': 631,\n",
       " 'treatment': 632,\n",
       " 'baroque': 633,\n",
       " 'unilaterally': 634,\n",
       " 'monkey': 635,\n",
       " 'slacking': 636,\n",
       " 'nearest': 637,\n",
       " 'mutual': 638,\n",
       " 'allows': 639,\n",
       " 'feeling': 640,\n",
       " 'eepctqlhiafjwnrrmas': 641,\n",
       " 'farm': 642,\n",
       " 'enjorlas': 643,\n",
       " 'diy': 644,\n",
       " 'nearly': 645,\n",
       " 'compatible': 646,\n",
       " 'misunderstanding': 647,\n",
       " 'isabella': 648,\n",
       " 'alcoholic': 649,\n",
       " 'marketing': 650,\n",
       " 'elle': 651,\n",
       " 'knowing': 652,\n",
       " 'beared': 653,\n",
       " 'loquacious': 654,\n",
       " 'unnoticeable': 655,\n",
       " 'deformed': 656,\n",
       " 'employ': 657,\n",
       " 'matthew': 658,\n",
       " 'glam': 659,\n",
       " 'omd': 660,\n",
       " 'emergency': 661,\n",
       " 'owl': 662,\n",
       " 'mrt': 663,\n",
       " 'indecisi': 664,\n",
       " 'friggin': 665,\n",
       " 'kremlin': 666,\n",
       " 'jest': 667,\n",
       " 'weaving': 668,\n",
       " 'asleep': 669,\n",
       " 'na': 670,\n",
       " 'exile': 671,\n",
       " 'remix': 672,\n",
       " 'tic': 673,\n",
       " 'athf': 674,\n",
       " 'jot': 675,\n",
       " 'washington': 676,\n",
       " 'trustworthy': 677,\n",
       " 'dutch': 678,\n",
       " 'detailed': 679,\n",
       " 'thebalm': 680,\n",
       " 'gon': 681,\n",
       " 'embarrased': 682,\n",
       " 'involve': 683,\n",
       " 'manuscript': 684,\n",
       " 'receiving': 685,\n",
       " 'violin': 686,\n",
       " 'lens': 687,\n",
       " 'techfeel': 688,\n",
       " 'antm': 689,\n",
       " 'alfie': 690,\n",
       " 'bare': 691,\n",
       " 'yk': 692,\n",
       " 'shes': 693,\n",
       " 'alaska': 694,\n",
       " 'temp': 695,\n",
       " 'sluggish': 696,\n",
       " 'antivirus': 697,\n",
       " 'belonging': 698,\n",
       " 'lololol': 699,\n",
       " 'blew': 700,\n",
       " 'kueh': 701,\n",
       " 'veterinary': 702,\n",
       " 'rebellious': 703,\n",
       " 'flourish': 704,\n",
       " 'patiently': 705,\n",
       " 'handful': 706,\n",
       " 'beforehand': 707,\n",
       " 'transform': 708,\n",
       " 'assured': 709,\n",
       " 'minissha': 710,\n",
       " 'anthony': 711,\n",
       " 'manage': 712,\n",
       " 'fully': 713,\n",
       " 'neva': 714,\n",
       " 'pinterest': 715,\n",
       " 'jong': 716,\n",
       " 'bleh': 717,\n",
       " 'buoied': 718,\n",
       " 'woman': 719,\n",
       " 'stronger': 720,\n",
       " 'ongoing': 721,\n",
       " 'cloth': 722,\n",
       " 'fucker': 723,\n",
       " 'sympathize': 724,\n",
       " 'muay': 725,\n",
       " 'flu': 726,\n",
       " 'longbottom': 727,\n",
       " 'expectation': 728,\n",
       " 'acquainted': 729,\n",
       " 'forefoot': 730,\n",
       " 'desolate': 731,\n",
       " 'cluttered': 732,\n",
       " 'affection': 733,\n",
       " 'penn': 734,\n",
       " 'leisurely': 735,\n",
       " 'done': 736,\n",
       " 'necessary': 737,\n",
       " 'beneath': 738,\n",
       " 'mistress': 739,\n",
       " 'tpt': 740,\n",
       " 'covered': 741,\n",
       " 'broadening': 742,\n",
       " 'protest': 743,\n",
       " 'sufficiently': 744,\n",
       " 'photographed': 745,\n",
       " 'mindless': 746,\n",
       " 'excuse': 747,\n",
       " 'angeles': 748,\n",
       " 'throat': 749,\n",
       " 'strait': 750,\n",
       " 'twenty': 751,\n",
       " 'redden': 752,\n",
       " 'loaded': 753,\n",
       " 'coldness': 754,\n",
       " 'brunch': 755,\n",
       " 'hence': 756,\n",
       " 'aways': 757,\n",
       " 'hurtful': 758,\n",
       " 'ushering': 759,\n",
       " 'courage': 760,\n",
       " 'basis': 761,\n",
       " 'boomer': 762,\n",
       " 'flipping': 763,\n",
       " 'destroyed': 764,\n",
       " 'chat': 765,\n",
       " 'container': 766,\n",
       " 'fullbright': 767,\n",
       " 'rap': 768,\n",
       " 'crushing': 769,\n",
       " 'aol': 770,\n",
       " 'textile': 771,\n",
       " 'duo': 772,\n",
       " 'admiration': 773,\n",
       " 'pam': 774,\n",
       " 'skip': 775,\n",
       " 'detergent': 776,\n",
       " 'captured': 777,\n",
       " 'amenting': 778,\n",
       " 'gag': 779,\n",
       " 'seized': 780,\n",
       " 'injured': 781,\n",
       " 'jedi': 782,\n",
       " 'brag': 783,\n",
       " 'holi': 784,\n",
       " 'inhaler': 785,\n",
       " 'circus': 786,\n",
       " 'cyst': 787,\n",
       " 'austen': 788,\n",
       " 'thesis': 789,\n",
       " 'legal': 790,\n",
       " 'cpine': 791,\n",
       " 'attend': 792,\n",
       " 'marvellous': 793,\n",
       " 'enhance': 794,\n",
       " 'humour': 795,\n",
       " 'robot': 796,\n",
       " 'brotherhood': 797,\n",
       " 'sophist': 798,\n",
       " 'sponge': 799,\n",
       " 'samuel': 800,\n",
       " 'knotting': 801,\n",
       " 'publishing': 802,\n",
       " 'intolerably': 803,\n",
       " 'nemesis': 804,\n",
       " 'garmin': 805,\n",
       " 'chatter': 806,\n",
       " 'trickery': 807,\n",
       " 'homage': 808,\n",
       " 'addressed': 809,\n",
       " 'dumbass': 810,\n",
       " 'scribble': 811,\n",
       " 'delayed': 812,\n",
       " 'stumbled': 813,\n",
       " 'learnt': 814,\n",
       " 'eclipse': 815,\n",
       " 'seeing': 816,\n",
       " 'com': 817,\n",
       " 'veering': 818,\n",
       " 'sensing': 819,\n",
       " 'ish': 820,\n",
       " 'bobbin': 821,\n",
       " 'neatly': 822,\n",
       " 'macendarfer': 823,\n",
       " 'flex': 824,\n",
       " 'caused': 825,\n",
       " 'waaaaayyyy': 826,\n",
       " 'nibbling': 827,\n",
       " 'incomplete': 828,\n",
       " 'apt': 829,\n",
       " 'ttaekkaji': 830,\n",
       " 'therapy': 831,\n",
       " 'balloon': 832,\n",
       " 'psycho': 833,\n",
       " 'refered': 834,\n",
       " 'scarred': 835,\n",
       " 'invade': 836,\n",
       " 'granny': 837,\n",
       " 'whack': 838,\n",
       " 'town': 839,\n",
       " 'omegle': 840,\n",
       " 'glow': 841,\n",
       " 'despair': 842,\n",
       " 'venus': 843,\n",
       " 'robert': 844,\n",
       " 'murrelets': 845,\n",
       " 'needlessly': 846,\n",
       " 'revulsion': 847,\n",
       " 'tolerable': 848,\n",
       " 'dunno': 849,\n",
       " 'safety': 850,\n",
       " 'produce': 851,\n",
       " 'retail': 852,\n",
       " 'listened': 853,\n",
       " 'generation': 854,\n",
       " 'bedsi': 855,\n",
       " 'condone': 856,\n",
       " 'compunction': 857,\n",
       " 'withstand': 858,\n",
       " 'triggering': 859,\n",
       " 'tired': 860,\n",
       " 'mainstream': 861,\n",
       " 'dog': 862,\n",
       " 'era': 863,\n",
       " 'brokeup': 864,\n",
       " 'ugh': 865,\n",
       " 'come': 866,\n",
       " 'preview': 867,\n",
       " 'enable': 868,\n",
       " 'amit': 869,\n",
       " 'tiny': 870,\n",
       " 'layer': 871,\n",
       " 'packed': 872,\n",
       " 'named': 873,\n",
       " 'nestled': 874,\n",
       " 'breeze': 875,\n",
       " 'reaction': 876,\n",
       " 'urban': 877,\n",
       " 'spew': 878,\n",
       " 'rego': 879,\n",
       " 'damnation': 880,\n",
       " 'haircut': 881,\n",
       " 'wouldably': 882,\n",
       " 'vogue': 883,\n",
       " 'mojo': 884,\n",
       " 'romance': 885,\n",
       " 'purse': 886,\n",
       " 'miraculously': 887,\n",
       " 'classmate': 888,\n",
       " 'uggs': 889,\n",
       " 'finchers': 890,\n",
       " 'saw': 891,\n",
       " 'moved': 892,\n",
       " 'disorients': 893,\n",
       " 'blast': 894,\n",
       " 'musically': 895,\n",
       " 'andover': 896,\n",
       " 'puff': 897,\n",
       " 'getin': 898,\n",
       " 'tampon': 899,\n",
       " 'verbalize': 900,\n",
       " 'hihi': 901,\n",
       " 'positi': 902,\n",
       " 'pulling': 903,\n",
       " 'path': 904,\n",
       " 'sparring': 905,\n",
       " 'drafted': 906,\n",
       " 'freeer': 907,\n",
       " 'forming': 908,\n",
       " 'gauche': 909,\n",
       " 'dresser': 910,\n",
       " 'indirectly': 911,\n",
       " 'regulated': 912,\n",
       " 'drum': 913,\n",
       " 'moonlight': 914,\n",
       " 'firsthand': 915,\n",
       " 'inexplicably': 916,\n",
       " 'sssas': 917,\n",
       " 'recent': 918,\n",
       " 'proudest': 919,\n",
       " 'tricky': 920,\n",
       " 'sharpe': 921,\n",
       " 'zackmdavis': 922,\n",
       " 'corpse': 923,\n",
       " 'kindest': 924,\n",
       " 'update': 925,\n",
       " 'jersey': 926,\n",
       " 'ampact': 927,\n",
       " 'bereft': 928,\n",
       " 'silky': 929,\n",
       " 'band': 930,\n",
       " 'bandai': 931,\n",
       " 'lovely': 932,\n",
       " 'abou': 933,\n",
       " 'scotched': 934,\n",
       " 'prejudice': 935,\n",
       " 'umpteenth': 936,\n",
       " 'kong': 937,\n",
       " 'worst': 938,\n",
       " 'graduate': 939,\n",
       " 'congenial': 940,\n",
       " 'deform': 941,\n",
       " 'noleans': 942,\n",
       " 'thackerey': 943,\n",
       " 'mood': 944,\n",
       " 'mexican': 945,\n",
       " 'dreary': 946,\n",
       " 'narcissism': 947,\n",
       " 'hopper': 948,\n",
       " 'overwhelming': 949,\n",
       " 'womb': 950,\n",
       " 'obviously': 951,\n",
       " 'jerald': 952,\n",
       " 'headlamp': 953,\n",
       " 'reverence': 954,\n",
       " 'fluffy': 955,\n",
       " 'holden': 956,\n",
       " 'plantar': 957,\n",
       " 'sd': 958,\n",
       " 'uneventful': 959,\n",
       " 'wheel': 960,\n",
       " 'often': 961,\n",
       " 'hurling': 962,\n",
       " 'six': 963,\n",
       " 'pendleton': 964,\n",
       " 'punctual': 965,\n",
       " 'worm': 966,\n",
       " 'perpetrator': 967,\n",
       " 'ravioli': 968,\n",
       " 'doc': 969,\n",
       " 'cyber': 970,\n",
       " 'arty': 971,\n",
       " 'bungling': 972,\n",
       " 'interpreted': 973,\n",
       " 'char': 974,\n",
       " 'solondz': 975,\n",
       " 'fashion': 976,\n",
       " 'tour': 977,\n",
       " 'trinity': 978,\n",
       " 'conceptualized': 979,\n",
       " 'shuts': 980,\n",
       " 'scissor': 981,\n",
       " 'tia': 982,\n",
       " 'visting': 983,\n",
       " 'cavity': 984,\n",
       " 'hypersexual': 985,\n",
       " 'isolation': 986,\n",
       " 'hyped': 987,\n",
       " 'trigonometry': 988,\n",
       " 'brandish': 989,\n",
       " 'vase': 990,\n",
       " 'eyeliner': 991,\n",
       " 'movign': 992,\n",
       " 'asma': 993,\n",
       " 'utilize': 994,\n",
       " 'sombre': 995,\n",
       " 'flac': 996,\n",
       " 'suspect': 997,\n",
       " 'spurging': 998,\n",
       " 'collected': 999,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def id_each_unique_word(df):\n",
    "    \"\"\"\" \n",
    "    Mapping each unique word to an id\n",
    "    \n",
    "    Returns: \n",
    "    - word_to_id (maps the words to an id) \n",
    "    - id_to_word (maps the id to a word)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    index_each_word = df['tokenized_text'].explode().unique()\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "\n",
    "\n",
    "    for i, token in enumerate(set(index_each_word)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "\n",
    "    print(f'Unique words in the text: {len(word_to_id)}')\n",
    "    display(word_to_id)\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "\n",
    "word_to_id, id_to_word = id_each_unique_word(df)\n",
    "\n",
    "\n",
    "def change_label_to_int(df):\n",
    "    \"\"\" Change the labels column positive, neutral, negative to ints \"\"\"\n",
    "\n",
    "    df['label'] = df['label'].replace({'Negative': -1, 'Neutral': 0, 'Positive': 1})\n",
    "\n",
    "    return\n",
    "\n",
    "change_label_to_int(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel so pissed off over an old friend and some friends</td>\n",
       "      <td>-1</td>\n",
       "      <td>[feel, pissed, old, friend, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated</td>\n",
       "      <td>-1</td>\n",
       "      <td>[found, made, huge, difference, especially, finger, ring, skin, feel, much, softer, le, irritated]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i also feel it is unfortunate that nearly all the readers of going to meet the man wi will be african americans unlike myself</td>\n",
       "      <td>-1</td>\n",
       "      <td>[also, feel, unfortunate, nearly, reader, going, meet, man, wi, african, american, unlike]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel petty a href http clairee</td>\n",
       "      <td>-1</td>\n",
       "      <td>[feel, petty, href, http, clairee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i used to believe that a feeling like fear was to be ignored or suppressed right away more on this in a moment</td>\n",
       "      <td>-1</td>\n",
       "      <td>[used, believe, feeling, like, fear, ignored, suppressed, right, away, moment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>i was i might be buying stuff from there but i feel the clothes are too casual</td>\n",
       "      <td>1</td>\n",
       "      <td>[might, buying, stuff, feel, clothes, casual]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>i like sonam deepika and genelia who i feel are very talented and beautiful</td>\n",
       "      <td>1</td>\n",
       "      <td>[like, sonam, deepika, genelia, feel, talented, beautiful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>i feel pathetic that i can hardly go a whole day not talking to hi am</td>\n",
       "      <td>-1</td>\n",
       "      <td>[feel, pathetic, hardly, go, whole, day, talking, hi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>i would have spent more ti ame with her on reading i feel a bit guilty about that</td>\n",
       "      <td>-1</td>\n",
       "      <td>[would, spent, ti, ame, reading, feel, bit, guilty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>i do however feel like one of those pathetic girls who make up excuses because of a guy</td>\n",
       "      <td>-1</td>\n",
       "      <td>[however, feel, like, one, pathetic, girl, make, excuse, guy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0                                                                                      i feel so pissed off over an old friend and some friends   \n",
       "1      i have found it has made a huge difference especially on the finger with my ring and the my skin feels so much softer and less irritated   \n",
       "2                 i also feel it is unfortunate that nearly all the readers of going to meet the man wi will be african americans unlike myself   \n",
       "3                                                                                                              i feel petty a href http clairee   \n",
       "4                                i used to believe that a feeling like fear was to be ignored or suppressed right away more on this in a moment   \n",
       "...                                                                                                                                         ...   \n",
       "19995                                                            i was i might be buying stuff from there but i feel the clothes are too casual   \n",
       "19996                                                               i like sonam deepika and genelia who i feel are very talented and beautiful   \n",
       "19997                                                                     i feel pathetic that i can hardly go a whole day not talking to hi am   \n",
       "19998                                                         i would have spent more ti ame with her on reading i feel a bit guilty about that   \n",
       "19999                                                   i do however feel like one of those pathetic girls who make up excuses because of a guy   \n",
       "\n",
       "       label  \\\n",
       "0         -1   \n",
       "1         -1   \n",
       "2         -1   \n",
       "3         -1   \n",
       "4         -1   \n",
       "...      ...   \n",
       "19995      1   \n",
       "19996      1   \n",
       "19997     -1   \n",
       "19998     -1   \n",
       "19999     -1   \n",
       "\n",
       "                                                                                           tokenized_text  \n",
       "0                                                                     [feel, pissed, old, friend, friend]  \n",
       "1      [found, made, huge, difference, especially, finger, ring, skin, feel, much, softer, le, irritated]  \n",
       "2              [also, feel, unfortunate, nearly, reader, going, meet, man, wi, african, american, unlike]  \n",
       "3                                                                      [feel, petty, href, http, clairee]  \n",
       "4                          [used, believe, feeling, like, fear, ignored, suppressed, right, away, moment]  \n",
       "...                                                                                                   ...  \n",
       "19995                                                       [might, buying, stuff, feel, clothes, casual]  \n",
       "19996                                          [like, sonam, deepika, genelia, feel, talented, beautiful]  \n",
       "19997                                               [feel, pathetic, hardly, go, whole, day, talking, hi]  \n",
       "19998                                                 [would, spent, ti, ame, reading, feel, bit, guilty]  \n",
       "19999                                       [however, feel, like, one, pathetic, girl, make, excuse, guy]  \n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ['feel', 'pissed', 'old', 'friend', 'friend']\n",
      "Converted to Sequence: 1: [13985, 6798, 9567, 14854, 14854]\n",
      "======================================================\n",
      "Sentence 2: ['found', 'made', 'huge', 'difference', 'especially', 'finger', 'ring', 'skin', 'feel', 'much', 'softer', 'le', 'irritated']\n",
      "Converted to Sequence: 2: [12097, 5051, 13969, 2016, 3282, 7622, 6763, 5525, 13985, 2167, 9391, 4629, 12078]\n",
      "======================================================\n",
      "Sentence 3: ['also', 'feel', 'unfortunate', 'nearly', 'reader', 'going', 'meet', 'man', 'wi', 'african', 'american', 'unlike']\n",
      "Converted to Sequence: 3: [6291, 13985, 5916, 645, 9802, 3483, 6983, 4746, 3793, 9444, 9464, 13292]\n",
      "======================================================\n",
      "Sentence 4: ['feel', 'petty', 'href', 'http', 'clairee']\n",
      "Converted to Sequence: 4: [13985, 9773, 1140, 14360, 7486]\n",
      "======================================================\n",
      "Sentence 5: ['used', 'believe', 'feeling', 'like', 'fear', 'ignored', 'suppressed', 'right', 'away', 'moment']\n",
      "Converted to Sequence: 5: [14967, 13369, 640, 13900, 8869, 4428, 7204, 9627, 6619, 93]\n",
      "======================================================\n",
      "Amount of text sequences (rows): 20000\n"
     ]
    }
   ],
   "source": [
    "def text_to_sequence(word_to_id):\n",
    "    \"\"\" \n",
    "    Convert sentences to sequences from word_of_id mapping\n",
    "      \n",
    "    Returns: sequence of ints where each sequence corresponds to a sentence\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    sequences = []\n",
    "\n",
    "\n",
    "    for index, sentence in enumerate(df['tokenized_text']):\n",
    "\n",
    "        # for each word in the sentence get the corresponding id from mapping and put in list, if not found return None\n",
    "        seq = [word_to_id.get(word, None) for word in sentence]\n",
    "        sequences.append(seq)\n",
    "\n",
    "        if index < 5: # 5 examples to make sure its correct converted\n",
    "            print(f\"Sentence {index+1}: {sentence}\")\n",
    "            get_separator\n",
    "            print(f\"Converted to Sequence: {index+1}: {seq}\")\n",
    "            get_separator()\n",
    "\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "sequences = text_to_sequence(word_to_id)\n",
    "\n",
    "\n",
    "print(f'Amount of text sequences (rows): {len(sequences)}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 38\n",
      "Min sequence length: 1\n",
      "Mean sequence length: 9.55455\n",
      "======================================================\n",
      "Max sequence length: 38\n",
      "Min sequence length: 38\n",
      "Mean sequence length: 38.0\n"
     ]
    }
   ],
   "source": [
    "def get_seq_length(sequences):\n",
    "    \"\"\" Get the length of the maximum sequence in the sequences list to determine padding \"\"\"\n",
    "    max_length = max(len(sequence) for sequence in sequences)\n",
    "    min_length = min(len(sequence) for sequence in sequences)\n",
    "    mean_length = sum(map(len, sequences)) / len(sequences)\n",
    "\n",
    "    print(f'Max sequence length: {max_length}')\n",
    "    print(f'Min sequence length: {min_length}')\n",
    "    print(f'Mean sequence length: {mean_length}')\n",
    "\n",
    "    return max_length, min_length, mean_length\n",
    "\n",
    "\n",
    "def pad_data(sequences, max_length):\n",
    "    \"\"\" Pad the sequences to the max length \"\"\"\n",
    "\n",
    "    padded_sequences = []\n",
    "\n",
    "    for sequence in sequences:\n",
    "        padded_sequence = sequence + [0] * (max_length - len(sequence)) # padding the sequences with 0 (neutral label)\n",
    "        padded_sequences.append(padded_sequence)\n",
    "\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "# investigating so the padding works as intented\n",
    "og_max_length, og_min_length, og_mean_length = get_seq_length(sequences)\n",
    "get_separator()\n",
    "padded_sequences = pad_data(sequences, og_max_length)\n",
    "max_length, min_length, mean_length = get_seq_length(padded_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the paddding might affect the overall model and further deep diving into the text data itself is needed. But it's not the priority for me in this assignment, the goal is to follow the whole workflow and create a model from scratch for learning. For instance some words that are misspelled i haven't done anything about either, and this will also affect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (12000, 38)\n",
      "X_test shape: (4000, 38)\n",
      "X_val shape: (4000, 38)\n",
      "y_train shape: (12000,)\n",
      "y_test shape: (4000,)\n",
      "y_val shape: (4000,)\n"
     ]
    }
   ],
   "source": [
    "def get_train_test_val(df, padded_sequences):\n",
    "    \"\"\" \n",
    "    Converts and aligns the padded sequence and the labels to a numpy array.\n",
    "    Then splits up the data.\n",
    "\n",
    "    Takes in:\n",
    "    - padded_sequence(list)\n",
    "    - df['labels'] \n",
    "\n",
    "    Returns: train, test, val split of the data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Converting to numpy arrays, these lists are in the same order\n",
    "    np_seq = np.array(padded_sequences)\n",
    "    np_labels = np.array(df['label'].values)\n",
    "\n",
    "\n",
    "    # Splitting the data into:\n",
    "    # - train (60%)\n",
    "    # - test (20%)\n",
    "    # - validation (20% of train data)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np_seq, np_labels, test_size=0.2, random_state=137)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=137)\n",
    "\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = get_train_test_val(df, padded_sequences)\n",
    "\n",
    "\n",
    "def check_x_shape(X_train, X_test, X_val, y_train, y_test, y_val):\n",
    "    \"\"\" Check the shape of the arrays \"\"\"\n",
    "\n",
    "    print(f'X_train shape: {X_train.shape}')\n",
    "    print(f'X_test shape: {X_test.shape}')\n",
    "    print(f'X_val shape: {X_val.shape}')\n",
    "    print(f'y_train shape: {y_train.shape}')\n",
    "    print(f'y_test shape: {y_test.shape}')\n",
    "    print(f'y_val shape: {y_val.shape}')\n",
    "\n",
    "\n",
    "def check_alignment():\n",
    "    \"\"\" Check and inspect the array \"\"\"\n",
    "    for i in range(5):\n",
    "        get_separator()\n",
    "        print(\"Sequence:\", X_train[i])\n",
    "        print(\"Label:\", y_train[i])\n",
    "    \n",
    "    get_separator()\n",
    "\n",
    "check_x_shape(X_train, X_test, X_val, y_train, y_test, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "Sequence: [13985 10509  5139 11343 12476  4423 11352  9750     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "Label: 1\n",
      "======================================================\n",
      "Sequence: [13985  3610  9138    93     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "Label: -1\n",
      "======================================================\n",
      "Sequence: [13985   449 10100     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "Label: -1\n",
      "======================================================\n",
      "Sequence: [13985   121  7390  6540  2052 14599  8283  8172  2546  4170     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "Label: 1\n",
      "======================================================\n",
      "Sequence: [ 6596  8270 13985  5516 11143 13168  8502 11143  2035  6830  3621     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "Label: 1\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "check_alignment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: input_dim is equal 38  ( equal to the length padded_sequence)\n",
    "TODO: Vocab_size is the len of word_to_id\n",
    "TODO: hidden_dim (????)\n",
    "\n",
    "\n",
    "TODO: Split up the data and so they correspond with the label from the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next is padding the data?\n",
    "But what do i with the labels? And split up the data and labels (And i need to make sure that the labels follow correct sentence)\n",
    "\n",
    "\n",
    "# use constant padding since its 0 and affects the model as little as possible.\n",
    "\n",
    "\n",
    "\n",
    "# My choice in the model is gonna be an RNN network. Since i think even though its a multiclass classficiation problem. I only have 3 classes so its not that complex i hope. And also i am combating the vanishing gradient problem by using tanh as the activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label', 'tokenized_text'], dtype='object')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'www' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwww\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'www' is not defined"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "www"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class RNNCell(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Initialize weights and biases\n",
    "        self.Wxh = tf.Variable(tf.random.normal([input_dim, hidden_dim]))\n",
    "        self.Whh = tf.Variable(tf.random.normal([hidden_dim, hidden_dim]))\n",
    "        self.bh = tf.Variable(tf.zeros([hidden_dim]))\n",
    "\n",
    "    def __call__(self, x, h):\n",
    "        # Compute the next hidden state\n",
    "        h_next = tf.tanh(tf.matmul(x, self.Wxh) + tf.matmul(h, self.Whh) + self.bh)\n",
    "        return h_next\n",
    "\n",
    "class MyRNNModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=1, sequence_length=100):\n",
    "        super().__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = tf.Variable(tf.random.normal([vocab_size, embedding_dim]))\n",
    "        # RNN Cell\n",
    "        self.rnn_cell = RNNCell(embedding_dim, hidden_dim)\n",
    "        # Output layer weights and biases\n",
    "        self.Why = tf.Variable(tf.random.normal([hidden_dim, output_dim]))\n",
    "        self.by = tf.Variable(tf.zeros([output_dim]))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Embedding lookup\n",
    "        x = tf.nn.embedding_lookup(self.embedding, x)\n",
    "        # Initial hidden state (zeros)\n",
    "        h = tf.zeros([x.shape[0], self.rnn_cell.Whh.shape[0]])\n",
    "\n",
    "        # Process the input sequence\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h = self.rnn_cell(x_t, h)\n",
    "\n",
    "        # Compute the output\n",
    "        y = tf.matmul(h, self.Why) + self.by\n",
    "        return tf.sigmoid(y)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = tf.losses.BinaryCrossentropy()\n",
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "def train_step(model, inputs, targets, clip_norm=1.0):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(targets, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    gradients = [tf.clip_by_norm(g, clip_norm) for g in gradients]  # Gradient clipping\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Example usage\n",
    "# Define your model, dataset, and training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 17:03:28.307898: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def pad_data(sequences):\n",
    "    # Find the maximum sequence length\n",
    "    max_seq_length = max(len(s) for s in sequences)\n",
    "\n",
    "    # Pad the sequences to ensure uniform length\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "    return padded_sequences, max_seq_length\n",
    "\n",
    "\n",
    "padded_sequences, max_seq_length = pad_data(sequences)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define model parameters\n",
    "vocab_size = len(word_to_id)  # Total number of unique words\n",
    "embed_size = 100  # Size of the embeddings\n",
    "\n",
    "# Adjust the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=max_seq_length))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(27, activation='tanh'))\n",
    "model.add(Dense(9, activation='tanh'))\n",
    "model.add(Dense(3, activation='tanh'))\n",
    "\n",
    "\n",
    "# I am using tanh since activation since it introduces non-linearity to the model and it is good for classification problems. The curve goes from -1 to 1 and it is good considering positive, neutral and negative.\n",
    "# S\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorial_crossentrop', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into train and test sets (e.g., 80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting the training data into train and validation sets (e.g., 80% train, 20% validation of the original training data)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "375/375 [==============================] - 12s 27ms/step - loss: 0.6872 - accuracy: 0.1715 - val_loss: 0.2195 - val_accuracy: 0.3918\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 9s 24ms/step - loss: 0.1148 - accuracy: 0.4134 - val_loss: 0.1535 - val_accuracy: 0.4175\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 9s 25ms/step - loss: 0.0456 - accuracy: 0.4352 - val_loss: 0.1185 - val_accuracy: 0.4150\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.0268 - accuracy: 0.4420 - val_loss: 0.1103 - val_accuracy: 0.4353\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.0256 - accuracy: 0.4454 - val_loss: 0.2796 - val_accuracy: 0.4358\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.1423 - accuracy: 0.3992 - val_loss: 0.2348 - val_accuracy: 0.4142\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 9s 25ms/step - loss: 0.0748 - accuracy: 0.4212 - val_loss: 0.1984 - val_accuracy: 0.4035\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 9s 25ms/step - loss: 0.0481 - accuracy: 0.4336 - val_loss: 0.1631 - val_accuracy: 0.4230\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.0258 - accuracy: 0.4421 - val_loss: 0.1422 - val_accuracy: 0.4235\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 11s 30ms/step - loss: 0.0134 - accuracy: 0.4481 - val_loss: 0.1455 - val_accuracy: 0.4327\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.0089 - accuracy: 0.4506 - val_loss: 0.1542 - val_accuracy: 0.4360\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.0157 - accuracy: 0.4495 - val_loss: 0.1411 - val_accuracy: 0.4338\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 9s 24ms/step - loss: 0.0215 - accuracy: 0.4471 - val_loss: 0.1576 - val_accuracy: 0.4280\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 9s 25ms/step - loss: 0.0141 - accuracy: 0.4498 - val_loss: 0.1659 - val_accuracy: 0.4280\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 9s 24ms/step - loss: 0.0156 - accuracy: 0.4492 - val_loss: 0.1704 - val_accuracy: 0.4275\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 9s 25ms/step - loss: 0.0188 - accuracy: 0.4471 - val_loss: 0.1856 - val_accuracy: 0.4243\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.0135 - accuracy: 0.4493 - val_loss: 0.1774 - val_accuracy: 0.4268\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.0240 - accuracy: 0.4471 - val_loss: 0.1526 - val_accuracy: 0.4195\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 11s 29ms/step - loss: 0.0164 - accuracy: 0.4487 - val_loss: 0.1429 - val_accuracy: 0.4230\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 11s 30ms/step - loss: 0.0093 - accuracy: 0.4499 - val_loss: 0.1374 - val_accuracy: 0.4342\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 12s 32ms/step - loss: 0.0077 - accuracy: 0.4498 - val_loss: 0.1391 - val_accuracy: 0.4280\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 9s 24ms/step - loss: 0.0052 - accuracy: 0.4515 - val_loss: 0.1446 - val_accuracy: 0.4397\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.0236 - accuracy: 0.4468 - val_loss: 0.1476 - val_accuracy: 0.4207\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.0155 - accuracy: 0.4489 - val_loss: 0.1804 - val_accuracy: 0.4212\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.0191 - accuracy: 0.4493 - val_loss: 0.1391 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 8s 22ms/step - loss: 0.0122 - accuracy: 0.4495 - val_loss: 0.1372 - val_accuracy: 0.4405\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 9s 25ms/step - loss: 0.0100 - accuracy: 0.4488 - val_loss: 0.1437 - val_accuracy: 0.4295\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 9s 24ms/step - loss: 0.0110 - accuracy: 0.4493 - val_loss: 0.1752 - val_accuracy: 0.4250\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.0124 - accuracy: 0.4501 - val_loss: 0.1491 - val_accuracy: 0.4367\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 12s 31ms/step - loss: 0.0109 - accuracy: 0.4504 - val_loss: 0.1441 - val_accuracy: 0.4448\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=NUM_EPOCHS,              # Number of epochs\n",
    "                    batch_size=32,          # Size of each batch\n",
    "                    validation_data=(X_val, y_val))  # Validation data to monitor performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_history(history, X_train, y_train, X_val, y_val, NUM_EPOCHS, NUM_EPOCHS):\n",
    "    time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    file = (f'history_{NUM_EPOCHS}_{time}.npy')\n",
    "    file = os.path.join(LOGS_PATH, file)\n",
    "\n",
    "    \n",
    "    # Create logs folder if it does not exist\n",
    "    if not LOGS_PATH.exists:\n",
    "        os.makedirs('logs')\n",
    "\n",
    "\n",
    "    np.save(file, history.history) # saves history as a dict with key values\n",
    "\n",
    "    model.save('model/model_1.h5')  #TODO generalize this solution\n",
    "    \n",
    "    # np.save('logs/history.npy', history.history)\n",
    "    \n",
    "    get_separator()\n",
    "    print(f'Saved training history to: {file}')\n",
    "    get_separator()\n",
    "    return history, save_history(history)\n",
    "\n",
    "# train_model(model, X_train, y_train, X_val, y_val, NUM_EPOCHS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots metrics over training history\n",
    "def plot_metrics(metrics):\n",
    "    _, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    metrics[['loss', 'val_loss']].plot(ax=ax[0], title='Loss', grid=True)\n",
    "    metrics[['accuracy', 'val_accuracy']].plot(ax=ax[1], title='Accuracy', grid=True) \n",
    "\n",
    "plot_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 1s 8ms/step - loss: 0.1333 - accuracy: 0.4378\n",
      "Test Loss: 0.13325612246990204\n",
      "Test Accuracy: 0.4377500116825104\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-rnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
